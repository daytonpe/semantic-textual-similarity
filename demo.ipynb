{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 6320 Natural Language Processing\n",
    "## Shruti Agrawal & Pat Dayton\n",
    "\n",
    "This notebook demos our code for Tasks 1 & 2 of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "\n",
    "\n",
    "# SPACY IMPORT\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# CoreNLP setup\n",
    "core_nlp_url = 'http://localhost:9000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Stanford CoreNLP Server\n",
    "In another console run the script below in order to start the Stanford CoreNLP Server on port 9000. We will hit this API in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "#java -mx4g -cp \"./corenlp/*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Parse the Corpus\n",
    "First read in the corpus and do basic parsing to split out the first sentence, second sentence, and score for each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(fileName, test=False):\n",
    "\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    score = []\n",
    "    file = open(fileName, encoding=\"utf8\")\n",
    "    text = file.readline()\n",
    "    text = file.read()\n",
    "    \n",
    "    # loop to extract a set of two sentences\n",
    "    for sentence in text.split('\\n'):\n",
    "\n",
    "        # creating two separate lists of the sentences\n",
    "        # '.rstrip('.') only removes the last period in the sentence\n",
    "        \n",
    "        s1.insert(len(s1), (sentence.split('\\t')[1].lower()).rstrip('.'))\n",
    "        s2.insert(len(s1), (sentence.split('\\t')[2].lower()).rstrip('.'))\n",
    "        \n",
    "        # inserting the score as a separate lists\n",
    "        if (not test):\n",
    "            score.insert(len(s1), (sentence.split('\\t')[3]))\n",
    "\n",
    "    # print(s1)\n",
    "    if test:\n",
    "        return s1, s2\n",
    "    else:\n",
    "        return s1, s2, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same readData from STS.py\n",
    "def preprocess(fileName, test=False):\n",
    "\n",
    "    if (test):\n",
    "        s1, s2 = readData(fileName, test)\n",
    "    else:\n",
    "        s1, s2, scores = readData(fileName, test)\n",
    "\n",
    "    s1_toks = []\n",
    "    s2_toks = []\n",
    "\n",
    "    # tokenizing and tagging\n",
    "    s1_tags = []\n",
    "    s2_tags = []\n",
    "\n",
    "    for sentence in s1:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        s1_toks.insert(len(s1_toks), tokens)\n",
    "        s1_tags.insert(\n",
    "            len(s1_tags), nltk.pos_tag(tokens))\n",
    "\n",
    "    for sentence in s2:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        s2_toks.insert(len(s2_toks), tokens)\n",
    "        s2_tags.insert(\n",
    "            len(s2_tags), nltk.pos_tag(tokens))\n",
    "    \n",
    "    # Remove the unnecessary tuple and keep just the tags\n",
    "    for i, tag_list in enumerate(s1_tags):\n",
    "        s1_tags[i] = [tup[1] for tup in tag_list]\n",
    "    for i, tag_list in enumerate(s2_tags):\n",
    "        s2_tags[i] = [tup[1] for tup in tag_list]\n",
    "\n",
    "    # lemmatizing\n",
    "    s1_lemmas = []\n",
    "    s2_lemmas = []\n",
    "    s1_ls = []\n",
    "    s2_ls = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for sentence in s1_toks:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        s1_lemmas.insert(\n",
    "            len(s1_lemmas), sentence_components)\n",
    "        s1_ls.insert(len(s1_ls), ' '.join(word for word in sentence_components))\n",
    "    \n",
    "    for sentence in s2_toks:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        s2_lemmas.insert(\n",
    "            len(s2_lemmas), sentence_components)\n",
    "        s2_ls.insert(len(s2_ls), ' '.join(word for word in sentence_components))\n",
    "\n",
    "\n",
    "        \n",
    "    # Zipping it all together into one object for each word\n",
    "    s1_word_lists = []\n",
    "    s2_word_lists = []\n",
    "    \n",
    "    for tok_list, lem_list, tag_list in zip(s1_toks, s1_lemmas, s1_tags):\n",
    "        sentence_words = []\n",
    "        for tok, lem, tag in zip(tok_list, lem_list, tag_list):\n",
    "            word = {}\n",
    "            word['tok'] = tok\n",
    "            word['lem'] = lem\n",
    "            word['tag'] = tag\n",
    "            sentence_words.append(word)\n",
    "        s1_word_lists.append(sentence_words) \n",
    "        \n",
    "    for tok_list, lem_list, tag_list in zip(s2_toks, s2_lemmas, s2_tags):\n",
    "        sentence_words = []\n",
    "        for tok, lem, tag in zip(tok_list, lem_list, tag_list):\n",
    "            word = {}\n",
    "            word['tok'] = tok\n",
    "            word['lem'] = lem\n",
    "            word['tag'] = tag\n",
    "            sentence_words.append(word)\n",
    "        s2_word_lists.append(sentence_words)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create a corpus object to represent our corpus\n",
    "    corpus = {}\n",
    "    corpus[\"s1\"] = {}\n",
    "    corpus[\"s2\"] = {}\n",
    "    if (not test):\n",
    "        corpus['scores'] = [int(i) for i in scores]\n",
    "    \n",
    "    corpus[\"s1\"][\"sentences\"] = s1\n",
    "    corpus[\"s2\"][\"sentences\"] = s2\n",
    "    \n",
    "    corpus[\"s1\"][\"tokens\"] = s1_toks\n",
    "    corpus[\"s2\"][\"tokens\"] = s2_toks\n",
    "    \n",
    "    corpus[\"s1\"][\"lemmas\"] = s1_lemmas\n",
    "    corpus[\"s2\"][\"lemmas\"] = s2_lemmas\n",
    "    \n",
    "    corpus[\"s1\"][\"tags\"] = s1_tags\n",
    "    corpus[\"s2\"][\"tags\"] = s2_tags\n",
    "    \n",
    "    corpus[\"s1\"][\"words\"] = s1_word_lists\n",
    "    corpus[\"s2\"][\"words\"] = s2_word_lists\n",
    "\n",
    "    corpus[\"s1\"][\"ls\"] = s1_ls\n",
    "    corpus[\"s2\"][\"ls\"] = s2_ls\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "the fine are part of failed republican effort to force or entice the democrat to return\n"
    }
   ],
   "source": [
    "train_data = preprocess(\"./data/train-set.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1484\n1484\n"
    }
   ],
   "source": [
    "print(len(train_data[\"s1\"]['sentences']))\n",
    "print(len(train_data[\"s2\"]['sentences']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "ROW 286 FROM TEST DATA\n\nSentence 1\n\nRaw:  gemstar's shares gathered up 2.6 percent, adding 14 cents to $5.49 at the close\nSentence 2\n\nRaw:  gemstar shares moved higher on the news, closing up 2.6 percent at $5.49 on nasdaq\nScore:  4\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tokens</th>\n      <th>Lemmas</th>\n      <th>Tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gemstar</td>\n      <td>gemstar</td>\n      <td>NN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>'s</td>\n      <td>'s</td>\n      <td>POS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>shares</td>\n      <td>share</td>\n      <td>NNS</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gathered</td>\n      <td>gathered</td>\n      <td>VBD</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>up</td>\n      <td>up</td>\n      <td>RP</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2.6</td>\n      <td>2.6</td>\n      <td>CD</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>percent</td>\n      <td>percent</td>\n      <td>NN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>,</td>\n      <td>,</td>\n      <td>,</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>adding</td>\n      <td>adding</td>\n      <td>VBG</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>14</td>\n      <td>14</td>\n      <td>CD</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>cents</td>\n      <td>cent</td>\n      <td>NNS</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>to</td>\n      <td>to</td>\n      <td>TO</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>$</td>\n      <td>$</td>\n      <td>$</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>5.49</td>\n      <td>5.49</td>\n      <td>CD</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>at</td>\n      <td>at</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>the</td>\n      <td>the</td>\n      <td>DT</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>close</td>\n      <td>close</td>\n      <td>NN</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "      Tokens    Lemmas Tags\n0    gemstar   gemstar   NN\n1         's        's  POS\n2     shares     share  NNS\n3   gathered  gathered  VBD\n4         up        up   RP\n5        2.6       2.6   CD\n6    percent   percent   NN\n7          ,         ,    ,\n8     adding    adding  VBG\n9         14        14   CD\n10     cents      cent  NNS\n11        to        to   TO\n12         $         $    $\n13      5.49      5.49   CD\n14        at        at   IN\n15       the       the   DT\n16     close     close   NN"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tokens</th>\n      <th>Lemmas</th>\n      <th>Tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gemstar</td>\n      <td>gemstar</td>\n      <td>NN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>shares</td>\n      <td>share</td>\n      <td>NNS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>moved</td>\n      <td>moved</td>\n      <td>VBD</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>higher</td>\n      <td>higher</td>\n      <td>RBR</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>on</td>\n      <td>on</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>the</td>\n      <td>the</td>\n      <td>DT</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>news</td>\n      <td>news</td>\n      <td>NN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>,</td>\n      <td>,</td>\n      <td>,</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>closing</td>\n      <td>closing</td>\n      <td>VBG</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>up</td>\n      <td>up</td>\n      <td>RP</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2.6</td>\n      <td>2.6</td>\n      <td>CD</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>percent</td>\n      <td>percent</td>\n      <td>NN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>at</td>\n      <td>at</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>$</td>\n      <td>$</td>\n      <td>$</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>5.49</td>\n      <td>5.49</td>\n      <td>CD</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>on</td>\n      <td>on</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>nasdaq</td>\n      <td>nasdaq</td>\n      <td>NNS</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "     Tokens   Lemmas Tags\n0   gemstar  gemstar   NN\n1    shares    share  NNS\n2     moved    moved  VBD\n3    higher   higher  RBR\n4        on       on   IN\n5       the      the   DT\n6      news     news   NN\n7         ,        ,    ,\n8   closing  closing  VBG\n9        up       up   RP\n10      2.6      2.6   CD\n11  percent  percent   NN\n12       at       at   IN\n13        $        $    $\n14     5.49     5.49   CD\n15       on       on   IN\n16   nasdaq   nasdaq  NNS"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r=286\n",
    "\n",
    "tkns1 = train_data[\"s1\"]['tokens'][r]\n",
    "lems1 = train_data[\"s1\"]['lemmas'][r]\n",
    "tags1 = train_data[\"s1\"]['tags'][r]\n",
    "tkns2 = train_data[\"s2\"]['tokens'][r]\n",
    "lems2 = train_data[\"s2\"]['lemmas'][r]\n",
    "tags2 = train_data[\"s2\"]['tags'][r]\n",
    "\n",
    "data1 = []\n",
    "data2 = []\n",
    "\n",
    "for i in range(0, len(tkns1)):\n",
    "    data1.append([tkns1[i], lems1[i], tags1[i]])\n",
    "    \n",
    "for i in range(0, len(tkns2)):\n",
    "    data2.append([tkns2[i], lems2[i], tags2[i]])\n",
    "    \n",
    "df1 = pd.DataFrame(\n",
    "    data1, \n",
    "    columns = ['Tokens', 'Lemmas', 'Tags']) \n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "    data2, \n",
    "    columns = ['Tokens', 'Lemmas', 'Tags']) \n",
    "\n",
    "\n",
    "print('ROW {} FROM TEST DATA\\n'.format(r))\n",
    "print('Sentence 1\\n')\n",
    "print('Raw: ', train_data[\"s1\"]['sentences'][r])\n",
    "display(df1)\n",
    "print('Sentence 2\\n')\n",
    "print('Raw: ', train_data[\"s2\"]['sentences'][r])\n",
    "display(df2)\n",
    "print('Score: ', train_data[\"scores\"][r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nDependency Parsing Sentence 1\n\ngemstar\tNN\t3\tnmod:poss\n's\tPOS\t1\tcase\nshares\tNNS\t4\tnsubj\ngathered\tVBD\t0\tROOT\nup\tRP\t4\tcompound:prt\n2.6\tCD\t7\tnummod\npercent\tNN\t4\tdobj\n,\t,\t4\tpunct\nadding\tVBG\t4\tadvcl\n14\tCD\t11\tnummod\ncents\tNNS\t9\tdobj\nto\tTO\t14\tcase\n$\t$\t14\tdep\n5.49\tCD\t9\tnmod\nat\tIN\t17\tcase\nthe\tDT\t17\tdet\nclose\tNN\t9\tnmod\n\n\nDependency Parsing Sentence 2\n\ngemstar\tJJ\t2\tamod\nshares\tNNS\t3\tnsubj\nmoved\tVBD\t0\tROOT\nhigher\tRBR\t3\tadvmod\non\tIN\t7\tcase\nthe\tDT\t7\tdet\nnews\tNN\t3\tnmod\n,\t,\t3\tpunct\nclosing\tVBG\t3\tadvcl\nup\tRP\t9\tcompound:prt\n2.6\tCD\t12\tnummod\npercent\tNN\t9\tdobj\nat\tIN\t15\tcase\n$\t$\t15\tdep\n5.49\tCD\t9\tnmod\non\tIN\t17\tcase\nnasdaq\tNN\t9\tnmod\n\n"
    }
   ],
   "source": [
    "# dependency parsing\n",
    "print(\"\\nDependency Parsing Sentence 1\\n\")\n",
    "dependency_parser = CoreNLPDependencyParser(url=core_nlp_url)\n",
    "parse, = dependency_parser.raw_parse(train_data[\"s1\"]['sentences'][r])\n",
    "print(parse.to_conll(4))\n",
    "\n",
    "print(\"\\nDependency Parsing Sentence 2\\n\")\n",
    "dependency_parser = CoreNLPDependencyParser(url=core_nlp_url)\n",
    "parse, = dependency_parser.raw_parse(train_data[\"s2\"]['sentences'][r])\n",
    "print(parse.to_conll(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Parsing\n",
    "https://www.nltk.org/api/nltk.parse.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nFull syntactic parse tree for sentence 1: \n                                                         ROOT                                                  \n                                                          |                                                     \n                                                          S                                                    \n              ____________________________________________|_____                                                \n             |                                                  VP                                             \n             |             _____________________________________|_________________                              \n             |            |      |       |           |                            S                            \n             |            |      |       |           |                            |                             \n             |            |      |       |           |                            VP                           \n             |            |      |       |           |     _______________________|_______                      \n             |            |      |       |           |    |         |                     PP                   \n             |            |      |       |           |    |         |          ___________|____                 \n             |            |      |       |           |    |         |         |                NP              \n             |            |      |       |           |    |         |         |        ________|___             \n             NP           |      |       |           |    |         |         |       |            PP          \n          ___|____        |      |       |           |    |         |         |       |         ___|___         \n         NP       |       |     PRT      NP          |    |         NP        |       NP       |       NP      \n    _____|___     |       |      |    ___|_____      |    |      ___|____     |    ___|___     |    ___|____    \n   NN       POS  NNS     VBD     RP  CD        NN    ,   VBG    CD      NNS   TO  $       CD   IN  DT       NN \n   |         |    |       |      |   |         |     |    |     |        |    |   |       |    |   |        |   \ngemstar      's shares gathered  up 2.6     percent  ,  adding  14     cents  to  $      5.49  at the     close\n\n"
    }
   ],
   "source": [
    "# syntactic parsing\n",
    "print(\"\\nFull syntactic parse tree for sentence 1: \")\n",
    "syntactic_parser = CoreNLPParser(url=core_nlp_url)\n",
    "s1_tree = next(syntactic_parser.raw_parse(train_data[\"s1\"]['sentences'][r]))\n",
    "s1_tree.pretty_print()\n",
    "\n",
    "# type(s1_tree)\n",
    "# s1_parse_tree_file = open(\"./output/s1_parse_tree.txt\", \"w\") \n",
    "# s1_parse_tree_file.write(str(s1_tree))\n",
    "# s1_parse_tree_file.close()\n",
    "\n",
    "f = open(\"./output/s1_parse_tree.txt\", \"w\", encoding=\"utf-8\")\n",
    "s1_tree.pretty_print(stream=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nFull syntactic parse tree for sentence 1: \n                                                       ROOT                                                         \n                                                        |                                                            \n                                                        S                                                           \n          ______________________________________________|_____                                                       \n         |                                                    VP                                                    \n         |            ________________________________________|___________________                                   \n         |           |     |         |            |                               S                                 \n         |           |     |         |            |                               |                                  \n         |           |     |         |            |                               VP                                \n         |           |     |         |            |      _________________________|___                               \n         |           |     |         |            |     |     |                       NP                            \n         |           |     |         |            |     |     |        _______________|_______                       \n         |           |     |         |            |     |     |       |                       PP                    \n         |           |     |         |            |     |     |       |            ___________|____                  \n         |           |     |         |            |     |     |       |           |                NP               \n         |           |     |         |            |     |     |       |           |        ________|_______          \n         |           |     |         PP           |     |     |       |           |       |                PP       \n         |           |     |      ___|___         |     |     |       |           |       |             ___|____     \n         NP          |    ADVP   |       NP       |     |    PRT      NP          |       NP           |        NP  \n    _____|____       |     |     |    ___|___     |     |     |    ___|_____      |    ___|___         |        |    \n   JJ        NNS    VBD   RBR    IN  DT      NN   ,    VBG    RP  CD        NN    IN  $       CD       IN       NN  \n   |          |      |     |     |   |       |    |     |     |   |         |     |   |       |        |        |    \ngemstar     shares moved higher  on the     news  ,  closing  up 2.6     percent  at  $      5.49      on     nasdaq\n\n"
    }
   ],
   "source": [
    "# syntactic parsing\n",
    "print(\"\\nFull syntactic parse tree for sentence 1: \")\n",
    "syntactic_parser = CoreNLPParser(url=core_nlp_url)\n",
    "s1_tree = next(syntactic_parser.raw_parse(train_data[\"s2\"]['sentences'][r]))\n",
    "s1_tree.pretty_print()\n",
    "\n",
    "f = open(\"./output/s2_parse_tree.txt\", \"w\", encoding=\"utf-8\")\n",
    "s1_tree.pretty_print(stream=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\n***********************************************************************************\ngemstar NN\n\nSynonyms:  []\n\nHypernyms:  []\n\nHyponyms:  []\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\nshares POS\n\nSynonyms:  ['share', 'portion', 'part', 'percentage', 'parcel', 'contribution', 'plowshare', 'ploughshare', 'partake', 'partake_in', 'divvy_up', 'portion_out', 'apportion', 'deal']\n\nHypernyms:  ['assets', 'stock_certificate', 'stock', 'allotment', 'apportionment', 'apportioning', 'allocation', 'parceling', 'parcelling', 'assignation', 'attempt', 'effort', 'endeavor', 'endeavour', 'try', 'wedge', 'overlap', 'use', 'utilize', 'utilise', 'apply', 'employ', 'get', 'acquire', 'distribute', 'give_out', 'hand_out', 'pass_out', 'communicate', 'intercommunicate']\n\nHyponyms:  ['allotment', 'allocation', 'allowance', 'cut', 'dispensation', 'dole', 'interest', 'stake', 'profit_sharing', 'ration', 'slice', 'piece', 'split', 'tranche', 'way', 'end', 'osculate', 'partake', 'communalize', 'communalise', 'double_up', 'pool', 'cut_in']\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\nmoved NNS\n\nSynonyms:  ['travel', 'go', 'move', 'locomote', 'displace', 'proceed', 'be_active', 'act', 'affect', 'impress', 'strike', 'motivate', 'actuate', 'propel', 'prompt', 'incite', 'run', 'make_a_motion', 'moved', 'affected', 'stirred', 'touched']\n\nHypernyms:  ['act', 'move', 'change', 'alter', 'vary', 'cause', 'do', 'make', 'affect', 'impress', 'strike', 'sell', 'live', 'propose', 'suggest', 'advise']\n\nHyponyms:  ['accompany', 'advance', 'progress', 'pass_on', 'move_on', 'march_on', 'go_on', 'angle', 'ascend', 'go_up', 'automobile', 'back', 'bang', 'beetle', 'betake_oneself', 'billow', 'bounce', 'jounce', 'breeze', 'caravan', 'career', 'carry', 'circle', 'circulate', 'circuit', 'come', 'come_up', 'crawl', 'creep', 'cruise', 'derail', 'jump', 'descend', 'fall', 'go_down', 'come_down', 'do', 'drag', 'draw', 'drive', 'motor', 'ease', 'ferry', 'float', 'drift', 'be_adrift', 'blow', 'swim', 'flock', 'fly', 'wing', 'follow', 'travel_along', 'forge', 'spurt', 'spirt', 'get_around', 'get_about', 'ghost', 'glide', 'go_around', 'spread', 'hiss', 'whoosh', 'hurtle', 'island_hop', 'lance', 'lurch', 'outflank', 'pace', 'pan', 'pass', 'go_through', 'go_across', 'pass_over', 'overfly', 'play', 'plow', 'plough', 'prance', 'precede', 'lead', 'precess', 'proceed', 'go_forward', 'continue', 'propagate', 'pursue', 'push', 'raft', 'repair', 'resort', 'retreat', 'retrograde', 'return', 'ride', 'sit', 'rise', 'lift', 'arise', 'move_up', 'uprise', 'roll', 'wander', 'swan', 'stray', 'tramp', 'roam', 'cast', 'ramble', 'rove', 'range', 'vagabond', 'round', 'run', 'rush', 'hotfoot', 'hasten', 'hie', 'speed', 'race', 'pelt_along', 'rush_along', 'cannonball_along', 'bucket_along', 'belt_along', 'step_on_it', 'scramble', 'seek', 'shuttle', 'sift', 'ski', 'slice_into', 'slice_through', 'slither', 'slide', 'snowshoe', 'steamer', 'steam', 'step', 'tread', 'err', 'swap', 'swash', 'swing', 'taxi', 'trail', 'shack', 'tram', 'transfer', 'change', 'travel', 'journey', 'move_around', 'travel_by', 'pass_by', 'surpass', 'go_past', 'go_by', 'travel_purposefully', 'travel_rapidly', 'hurry', 'zip', 'trundle', 'turn', 'walk', 'take_the_air', 'weave', 'wind', 'thread', 'meander', 'wend', 'wheel', 'whine', 'whish', 'whisk', 'whistle', 'withdraw', 'pull_away', 'draw_back', 'recede', 'pull_back', 'retire', 'move_back', 'zigzag', 'crank', 'zoom', 'bring_forward', 'agitate', 'vex', 'disturb', 'commove', 'shake_up', 'stir_up', 'raise_up', 'beat', 'flap', 'brandish', 'flourish', 'wave', 'center', 'centre', 'change_hands', 'change_owners', 'chase_away', 'drive_out', 'turn_back', 'drive_away', 'dispel', 'drive_off', 'run_off', 'pass_around', 'distribute', 'dandle', 'disarrange', 'dislocate', 'luxate', 'splay', 'slip', 'displace', 'drop', 'engage', 'mesh', 'lock', 'operate', 'expel', 'throw_out', 'kick_out', 'exteriorize', 'bring_outside', 'flick', 'ruffle', 'riffle', 'fluctuate', 'funnel', 'herd', 'crowd', 'hit', 'strike', 'hustle', 'jar', 'bump_around', 'lateralize', 'launch', 'set_in_motion', 'raise', 'lower', 'take_down', 'let_down', 'get_down', 'bring_down', 'mobilize', 'mobilise', 'overturn', 'tip_over', 'turn_over', 'upset', 'knock_over', 'bowl_over', 'tump_over', 'pour', 'press_down', 'depress', 'propel', 'impel', 'pull', 'force', 'pulse', 'pump', 'put', 'set', 'place', 'pose', 'position', 'lay', 'elevate', 'get_up', 'bring_up', 'rake', 'relocate', 'rock', 'sway', 'revolve', 'rout_out', 'force_out', 'rouse', 'saltate', 'scan', 'send', 'direct', 'separate', 'disunite', 'divide', 'part', 'shift', 'dislodge', 'reposition', 'singsong', 'sink', 'sling', 'spill', 'slop', 'splatter', 'shed', 'disgorge', 'station', 'post', 'stir', 'take_back', 'translate', 'transmit', 'transport', 'channel', 'channelize', 'channelise', 'ship', 'tug', 'unseat', 'unwind', 'wind_off', 'unroll', 'uproot', 'extirpate', 'deracinate', 'root_out', 'upstage', 'wash', 'wedge', 'squeeze', 'wrap', 'twine', 'woosh', 'work', 'arouse', 'assume', 'take', 'take_up', 'pound', 'thump', 'bob', 'bolt', 'brush', 'sweep', 'buck', 'jerk', 'hitch', 'bustle', 'bustle_about', 'cant', 'cant_over', 'tilt', 'slant', 'pitch', 'careen', 'wobble', 'chop', 'churn', 'boil', 'moil', 'roil', 'climb', 'close', 'come_together', 'crash', 'cut', 'cut_to', 'dance', 'trip_the_light_fantastic', 'trip_the_light_fantastic_toe', 'diverge', 'dodge', 'drop_back', 'duck', 'exit', 'go_out', 'get_out', 'leave', 'falter', 'waver', 'fidget', 'flex', 'bend', 'flinch', 'squinch', 'funk', 'cringe', 'shrink', 'wince', 'recoil', 'quail', 'fling', 'flip', 'twitch', 'flow', 'flux', 'flurry', 'grab', 'gravitate', 'heave', 'hit_the_dirt', 'hit_the_deck', 'hop', 'hop_on', 'mount', 'mount_up', 'get_on', 'jump_on', 'climb_on', 'bestride', 'jolt', 'leap', 'bound', 'spring', 'jump_off', 'linger', 'dawdle', 'list', 'lean', 'lunge', 'hurl', 'thrust', 'make_way', 'mill', 'mill_about', 'mill_around', 'mope', 'mope_around', 'move_back_and_forth', 'move_involuntarily', 'move_reflexively', 'move_over', 'give_way', 'give', 'ease_up', 'yield', 'nod', 'pulsate', 'quiver', 'putter', 'potter', 'potter_around', 'putter_around', 'quicken', 'reach', 'reach_out', 'reciprocate', 'undulate', 'feed', 'course', 'seesaw', 'split', 'shake', 'sidle', 'sashay', 'snap', 'click', 'startle', 'start', 'steal', 'budge', 'streak', 'stretch', 'stretch_out', 'strike_out', 'stumble', 'trip', 'sail', 'swoop', 'teeter', 'totter', 'throw', 'thunder', 'vibrate', 'wallow', 'welter', 'wamble', 'waggle', 'whirl', 'tumble', 'whirl_around', 'coggle', 'writhe', 'wrestle', 'wriggle', 'worm', 'squirm', 'twist', 'evacuate', 'migrate', 'transmigrate', 'move_in', 'move_out', 'steamroller', 'steamroll', 'venture', 'embark', 'bestir', 'scroll', 'lapse', 'act_on', 'alternate', 'take_turns', 'antagonize', 'antagonise', 'counteract', 'anticipate', 'foresee', 'forestall', 'counter', 'attack', 'aggress', 'begin', 'behave', 'acquit', 'bear', 'deport', 'conduct', 'comport', 'coact', 'come_close', 'come_to_the_fore', 'step_forward', 'come_forward', 'step_up', 'step_to_the_fore', 'come_out', 'condescend', 'deign', 'stoop', 'lower_oneself', 'go_along', 'keep', 'persist_in', 'cope', 'get_by', 'make_out', 'make_do', 'contend', 'grapple', 'deal', 'manage', 'court', 'create', 'dally', 'toy', 'flirt', 'dare', 'dispatch', 'do_well', 'had_best', 'effect', 'egotrip', 'evade', 'exert', 'finish_up', 'land_up', 'fetch_up', 'end_up', 'wind_up', 'finish', 'get_around_to', 'go', 'move', 'go_ahead', 'plow_ahead', 'go_off_half-cocked', 'go_off_at_half-cock', 'guard', 'interact', 'interrupt', 'lord_it_over', 'queen_it_over', 'put_on_airs', 'act_superior', 'make_a_point', 'make_sure', 'make_bold', 'presume', 'maneuver', 'manoeuver', 'manoeuvre', 'misbehave', 'misconduct', 'misdemean', 'participate', 'take_part', 'partner', 'perform', 'perpetrate', 'commit', 'play_it_by_ear', 'prosecute', 'rampage', 'react', 'respond', 'oppose', 'repeat', 'take_over', 'reward', 'repay', 'pay_back', 'look_sharp', 'festinate', 'satisfice', 'satisfise', 'set_about', 'go_about', 'approach', 'sneak', 'stampede', 'surprise', 'take_care', 'take_time_by_the_forelock', 'try', 'attempt', 'essay', 'assay', 'use', 'volunteer', 'offer', 'wait', 'hold_off', 'hold_back', 'woo', 'romance', 'solicit', 'alienate', 'awaken', 'cloud', 'trouble', 'engrave', 'hit_home', 'strike_home', 'strike_a_chord', 'strike_a_note', 'impress', 'ingrain', 'instill', 'infect', 'pierce', 'sadden', 'smite', 'strike_dumb', 'sweep_away', 'sweep_off', 'touch', 'zap', 'bluff', 'bluff_out', 'castle', 'check', 'open', 'serve', 'stalemate', 'trump', 'ruff']\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\nhigher VBD\n\nSynonyms:  ['higher', 'high', 'eminent', 'high-pitched', 'in_high_spirits', 'gamey', 'gamy', 'mellow']\n\nHypernyms:  []\n\nHyponyms:  []\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\non RP\n\nSynonyms:  ['on', 'along']\n\nHypernyms:  []\n\nHyponyms:  []\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\nthe CD\n\nSynonyms:  []\n\nHypernyms:  []\n\nHyponyms:  []\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\nnews NN\n\nSynonyms:  ['news', 'intelligence', 'tidings', 'word', 'news_program', 'news_show', 'newsworthiness']\n\nHypernyms:  ['information', 'info', 'broadcast', 'program', 'programme', 'interest', 'interestingness']\n\nHyponyms:  ['good_word', 'latest', 'update', 'business_news', 'coverage', 'reporting', 'reportage', 'hard_news', 'newscast', 'report', 'news_report', 'story', 'account', 'write_up', 'soft_news', 'stop_press', 'television_news']\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\n, ,\n\nSynonyms:  []\n\nHypernyms:  []\n\nHyponyms:  []\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\nclosing VBG\n\nSynonyms:  ['shutting', 'closing', 'conclusion', 'end', 'close', 'ending', 'closure', 'closedown', 'shutdown', 'completion', 'culmination', 'windup', 'mop_up', 'shut', 'close_up', 'fold', 'shut_down', 'close_down', 'conclude', 'come_together', 'fill_up']\n\nHypernyms:  ['motion', 'movement', 'move', 'motility', 'section', 'subdivision', 'approach', 'approaching', 'coming', 'termination', 'ending', 'conclusion', 'change_state', 'turn', 'end', 'terminate', 'stop', 'finish', 'cease', 'trade', 'prosecute', 'engage', 'pursue', 'near', 'come_on', 'go_up', 'draw_near', 'draw_close', 'come_near', 'join', 'bring_together', 'barricade', 'block', 'blockade', 'block_off', 'block_up', 'bar', 'fill', 'complete']\n\nHyponyms:  ['anticlimax', 'bathos', 'epilogue', 'epilog', 'finale', 'coda', 'peroration', 'bank_closing', 'layoff', 'plant_closing', 'consummation', 'finalization', 'finalisation', 'finish', 'finishing', 'follow-through', 'graduation', 'bung', 'draw', 'roll_up', 'seal', 'seal_off', 'shutter', 'slam', 'bang', 'slat', 'snap', 'adjourn', 'withdraw', 'retire', 'coapt', 'conglutinate', 'plug', 'stop_up', 'secure']\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\nup CD\n\nSynonyms:  ['up', 'astir', 'improving', 'upward', 'upwards', 'upwardly']\n\nHypernyms:  ['increase']\n\nHyponyms:  []\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\n2.6 NNS\n\nSynonyms:  []\n\nHypernyms:  []\n\nHyponyms:  []\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\npercent TO\n\nSynonyms:  ['percentage', 'percent', 'per_centum', 'pct']\n\nHypernyms:  ['proportion']\n\nHyponyms:  ['absentee_rate', 'occupancy_rate', 'unemployment_rate', 'vacancy_rate']\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\nat $\n\nSynonyms:  ['astatine', 'At', 'atomic_number_85', 'at']\n\nHypernyms:  ['chemical_element', 'element', 'halogen', 'Laotian_monetary_unit']\n\nHyponyms:  []\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\n$ CD\n\nSynonyms:  []\n\nHypernyms:  []\n\nHyponyms:  []\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\n5.49 IN\n\nSynonyms:  []\n\nHypernyms:  []\n\nHyponyms:  []\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\non DT\n\nSynonyms:  ['on', 'along']\n\nHypernyms:  []\n\nHyponyms:  []\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n\n***********************************************************************************\nnasdaq NN\n\nSynonyms:  ['National_Association_of_Securities_Dealers_Automated_Quotations', 'NASDAQ']\n\nHypernyms:  []\n\nHyponyms:  []\n\nMeronyms (substance):  []\n\nMeronyms (part):  []\n\nHolonyms: []\n"
    }
   ],
   "source": [
    "for tk, tg in zip(train_data[\"s2\"]['tokens'][r], train_data[\"s1\"]['tags'][r]):\n",
    "    \n",
    "    print('\\n***********************************************************************************')\n",
    "    print(tk, tg)\n",
    "    synonyms = []\n",
    "    hypernyms = []\n",
    "    hyponyms = []\n",
    "    substance_meronyms = []\n",
    "    part_meronyms = []\n",
    "    holonyms = []\n",
    "\n",
    "    for syn in wn.synsets(tk):\n",
    "        # Synonyms\n",
    "        for l in syn.lemmas():\n",
    "            if l.name() not in synonyms:\n",
    "                synonyms.append(l.name())\n",
    "\n",
    "        # Hypernyms\n",
    "        for hpr in syn.hypernyms():\n",
    "            for l in hpr.lemmas():\n",
    "                if l.name() not in hypernyms:\n",
    "                    hypernyms.append(l.name())\n",
    "\n",
    "        # Hyponyms\n",
    "        for hpo in syn.hyponyms():\n",
    "            for l in hpo.lemmas():\n",
    "                if l.name() not in hyponyms:\n",
    "                    hyponyms.append(l.name())\n",
    "\n",
    "        # Substance Meronyms\n",
    "        for mrn in syn.substance_meronyms():\n",
    "            for l in mrn.lemmas():\n",
    "                if l.name() not in substance_meronyms:\n",
    "                    substance_meronyms.append(l.name())\n",
    "\n",
    "        # Part Meronyms\n",
    "        for mrn in syn.part_meronyms():\n",
    "            for l in mrn.lemmas():\n",
    "                if l.name() not in part_meronyms:\n",
    "                    part_meronyms.append(l.name())\n",
    "\n",
    "        # Holonyms\n",
    "        for hol in syn.member_holonyms():\n",
    "            for l in hol.lemmas():\n",
    "                if l.name() not in holonyms:\n",
    "                    holonyms.append(l.name())\n",
    "\n",
    "    print('\\nSynonyms: ', synonyms)\n",
    "    print('\\nHypernyms: ', hypernyms)\n",
    "    print('\\nHyponyms: ', hyponyms)\n",
    "    print('\\nMeronyms (substance): ', substance_meronyms)\n",
    "    print('\\nMeronyms (part): ', part_meronyms)\n",
    "    print('\\nHolonyms:', holonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "In our model we used 11 features for each pair of sentences to build our Machine Learning Model:\n",
    "- Cosine Similarity\n",
    "- Spacy (Cosine) Similarity \n",
    "- SIF Similarity\n",
    "- Word Overlap\n",
    "- Normalized Word Overlap\n",
    "- Lemma Overlap\n",
    "- Normalized Lemma Overlap\n",
    "- Synset Overlap\n",
    "- Normalized Synset Overlap\n",
    "- Path Similarity \n",
    "- Named Entity Overlap\n",
    "\n",
    "In the following cells we show some examples of these in use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences for the following demonstrations.\n",
    "\n",
    "# Similar Sentences\n",
    "s1 = 'I enjoy eating apples.'\n",
    "s1_tok = ['I', 'enjoy', 'eating', 'apples']\n",
    "s2 = 'I like munching red apples'\n",
    "s2_tok = ['I', 'like', 'munching', 'red', 'apples']\n",
    "\n",
    "# Dissimilar Sentences\n",
    "s3 = 'My final exam was very difficult.'\n",
    "s3_tok = ['My', 'final', 'exam', 'was', 'very', 'difficult']\n",
    "s4 = 'Your mother smelled of elderberries.'\n",
    "s4_tok = ['Your', 'mother', 'smelled', 'of', 'elderberries']"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cosine Similarity\n",
    "Cosine of embedding vectors in 3D Space. 0-1 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "similar:        0.171     \ndissimilar:     0.0       \nsame:           1.0       \n"
    }
   ],
   "source": [
    "def calc_cosine_similarity(s1, s2):\n",
    "\n",
    "    # remove the stopwords, transform into TF-IDF matrix, then\n",
    "    tfidf_matrix = TfidfVectorizer(\n",
    "        stop_words=\"english\").fit_transform([s1, s2])\n",
    "    \n",
    "    cos_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # print(tfidf_matrix.toarray())\n",
    "\n",
    "    cos_sim = cos_sim_matrix[0][1]\n",
    "\n",
    "    return cos_sim\n",
    "\n",
    "# Close Example\n",
    "print('{:15} {:<10.3}'.format('similar:', calc_cosine_similarity(s1, s2)) )\n",
    "\n",
    "# Different Example\n",
    "print('{:15} {:<10.3}'.format('dissimilar:', calc_cosine_similarity(s3, s4)) )\n",
    "\n",
    "# Same Example\n",
    "print('{:15} {:<10.3}'.format('same:', calc_cosine_similarity(s3, s3)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spacy Cosine Similarity\n",
    "Cosine similarity calculated with the Spacy embeddings (large file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "similar:        0.853     \ndissimilar:     0.637     \nsame:           1.0       \n"
    }
   ],
   "source": [
    "def calc_spacy_sim(s1, s2):\n",
    "    s2 = nlp(s2)\n",
    "    s1 = nlp(s1)\n",
    "    return s1.similarity(s2)\n",
    "\n",
    "# Close Example\n",
    "print('{:15} {:<10.3}'.format('similar:', calc_spacy_sim(s1, s2)) )\n",
    "\n",
    "# Different Example\n",
    "print('{:15} {:<10.3}'.format('dissimilar:', calc_spacy_sim(s3, s4)) )\n",
    "\n",
    "# Same Example\n",
    "print('{:15} {:<10.3}'.format('same:', calc_spacy_sim(s3, s3)) )    "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-49-634ea0e7b569>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-634ea0e7b569>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    Smooth Inverse Frequency is a weighted average of word vectors.\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "## Smooth Inverse Frequency (SIF) Similarity\n",
    "Smooth Inverse Frequency is a weighted average of word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "similar:        0.707     \ndissimilar:     0.0       \nsame:           1.0       \n"
    }
   ],
   "source": [
    "def frequency_distribution(corpus):\n",
    "    s1_toks = corpus['s1']['tokens']\n",
    "    s2_toks = corpus['s2']['tokens']    \n",
    "    freq_dist = FreqDist()\n",
    "    for i in range(len(s1_toks)):\n",
    "        for token in (s1_toks[i] + s2_toks[i]):\n",
    "            freq_dist[token.lower()] += 1\n",
    "    return freq_dist\n",
    "\n",
    "freq_dist = frequency_distribution(train_data)\n",
    "\n",
    "\n",
    "def calc_sif_similarity(s1, s2, a = .001):\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform([s1, s2])\n",
    "    X_arr = X.toarray()\n",
    "    sif_matrix = []\n",
    "    for i in range(0, len(X_arr)):\n",
    "        sif_arr = []\n",
    "        for j in range(0, len(X_arr[i])):\n",
    "            word = vectorizer.get_feature_names()[j]\n",
    "            w = a / (a + freq_dist[word])\n",
    "            v = X_arr[i][j]\n",
    "            sif_arr.append(v*w)\n",
    "        sif_matrix.append(sif_arr)\n",
    "    sif_cos_sim_matrix = cosine_similarity(sif_matrix, sif_matrix)\n",
    "    sif_cos_sim = sif_cos_sim_matrix[0][1]\n",
    "    return sif_cos_sim\n",
    "\n",
    "# Close Example\n",
    "print('{:15} {:<10.3}'.format('similar:', calc_sif_similarity(s1, s2)) )\n",
    "\n",
    "# Different Example\n",
    "print('{:15} {:<10.3}'.format('dissimilar:', calc_sif_similarity(s3, s4)) )\n",
    "\n",
    "# Same Example\n",
    "print('{:15} {:<10.3}'.format('same:', calc_sif_similarity(s3, s3)) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple Word Overlap (Raw and Normalized)\n",
    "How many words do the two sentences have in common? This doesn't count stopwords or duplicates.\n",
    "\n",
    "## Simple Lemma Overlap (Raw and Normalized)\n",
    "How many lemmas do the two sentences have in common? This doesn't count stopwords or duplicates.\n",
    "\n",
    "This is the same function as simple word overlap, except that it takes in lemmas vs tokens and thus would likely have more overlap.\n",
    "\n",
    "*Note* This method takes tokenized sentences so I've hard coded those in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "similar:        2     0.444     \ndessimilar:     0     0.0       \nsame:           4     1.0       \n"
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_sentence_list = train_data['s1']['tokens']+train_data['s2']['tokens']\n",
    "words_filtered = []\n",
    "\n",
    "# print(words)\n",
    "\n",
    "# looking through I've noticed there are a number of stop-words that can be added to the set\n",
    "stop_words.add(',')\n",
    "stop_words.add('``')\n",
    "stop_words.add(\"n't\")\n",
    "\n",
    "for tsl in tokenized_sentence_list:\n",
    "    for w in tsl:\n",
    "        if w not in stop_words and w not in words_filtered:\n",
    "            words_filtered.append(w)\n",
    "\n",
    "def remove_duplicate_tokens(token_list):\n",
    "    blank_list = []\n",
    "    for w in token_list:\n",
    "        if w not in blank_list:\n",
    "            blank_list.append(w)\n",
    "    return blank_list\n",
    "\n",
    "def remove_stopwords(token_list):\n",
    "    blank_list = []\n",
    "    for w in token_list:\n",
    "        if w not in stop_words:\n",
    "            blank_list.append(w)\n",
    "    return blank_list\n",
    "\n",
    "def calc_basic_overlap(s1_tokens, s2_tokens):\n",
    "    s1_tokens = remove_stopwords(s1_tokens)\n",
    "    s1_tokens = remove_duplicate_tokens(s1_tokens)\n",
    "\n",
    "    s2_tokens = remove_stopwords(s2_tokens)\n",
    "    s2_tokens = remove_duplicate_tokens(s2_tokens)\n",
    "        \n",
    "    overlap = 0\n",
    "    encountered_words = []\n",
    "    for word in (s1_tokens+s2_tokens):\n",
    "        try:\n",
    "            if word in encountered_words: # we know we have found an overlap\n",
    "                overlap += 1\n",
    "            encountered_words.append(word)\n",
    "        except ValueError:\n",
    "            # print(word + ' not found in lexicon. Skipping...')\n",
    "            continue\n",
    "\n",
    "    avg_sentence_len = len(s1_tokens+s2_tokens) / 2\n",
    "    \n",
    "    overlap_normlalized = overlap / avg_sentence_len\n",
    "    return overlap, overlap_normlalized\n",
    "\n",
    "s1s2_word_raw, s1s2_word_norm = calc_basic_overlap(s1_tok, s2_tok)\n",
    "s3s4_word_raw, s3s4_word_norm = calc_basic_overlap(s3_tok, s4_tok)\n",
    "s3s3_word_raw, s3s3_word_norm = calc_basic_overlap(s3_tok, s3_tok)\n",
    "\n",
    "# Close Example\n",
    "print('{:15} {:<5} {:<10.3}'.format('similar:',  s1s2_word_raw, s1s2_word_norm))\n",
    "\n",
    "# Different Example\n",
    "print('{:15} {:<5} {:<10.3}'.format('dessimilar:',  s3s4_word_raw, s3s4_word_norm))\n",
    "\n",
    "# Same Example\n",
    "print('{:15} {:<5} {:<10.3}'.format('same:',  s3s3_word_raw, s3s3_word_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Synset Overlap\n",
    "\n",
    "\n",
    "*NOTE* This feature also ingests tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "similar:        13    0.306     \ndessimilar:     0     0.0       \nsame:           13    1.0       \n"
    }
   ],
   "source": [
    "def calc_synset_overlap(s1_tokens, s2_tokens):\n",
    "    s1_tokens = remove_stopwords(s1_tokens)\n",
    "    s1_tokens = remove_duplicate_tokens(s1_tokens)\n",
    "\n",
    "    s2_tokens = remove_stopwords(s2_tokens)\n",
    "    s2_tokens = remove_duplicate_tokens(s2_tokens)\n",
    "    \n",
    "#     print(s2_tokens)\n",
    "#     print(s1_tokens)\n",
    "\n",
    "    s1_spread = []\n",
    "    s2_spread = []\n",
    "    \n",
    "    for word in s1_tokens:\n",
    "        for synset in wn.synsets(word):\n",
    "            for i in range(0, len(synset.lemmas())):\n",
    "                syn_word = synset.lemmas()[i].name()\n",
    "                if syn_word not in s1_spread:\n",
    "                    s1_spread.append(syn_word)\n",
    "\n",
    "    for word in s2_tokens:\n",
    "        for synset in wn.synsets(word):\n",
    "            for i in range(0, len(synset.lemmas())):\n",
    "                syn_word = synset.lemmas()[i].name()\n",
    "                if syn_word not in s2_spread:\n",
    "                    s2_spread.append(syn_word)         \n",
    "    \n",
    "    return calc_basic_overlap(s1_spread, s2_spread)\n",
    "    \n",
    "s1s2_syn_raw, s1s2_syn_norm = calc_synset_overlap(s1_tok, s2_tok)\n",
    "s3s4_syn_raw, s3s4_syn_norm = calc_synset_overlap(s3_tok, s4_tok)\n",
    "s3s3_syn_raw, s3s3_syn_norm = calc_synset_overlap(s3_tok, s3_tok)\n",
    "\n",
    "# Close Example\n",
    "print('{:15} {:<5} {:<10.3}'.format('similar:',  s1s2_syn_raw, s1s2_syn_norm))\n",
    "\n",
    "# Different Example\n",
    "print('{:15} {:<5} {:<10.3}'.format('dessimilar:',  s3s4_syn_raw, s3s4_syn_norm))\n",
    "\n",
    "# Same Example\n",
    "print('{:15} {:<5} {:<10.3}'.format('same:',  s3s3_syn_raw, s3s3_syn_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Path Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "similar:        0.0       \ndissimilar:     0.0       \nsame:           0.0       \n"
    }
   ],
   "source": [
    "def get_synsets(sentence1, sentence2):\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tag_to_synset(sentence1, *tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tag_to_synset(sentence2, *tagged_word) for tagged_word in sentence2]\n",
    "    \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [syn for syn in synsets1 if syn]\n",
    "    synsets2 = [syn for syn in synsets2 if syn]\n",
    "\n",
    "    return synsets1, synsets2\n",
    "\n",
    "def postag_to_synsettag(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if tag.startswith('V') or tag == \"MD\":\n",
    "        return 'v'\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return\n",
    "\n",
    "def tag_to_synset(sent, word, tag):\n",
    "    wn_tag = postag_to_synsettag(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            return lesk(sent, word, wn_tag)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "def sentence_path_similarity(sentence1, sentence2):\n",
    "    synsets1, synsets2 = get_synsets(sentence1, sentence2)\n",
    "    score, count = 0.0, 0\n",
    "    best = 0.0\n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        for synset2 in synsets2:\n",
    "            if synset.path_similarity(synset2) is not None and synset.path_similarity(synset2) > best:\n",
    "                try:\n",
    "                    best = synset.path_similarity(synset2)\n",
    "                except TypeError:\n",
    "                    continue\n",
    "        # Check that the similarity could have been computed\n",
    "        if best is not None:\n",
    "            score += best\n",
    "            count += 1\n",
    "    # Average the values\n",
    "    try:\n",
    "        score /= count\n",
    "    except:\n",
    "        return 0.0\n",
    "    return score\n",
    "\n",
    "\n",
    "def symmetric_sentence_path_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\n",
    "    return (sentence_path_similarity(sentence1, sentence2) + sentence_path_similarity(sentence2, sentence1)) / 2     \n",
    "\n",
    "# Close Example\n",
    "print('{:15} {:<10.3}'.format('similar:', symmetric_sentence_path_similarity(s1, s2)) )\n",
    "\n",
    "# Different Example\n",
    "print('{:15} {:<10.3}'.format('dissimilar:', symmetric_sentence_path_similarity(s3, s4)) )\n",
    "\n",
    "# Same Example\n",
    "print('{:15} {:<10.3}'.format('same:', symmetric_sentence_path_similarity(s3, s3)) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Named Entity Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "similar:        0.0       \ndissimilar:     0.0       \nsame:           0.0       \n"
    }
   ],
   "source": [
    "def named_entity_overlap(s1, s2):    \n",
    "  \n",
    "    sentence_nlp1= nlp(s1)\n",
    "    ner1= [(word.text, word.ent_type_) for word in sentence_nlp1 if word.ent_type_]\n",
    "    \n",
    "    sentence_nlp2=nlp(s2)\n",
    "    ner2 = [(word.text, word.ent_type_) for word in sentence_nlp2 if word.ent_type_]\n",
    "\n",
    "    overlap = []\n",
    "\n",
    "    \n",
    "    da = {k:v for k,v in ner1}\n",
    "    db = {k:v for k,v in ner2}\n",
    "    total_length = len(set(ner1+ner2))\n",
    "    temp = []\n",
    "    for a in da.keys():\n",
    "        for b in db.keys():\n",
    "            if a==b:\n",
    "                temp.insert(len(temp), a)\n",
    "    if total_length != 0:\n",
    "        overlap =len(temp)/total_length\n",
    "    else:\n",
    "        overlap = 0.0\n",
    "\n",
    "    return overlap\n",
    "\n",
    "# Close Example\n",
    "print('{:15} {:<10.3}'.format('similar:', named_entity_overlap(s1, s2)) )\n",
    "\n",
    "# Different Example\n",
    "print('{:15} {:<10.3}'.format('dissimilar:', named_entity_overlap(s3, s4)) )\n",
    "\n",
    "# Same Example\n",
    "print('{:15} {:<10.3}'.format('same:', named_entity_overlap(s3, s3)) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "- Load in model with Pickle Dump\n",
    "- Preprocess test data\n",
    "- Run it through the model\n",
    "- Output a file of our models guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = \"./model/random_forest.pkl\"\n",
    "test_data = preprocess(\"./data/test-set.txt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/Users/daytonpe/anaconda3/lib/python3.7/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.19.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n/Users/daytonpe/anaconda3/lib/python3.7/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.19.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n"
    }
   ],
   "source": [
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(corpus):\n",
    "   \n",
    "    s1_array = corpus['s1']['sentences']\n",
    "    s2_array = corpus['s2']['sentences']\n",
    "    s1_tokens = corpus['s1']['tokens']\n",
    "    s2_tokens = corpus['s2']['tokens']\n",
    "    s1_lemmas = corpus['s1']['lemmas']\n",
    "    s2_lemmas = corpus['s2']['lemmas']\n",
    "    s1_ls = corpus['s2']['ls']\n",
    "    s2_ls = corpus['s2']['ls']\n",
    "    \n",
    "    data = []\n",
    "    for i in range(0, len(s1_array)):\n",
    "        cos_sim = calc_cosine_similarity(s1_array[i], s2_array[i])\n",
    "        sif_sim = calc_sif_similarity(s1_array[i], s2_array[i])\n",
    "        w_overlap, w_norm_overlap = calc_basic_overlap(s1_tokens[i], s2_tokens[i])\n",
    "        l_overlap, l_norm_overlap = calc_basic_overlap(s1_lemmas[i], s2_lemmas[i])\n",
    "        spacy_sim = calc_spacy_sim(s1_array[i], s2_array[i])\n",
    "        syn_overlap, normalized_syn_overlap = calc_synset_overlap(s1_tokens[i], s2_tokens[i])\n",
    "        path_similarity = symmetric_sentence_path_similarity(s1_ls[i], s2_ls[i]) \n",
    "        ne_overlap = named_entity_overlap(s1_array[i], s2_array[i])\n",
    "        data.insert(len(data),[len(s1_tokens), len(s2_tokens), w_norm_overlap, l_norm_overlap, spacy_sim, sif_sim, cos_sim, syn_overlap, normalized_syn_overlap, path_similarity, ne_overlap, len(s1_tokens[i]), len(s2_tokens[i])])\n",
    "\n",
    "    return data\n",
    "test_input = pipeline(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([4, 1, 4, 4, 1, 4, 3, 3, 4, 4, 4, 5, 5, 2, 4, 4, 3, 3, 3, 3, 4, 4,\n       4, 4, 4, 4, 4, 2, 3, 4, 4, 4, 3, 3, 4, 5, 4, 4, 3, 3, 3, 5, 4, 4,\n       4, 4, 2, 4, 3, 3, 4, 4, 4, 3, 4, 3, 4, 3, 2, 1, 5, 3, 3, 4, 4, 2,\n       4, 3, 2, 4, 4, 4, 1, 2, 4, 3, 2, 3, 4, 4, 4, 4, 5, 2, 5, 4, 4, 4,\n       3, 3, 3, 4, 4, 3, 3, 5, 3, 5, 2, 4, 4, 3, 4, 4, 4, 5, 5, 3, 5, 3,\n       3, 3, 3, 3, 4, 1, 3, 1, 4, 4, 4, 4, 3, 4, 3, 4, 5, 4, 5, 4, 4, 3,\n       3, 3, 3, 4, 4, 4, 4, 4, 3, 3, 4, 4, 4, 5, 4, 4, 3, 4, 4, 4, 3, 3,\n       3, 3, 5, 4, 4, 4, 4, 2, 4, 5, 3, 4, 3, 3, 4, 4, 5, 4, 5, 3, 2, 2,\n       3, 3, 4, 1, 5, 4, 3, 4, 3, 2, 4, 3, 4, 4, 5, 4, 4, 3, 3, 3, 4, 4,\n       1, 2, 5, 3, 3, 3, 4, 4, 5, 4, 3, 4, 4, 3, 3, 3, 4, 4, 3, 3, 4, 1,\n       3, 3, 4, 4, 3, 1, 4, 4, 4, 4, 4, 3, 4, 4, 1, 5, 3, 4, 4, 3, 4, 3,\n       2, 4, 5, 3, 1, 4, 5, 4, 3, 4, 4, 4, 4, 2, 4, 1, 2, 4, 3, 4, 3, 4,\n       4, 4, 5, 3, 2, 2, 3, 4, 5, 5, 3, 4, 4, 4, 2, 4, 4, 3, 3, 3, 4, 3,\n       4, 4, 4, 4, 3, 4, 4, 3, 4, 4, 4, 4, 3, 5, 4, 3, 4, 5, 3, 4, 4, 3,\n       4, 4, 4, 4, 3, 5, 4, 3, 3, 4, 3, 2, 4, 3, 5, 5, 3, 2, 4, 4, 4, 4,\n       4, 4, 4, 4, 4, 4, 5, 5, 4, 2, 3, 3, 4, 4, 4, 4, 4, 4, 3, 3, 5, 3,\n       3, 4, 4, 3, 4, 4, 3, 4, 3, 3, 3, 3, 4, 3, 4, 3, 1, 2, 3, 5, 3, 3,\n       3, 3, 4, 4, 4, 3, 3, 3, 5, 4, 3, 3, 4, 4, 2, 4, 5, 3, 4, 4, 3, 4,\n       3, 4, 3, 3, 1, 3, 4, 4, 3, 4, 4, 3, 3, 4, 4, 3, 5, 3, 4, 4, 3, 3,\n       3, 4, 1, 2, 4, 4, 4, 3, 4, 4, 4, 5, 5, 3, 3, 3, 4, 3, 4, 4, 3, 3,\n       4, 4, 2, 4, 3, 4, 3, 4, 1, 2, 3, 4, 3, 4, 4, 4, 3, 3, 4, 4, 5, 4,\n       3, 1, 3, 3, 3, 1, 5, 4, 3, 3, 4, 4, 4, 5, 4, 3, 4, 3, 2, 4, 4, 4,\n       4, 4, 3, 4, 4, 4, 3, 3, 3, 4, 3, 4, 4, 1, 4, 4, 3, 4, 2, 4, 4, 5,\n       3, 5, 3, 4, 1, 4, 5, 4, 2, 3, 4, 4, 2, 4, 4, 4, 4, 4, 5, 4, 3, 4,\n       4, 3, 3, 3, 4, 4, 3, 1, 4, 4, 1, 4, 4, 3, 5, 4, 5, 5, 4, 4, 3, 5,\n       4, 4, 4, 3, 4, 4, 1, 4, 4, 4, 3, 4, 4, 3, 4, 3, 2, 4, 4, 4, 4, 4,\n       4, 4, 3, 4, 4, 5, 3, 4, 4, 3, 3, 4, 4, 4, 3, 4, 4, 3, 3, 3, 4, 5,\n       4, 3, 4, 4, 4, 4, 4, 4, 4, 3, 4, 3, 4, 5, 5, 4, 5, 3, 3, 5, 4, 4,\n       4, 4, 3, 4, 3, 3, 4, 2, 3, 4, 4, 4, 4, 4, 3, 3, 4, 3, 2, 4, 3, 4,\n       3, 4, 3, 3, 4, 5, 4, 3, 2, 5, 3, 4, 4, 3, 3, 4, 4, 3, 3, 2, 2, 3,\n       4, 2, 4, 3, 3, 5, 3, 3, 5, 4, 3, 5, 3, 4, 3, 3, 3, 4, 3, 5, 4, 3,\n       1, 3, 4, 4, 3, 4, 3, 5, 1, 3, 4, 3, 4, 5, 4, 3, 4, 4, 4, 4, 3, 4,\n       4, 3, 1, 2, 3, 3, 4, 4, 3, 4, 3, 4, 4, 5, 4, 3, 4, 4, 4, 3, 3, 3,\n       4, 4, 3, 3, 2, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3,\n       3, 3], dtype=int32)"
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}