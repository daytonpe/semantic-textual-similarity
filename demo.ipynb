{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 6320 Natural Language Processing\n",
    "## Shruti Agrawal & Pat Dayton\n",
    "\n",
    "This notebook demos our code for Tasks 1 & 2 of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# SPACY IMPORT\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# CoreNLP setup\n",
    "core_nlp_url = 'http://localhost:9000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Stanford CoreNLP Server\n",
    "In another console run the script below in order to start the Stanford CoreNLP Server on port 9000. We will hit this API in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#java -mx4g -cp \"./corenlp/*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Parse the Corpus\n",
    "First read in the corpus and do basic parsing to split out the first sentence, second sentence, and score for each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(fileName):\n",
    "    \"\"\"Read in the file, strip out sentence 1, sentence 2, and score\"\"\"\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    score = []\n",
    "    file = open(fileName, encoding=\"utf8\")\n",
    "    text = file.readline()\n",
    "    text = file.read()\n",
    "    \n",
    "    # loop to extract a set of two sentences\n",
    "    for sentence in text.split('\\n'):\n",
    "\n",
    "        # creating two separate lists of the sentences\n",
    "        # '.rstrip('.') only removes the last period in the sentence\n",
    "        \n",
    "        s1.insert(len(s1), (sentence.split('\\t')[1].lower()).rstrip('.'))\n",
    "        s2.insert(len(s1), (sentence.split('\\t')[2].lower()).rstrip('.'))\n",
    "        \n",
    "        # inserting the score as a separate lists\n",
    "        score.insert(len(s1), (sentence.split('\\t')[3]))\n",
    "\n",
    "    # print(s1)\n",
    "    return s1, s2, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(fileName):\n",
    "\n",
    "    s1, s2, scores = readData(fileName)\n",
    "    s1_toks = []\n",
    "    s2_toks = []\n",
    "\n",
    "    # tokenizing and tagging\n",
    "    s1_tags = []\n",
    "    s2_tags = []\n",
    "\n",
    "    for sentence in s1:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        s1_toks.insert(len(s1_toks), tokens)\n",
    "        s1_tags.insert(\n",
    "            len(s1_tags), nltk.pos_tag(tokens))\n",
    "\n",
    "    for sentence in s2:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        s2_toks.insert(len(s2_toks), tokens)\n",
    "        s2_tags.insert(\n",
    "            len(s2_tags), nltk.pos_tag(tokens))\n",
    "    \n",
    "    # Remove the unnecessary tuple and keep just the tags\n",
    "    for i, tag_list in enumerate(s1_tags):\n",
    "        s1_tags[i] = [tup[1] for tup in tag_list]\n",
    "    for i, tag_list in enumerate(s2_tags):\n",
    "        s2_tags[i] = [tup[1] for tup in tag_list]\n",
    "\n",
    "    # lemmatizing\n",
    "    s1_lemmas = []\n",
    "    s2_lemmas = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for sentence in s1_toks:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        s1_lemmas.insert(\n",
    "            len(s1_lemmas), sentence_components)\n",
    "\n",
    "    for sentence in s2_toks:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        s2_lemmas.insert(\n",
    "            len(s2_lemmas), sentence_components)\n",
    "\n",
    "        \n",
    "    # Zipping it all together into one object for each word\n",
    "    s1_word_lists = []\n",
    "    s2_word_lists = []\n",
    "    \n",
    "    for tok_list, lem_list, tag_list in zip(s1_toks, s1_lemmas, s1_tags):\n",
    "        sentence_words = []\n",
    "        for tok, lem, tag in zip(tok_list, lem_list, tag_list):\n",
    "            word = {}\n",
    "            word['tok'] = tok\n",
    "            word['lem'] = lem\n",
    "            word['tag'] = tag\n",
    "            sentence_words.append(word)\n",
    "        s1_word_lists.append(sentence_words) \n",
    "        \n",
    "    for tok_list, lem_list, tag_list in zip(s2_toks, s2_lemmas, s2_tags):\n",
    "        sentence_words = []\n",
    "        for tok, lem, tag in zip(tok_list, lem_list, tag_list):\n",
    "            word = {}\n",
    "            word['tok'] = tok\n",
    "            word['lem'] = lem\n",
    "            word['tag'] = tag\n",
    "            sentence_words.append(word)\n",
    "        s2_word_lists.append(sentence_words)  \n",
    "              \n",
    "    \n",
    "    # Create a corpus object to represent our corpus\n",
    "    corpus = {}\n",
    "    corpus[\"s1\"] = {}\n",
    "    corpus[\"s2\"] = {}\n",
    "    corpus['scores'] = [int(i) for i in scores]\n",
    "    \n",
    "    corpus[\"s1\"][\"sentences\"] = s1\n",
    "    corpus[\"s2\"][\"sentences\"] = s2\n",
    "    \n",
    "    corpus[\"s1\"][\"tokens\"] = s1_toks\n",
    "    corpus[\"s2\"][\"tokens\"] = s2_toks\n",
    "    \n",
    "    corpus[\"s1\"][\"lemmas\"] = s1_lemmas\n",
    "    corpus[\"s2\"][\"lemmas\"] = s2_lemmas\n",
    "    \n",
    "    corpus[\"s1\"][\"tags\"] = s1_tags\n",
    "    corpus[\"s2\"][\"tags\"] = s2_tags\n",
    "    \n",
    "    corpus[\"s1\"][\"words\"] = s1_word_lists\n",
    "    corpus[\"s2\"][\"words\"] = s2_word_lists\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 Example Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess(\"./data/train-set.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1484\n1484\n"
    }
   ],
   "source": [
    "print(len(train_data[\"s1\"]['sentences']))\n",
    "print(len(train_data[\"s2\"]['sentences']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "ROW 286 FROM TEST DATA\n\nSentence 1\n\nRaw:  gemstar's shares gathered up 2.6 percent, adding 14 cents to $5.49 at the close\nSentence 2\n\nRaw:  gemstar shares moved higher on the news, closing up 2.6 percent at $5.49 on nasdaq\nScore:  4\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tokens</th>\n      <th>Lemmas</th>\n      <th>Tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gemstar</td>\n      <td>gemstar</td>\n      <td>NN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>'s</td>\n      <td>'s</td>\n      <td>POS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>shares</td>\n      <td>share</td>\n      <td>NNS</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>gathered</td>\n      <td>gathered</td>\n      <td>VBD</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>up</td>\n      <td>up</td>\n      <td>RP</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2.6</td>\n      <td>2.6</td>\n      <td>CD</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>percent</td>\n      <td>percent</td>\n      <td>NN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>,</td>\n      <td>,</td>\n      <td>,</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>adding</td>\n      <td>adding</td>\n      <td>VBG</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>14</td>\n      <td>14</td>\n      <td>CD</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>cents</td>\n      <td>cent</td>\n      <td>NNS</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>to</td>\n      <td>to</td>\n      <td>TO</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>$</td>\n      <td>$</td>\n      <td>$</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>5.49</td>\n      <td>5.49</td>\n      <td>CD</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>at</td>\n      <td>at</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>the</td>\n      <td>the</td>\n      <td>DT</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>close</td>\n      <td>close</td>\n      <td>NN</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "      Tokens    Lemmas Tags\n0    gemstar   gemstar   NN\n1         's        's  POS\n2     shares     share  NNS\n3   gathered  gathered  VBD\n4         up        up   RP\n5        2.6       2.6   CD\n6    percent   percent   NN\n7          ,         ,    ,\n8     adding    adding  VBG\n9         14        14   CD\n10     cents      cent  NNS\n11        to        to   TO\n12         $         $    $\n13      5.49      5.49   CD\n14        at        at   IN\n15       the       the   DT\n16     close     close   NN"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Tokens</th>\n      <th>Lemmas</th>\n      <th>Tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>gemstar</td>\n      <td>gemstar</td>\n      <td>NN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>shares</td>\n      <td>share</td>\n      <td>NNS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>moved</td>\n      <td>moved</td>\n      <td>VBD</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>higher</td>\n      <td>higher</td>\n      <td>RBR</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>on</td>\n      <td>on</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>the</td>\n      <td>the</td>\n      <td>DT</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>news</td>\n      <td>news</td>\n      <td>NN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>,</td>\n      <td>,</td>\n      <td>,</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>closing</td>\n      <td>closing</td>\n      <td>VBG</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>up</td>\n      <td>up</td>\n      <td>RP</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>2.6</td>\n      <td>2.6</td>\n      <td>CD</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>percent</td>\n      <td>percent</td>\n      <td>NN</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>at</td>\n      <td>at</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>$</td>\n      <td>$</td>\n      <td>$</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>5.49</td>\n      <td>5.49</td>\n      <td>CD</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>on</td>\n      <td>on</td>\n      <td>IN</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>nasdaq</td>\n      <td>nasdaq</td>\n      <td>NNS</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "     Tokens   Lemmas Tags\n0   gemstar  gemstar   NN\n1    shares    share  NNS\n2     moved    moved  VBD\n3    higher   higher  RBR\n4        on       on   IN\n5       the      the   DT\n6      news     news   NN\n7         ,        ,    ,\n8   closing  closing  VBG\n9        up       up   RP\n10      2.6      2.6   CD\n11  percent  percent   NN\n12       at       at   IN\n13        $        $    $\n14     5.49     5.49   CD\n15       on       on   IN\n16   nasdaq   nasdaq  NNS"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r=286\n",
    "\n",
    "tkns1 = train_data[\"s1\"]['tokens'][r]\n",
    "lems1 = train_data[\"s1\"]['lemmas'][r]\n",
    "tags1 = train_data[\"s1\"]['tags'][r]\n",
    "tkns2 = train_data[\"s2\"]['tokens'][r]\n",
    "lems2 = train_data[\"s2\"]['lemmas'][r]\n",
    "tags2 = train_data[\"s2\"]['tags'][r]\n",
    "\n",
    "data1 = []\n",
    "data2 = []\n",
    "\n",
    "for i in range(0, len(tkns1)):\n",
    "    data1.append([tkns1[i], lems1[i], tags1[i]])\n",
    "    \n",
    "for i in range(0, len(tkns2)):\n",
    "    data2.append([tkns2[i], lems2[i], tags2[i]])\n",
    "    \n",
    "df1 = pd.DataFrame(\n",
    "    data1, \n",
    "    columns = ['Tokens', 'Lemmas', 'Tags']) \n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "    data2, \n",
    "    columns = ['Tokens', 'Lemmas', 'Tags']) \n",
    "\n",
    "\n",
    "print('ROW {} FROM TEST DATA\\n'.format(r))\n",
    "print('Sentence 1\\n')\n",
    "print('Raw: ', train_data[\"s1\"]['sentences'][r])\n",
    "display(df1)\n",
    "print('Sentence 2\\n')\n",
    "print('Raw: ', train_data[\"s2\"]['sentences'][r])\n",
    "display(df2)\n",
    "print('Score: ', train_data[\"scores\"][r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "\nDependency Parsing Sentence 1\n\n"
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='localhost', port=9000): Max retries exceeded with url: /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cdepparse%22%2C+%22ssplit.eolonly%22%3A+%22true%22%2C+%22tokenize.whitespace%22%3A+%22false%22%7D (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1a4dede080>: Failed to establish a new connection: [Errno 61] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 159\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 168\u001b[0;31m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x1a4dede080>: Failed to establish a new connection: [Errno 61] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 398\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=9000): Max retries exceeded with url: /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cdepparse%22%2C+%22ssplit.eolonly%22%3A+%22true%22%2C+%22tokenize.whitespace%22%3A+%22false%22%7D (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1a4dede080>: Failed to establish a new connection: [Errno 61] Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-cb9496248336>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDependency Parsing Sentence 1\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdependency_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCoreNLPDependencyParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcore_nlp_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mparse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdependency_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"s1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_conll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/parse/corenlp.py\u001b[0m in \u001b[0;36mraw_parse\u001b[0;34m(self, sentence, properties, *args, **kwargs)\u001b[0m\n\u001b[1;32m    229\u001b[0m         return next(\n\u001b[1;32m    230\u001b[0m             self.raw_parse_sents(\n\u001b[0;32m--> 231\u001b[0;31m                 \u001b[0;34m[\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m             )\n\u001b[1;32m    233\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/parse/corenlp.py\u001b[0m in \u001b[0;36mraw_parse_sents\u001b[0;34m(self, sentences, verbose, properties, *args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \"\"\"\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0mparsed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_properties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparsed_sent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparsed_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/nltk/parse/corenlp.py\u001b[0m in \u001b[0;36mapi_call\u001b[0;34m(self, data, properties, timeout)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'properties'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_properties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m         )\n\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, url, data, json, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m         \"\"\"\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'POST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='localhost', port=9000): Max retries exceeded with url: /?properties=%7B%22outputFormat%22%3A+%22json%22%2C+%22annotators%22%3A+%22tokenize%2Cpos%2Clemma%2Cssplit%2Cdepparse%22%2C+%22ssplit.eolonly%22%3A+%22true%22%2C+%22tokenize.whitespace%22%3A+%22false%22%7D (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1a4dede080>: Failed to establish a new connection: [Errno 61] Connection refused'))"
     ]
    }
   ],
   "source": [
    "# dependency parsing\n",
    "print(\"\\nDependency Parsing Sentence 1\\n\")\n",
    "dependency_parser = CoreNLPDependencyParser(url=core_nlp_url)\n",
    "parse, = dependency_parser.raw_parse(train_data[\"s1\"]['sentences'][r])\n",
    "print(parse.to_conll(4))\n",
    "\n",
    "print(\"\\nDependency Parsing Sentence 2\\n\")\n",
    "dependency_parser = CoreNLPDependencyParser(url=core_nlp_url)\n",
    "parse, = dependency_parser.raw_parse(train_data[\"s2\"]['sentences'][r])\n",
    "print(parse.to_conll(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Parsing\n",
    "https://www.nltk.org/api/nltk.parse.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntactic parsing\n",
    "print(\"\\nFull syntactic parse tree for sentence 1: \")\n",
    "syntactic_parser = CoreNLPParser(url=core_nlp_url)\n",
    "s1_tree = next(syntactic_parser.raw_parse(train_data[\"s1\"]['sentences'][r]))\n",
    "s1_tree.pretty_print()\n",
    "\n",
    "# type(s1_tree)\n",
    "# s1_parse_tree_file = open(\"./output/s1_parse_tree.txt\", \"w\") \n",
    "# s1_parse_tree_file.write(str(s1_tree))\n",
    "# s1_parse_tree_file.close()\n",
    "\n",
    "f = open(\"./output/s1_parse_tree.txt\", \"w\", encoding=\"utf-8\")\n",
    "s1_tree.pretty_print(stream=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# syntactic parsing\n",
    "print(\"\\nFull syntactic parse tree for sentence 1: \")\n",
    "syntactic_parser = CoreNLPParser(url=core_nlp_url)\n",
    "s1_tree = next(syntactic_parser.raw_parse(train_data[\"s2\"]['sentences'][r]))\n",
    "s1_tree.pretty_print()\n",
    "\n",
    "f = open(\"./output/s2_parse_tree.txt\", \"w\", encoding=\"utf-8\")\n",
    "s1_tree.pretty_print(stream=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tk, tg in zip(train_data[\"s2\"]['tokens'][r], train_data[\"s1\"]['tags'][r]):\n",
    "    \n",
    "    print('\\n***********************************************************************************')\n",
    "    print(tk, ind)\n",
    "    synonyms = []\n",
    "    hypernyms = []\n",
    "    hyponyms = []\n",
    "    substance_meronyms = []\n",
    "    part_meronyms = []\n",
    "    holonyms = []\n",
    "\n",
    "    for syn in wn.synsets(tk):\n",
    "        # Synonyms\n",
    "        for l in syn.lemmas():\n",
    "            if l.name() not in synonyms:\n",
    "                synonyms.append(l.name())\n",
    "\n",
    "        # Hypernyms\n",
    "        for hpr in syn.hypernyms():\n",
    "            for l in hpr.lemmas():\n",
    "                if l.name() not in hypernyms:\n",
    "                    hypernyms.append(l.name())\n",
    "\n",
    "        # Hyponyms\n",
    "        for hpo in syn.hyponyms():\n",
    "            for l in hpo.lemmas():\n",
    "                if l.name() not in hyponyms:\n",
    "                    hyponyms.append(l.name())\n",
    "\n",
    "        # Substance Meronyms\n",
    "        for mrn in syn.substance_meronyms():\n",
    "            for l in mrn.lemmas():\n",
    "                if l.name() not in substance_meronyms:\n",
    "                    substance_meronyms.append(l.name())\n",
    "\n",
    "        # Part Meronyms\n",
    "        for mrn in syn.part_meronyms():\n",
    "            for l in mrn.lemmas():\n",
    "                if l.name() not in part_meronyms:\n",
    "                    part_meronyms.append(l.name())\n",
    "\n",
    "        # Holonyms\n",
    "        for hol in syn.member_holonyms():\n",
    "            for l in hol.lemmas():\n",
    "                if l.name() not in holonyms:\n",
    "                    holonyms.append(l.name())\n",
    "\n",
    "    print('\\nSynonyms: ', synonyms)\n",
    "    print('\\nHypernyms: ', hypernyms)\n",
    "    print('\\nHyponyms: ', hyponyms)\n",
    "    print('\\nMeronyms (substance): ', substance_meronyms)\n",
    "    print('\\nMeronyms (part): ', part_meronyms)\n",
    "    print('\\nHolonyms:', holonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "In our model we used 11 features for each pair of sentences to build our Machine Learning Model:\n",
    "- Cosine Similarity\n",
    "- Spacy (Cosine) Similarity \n",
    "- SIF Similarity\n",
    "- Word Overlap\n",
    "- Normalized Word Overlap\n",
    "- Lemma Overlap\n",
    "- Normalized Lemma Overlap\n",
    "- Synset Overlap\n",
    "- Normalized Synset Overlap\n",
    "- Path Similarity \n",
    "- Named Entity Overlap\n",
    "- Verb Overlap\n",
    "\n",
    "In the following cells we show some examples of these in use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences for the following demonstrations.\n",
    "\n",
    "# Similar Sentences\n",
    "s1 = 'I enjoy eating apples.'\n",
    "s2 = 'I like munching red apples'\n",
    "\n",
    "# Dissimilar Sentences\n",
    "s3 = 'My final exam was very difficult.'\n",
    "s4 = 'Your mother smelled of elderberries.'"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cosine Similarity\n",
    "Cosine of embedding vectors in 3D Space. 0-1 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "similar:        0.171     \ndissimilar:     0.0       \nsame:           1.0       \n"
    }
   ],
   "source": [
    "def calc_cosine_similarity(s1, s2):\n",
    "\n",
    "    # remove the stopwords, transform into TF-IDF matrix, then\n",
    "    tfidf_matrix = TfidfVectorizer(\n",
    "        stop_words=\"english\").fit_transform([s1, s2])\n",
    "    \n",
    "    cos_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # print(tfidf_matrix.toarray())\n",
    "\n",
    "    cos_sim = cos_sim_matrix[0][1]\n",
    "\n",
    "    return cos_sim\n",
    "\n",
    "# Close Example\n",
    "print('{:15} {:<10.3}'.format('similar:', calc_cosine_similarity(s1, s2)) )\n",
    "\n",
    "# Different Example\n",
    "print('{:15} {:<10.3}'.format('dissimilar:', calc_cosine_similarity(s3, s4)) )\n",
    "\n",
    "# Same Example\n",
    "print('{:15} {:<10.3}'.format('same:', calc_cosine_similarity(s3, s3)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spacy Cosine Similarity\n",
    "Cosine similarity calculated with the Spacy embeddings (large file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_spacy_sim(s1, s2):\n",
    "    s2 = nlp(s2)\n",
    "    s1 = nlp(s1)\n",
    "    return s1.similarity(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Use Pickle Dump here to Run the Model for given sentences or text file outputting in the correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}