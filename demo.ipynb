{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 6320 Natural Language Processing\n",
    "## Shruti Agrawal & Pat Dayton\n",
    "\n",
    "This notebook demos our code for Tasks 1 & 2 of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "\n",
    "\n",
    "# SPACY IMPORT\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# CoreNLP setup\n",
    "core_nlp_url = 'http://localhost:9000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Stanford CoreNLP Server\n",
    "In another console run the script below in order to start the Stanford CoreNLP Server on port 9000. We will hit this API in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#java -mx4g -cp \"./corenlp/*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Parse the Corpus\n",
    "First read in the corpus and do basic parsing to split out the first sentence, second sentence, and score for each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(fileName, test=False):\n",
    "\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    score = []\n",
    "    file = open(fileName, encoding=\"utf8\")\n",
    "    text = file.readline()\n",
    "    text = file.read()\n",
    "    \n",
    "    # loop to extract a set of two sentences\n",
    "    for sentence in text.split('\\n'):\n",
    "\n",
    "        # creating two separate lists of the sentences\n",
    "        # '.rstrip('.') only removes the last period in the sentence\n",
    "        \n",
    "        s1.insert(len(s1), (sentence.split('\\t')[1].lower()).rstrip('.'))\n",
    "        s2.insert(len(s1), (sentence.split('\\t')[2].lower()).rstrip('.'))\n",
    "        \n",
    "        # inserting the score as a separate lists\n",
    "        if (not test):\n",
    "            score.insert(len(s1), (sentence.split('\\t')[3]))\n",
    "\n",
    "    # print(s1)\n",
    "    if test:\n",
    "        return s1, s2\n",
    "    else:\n",
    "        return s1, s2, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same readData from STS.py\n",
    "def preprocess(fileName, test=False):\n",
    "\n",
    "    if (test):\n",
    "        s1, s2 = readData(fileName, test)\n",
    "    else:\n",
    "        s1, s2, scores = readData(fileName, test)\n",
    "\n",
    "    s1_toks = []\n",
    "    s2_toks = []\n",
    "\n",
    "    # tokenizing and tagging\n",
    "    s1_tags = []\n",
    "    s2_tags = []\n",
    "\n",
    "    for sentence in s1:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        s1_toks.insert(len(s1_toks), tokens)\n",
    "        s1_tags.insert(\n",
    "            len(s1_tags), nltk.pos_tag(tokens))\n",
    "\n",
    "    for sentence in s2:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        s2_toks.insert(len(s2_toks), tokens)\n",
    "        s2_tags.insert(\n",
    "            len(s2_tags), nltk.pos_tag(tokens))\n",
    "    \n",
    "    # Remove the unnecessary tuple and keep just the tags\n",
    "    for i, tag_list in enumerate(s1_tags):\n",
    "        s1_tags[i] = [tup[1] for tup in tag_list]\n",
    "    for i, tag_list in enumerate(s2_tags):\n",
    "        s2_tags[i] = [tup[1] for tup in tag_list]\n",
    "\n",
    "    # lemmatizing\n",
    "    s1_lemmas = []\n",
    "    s2_lemmas = []\n",
    "    s1_ls = []\n",
    "    s2_ls = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for sentence in s1_toks:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        s1_lemmas.insert(\n",
    "            len(s1_lemmas), sentence_components)\n",
    "        s1_ls.insert(len(s1_ls), ' '.join(word for word in sentence_components))\n",
    "    \n",
    "    for sentence in s2_toks:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        s2_lemmas.insert(\n",
    "            len(s2_lemmas), sentence_components)\n",
    "        s2_ls.insert(len(s2_ls), ' '.join(word for word in sentence_components))\n",
    "\n",
    "\n",
    "        \n",
    "    # Zipping it all together into one object for each word\n",
    "    s1_word_lists = []\n",
    "    s2_word_lists = []\n",
    "    \n",
    "    for tok_list, lem_list, tag_list in zip(s1_toks, s1_lemmas, s1_tags):\n",
    "        sentence_words = []\n",
    "        for tok, lem, tag in zip(tok_list, lem_list, tag_list):\n",
    "            word = {}\n",
    "            word['tok'] = tok\n",
    "            word['lem'] = lem\n",
    "            word['tag'] = tag\n",
    "            sentence_words.append(word)\n",
    "        s1_word_lists.append(sentence_words) \n",
    "        \n",
    "    for tok_list, lem_list, tag_list in zip(s2_toks, s2_lemmas, s2_tags):\n",
    "        sentence_words = []\n",
    "        for tok, lem, tag in zip(tok_list, lem_list, tag_list):\n",
    "            word = {}\n",
    "            word['tok'] = tok\n",
    "            word['lem'] = lem\n",
    "            word['tag'] = tag\n",
    "            sentence_words.append(word)\n",
    "        s2_word_lists.append(sentence_words)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create a corpus object to represent our corpus\n",
    "    corpus = {}\n",
    "    corpus[\"s1\"] = {}\n",
    "    corpus[\"s2\"] = {}\n",
    "    if (not test):\n",
    "        corpus['scores'] = [int(i) for i in scores]\n",
    "    \n",
    "    corpus[\"s1\"][\"sentences\"] = s1\n",
    "    corpus[\"s2\"][\"sentences\"] = s2\n",
    "    \n",
    "    corpus[\"s1\"][\"tokens\"] = s1_toks\n",
    "    corpus[\"s2\"][\"tokens\"] = s2_toks\n",
    "    \n",
    "    corpus[\"s1\"][\"lemmas\"] = s1_lemmas\n",
    "    corpus[\"s2\"][\"lemmas\"] = s2_lemmas\n",
    "    \n",
    "    corpus[\"s1\"][\"tags\"] = s1_tags\n",
    "    corpus[\"s2\"][\"tags\"] = s2_tags\n",
    "    \n",
    "    corpus[\"s1\"][\"words\"] = s1_word_lists\n",
    "    corpus[\"s2\"][\"words\"] = s2_word_lists\n",
    "\n",
    "    corpus[\"s1\"][\"ls\"] = s1_ls\n",
    "    corpus[\"s2\"][\"ls\"] = s2_ls\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess(\"./data/train-set.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484\n",
      "1484\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[\"s1\"]['sentences']))\n",
    "print(len(train_data[\"s2\"]['sentences']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Sentences\n",
    "Picking some test sentences to work with for the part 2 demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 286 # Index of the sentence pairs in the train_data\n",
    "\n",
    "# Uncomment these lines for train data sentences\n",
    "# ts1 = train_data[\"s1\"]['sentences'][r]\n",
    "# ts2 = train_data[\"s1\"]['sentences'][r]\n",
    "\n",
    "# Uncomment these lines to type in your own sentences\n",
    "ts1 = \"I love to eat incredibly large pizza slices.\"\n",
    "ts2 = \"I enjoy stuffing my face with pepperoni pizza.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROW 286 FROM TEST DATA\n",
      "\n",
      "Sentence 1\n",
      "\n",
      "Raw:  gemstar's shares gathered up 2.6 percent, adding 14 cents to $5.49 at the close\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemstar</td>\n",
       "      <td>gemstar</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'s</td>\n",
       "      <td>'s</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shares</td>\n",
       "      <td>share</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gathered</td>\n",
       "      <td>gathered</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>RP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>percent</td>\n",
       "      <td>percent</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adding</td>\n",
       "      <td>adding</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cents</td>\n",
       "      <td>cent</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.49</td>\n",
       "      <td>5.49</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>close</td>\n",
       "      <td>close</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tokens    Lemmas Tags\n",
       "0    gemstar   gemstar   NN\n",
       "1         's        's  POS\n",
       "2     shares     share  NNS\n",
       "3   gathered  gathered  VBD\n",
       "4         up        up   RP\n",
       "5        2.6       2.6   CD\n",
       "6    percent   percent   NN\n",
       "7          ,         ,    ,\n",
       "8     adding    adding  VBG\n",
       "9         14        14   CD\n",
       "10     cents      cent  NNS\n",
       "11        to        to   TO\n",
       "12         $         $    $\n",
       "13      5.49      5.49   CD\n",
       "14        at        at   IN\n",
       "15       the       the   DT\n",
       "16     close     close   NN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 2\n",
      "\n",
      "Raw:  gemstar shares moved higher on the news, closing up 2.6 percent at $5.49 on nasdaq\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemstar</td>\n",
       "      <td>gemstar</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shares</td>\n",
       "      <td>share</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>moved</td>\n",
       "      <td>moved</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>higher</td>\n",
       "      <td>higher</td>\n",
       "      <td>RBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>news</td>\n",
       "      <td>news</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>closing</td>\n",
       "      <td>closing</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>RP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>percent</td>\n",
       "      <td>percent</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.49</td>\n",
       "      <td>5.49</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nasdaq</td>\n",
       "      <td>nasdaq</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tokens   Lemmas Tags\n",
       "0   gemstar  gemstar   NN\n",
       "1    shares    share  NNS\n",
       "2     moved    moved  VBD\n",
       "3    higher   higher  RBR\n",
       "4        on       on   IN\n",
       "5       the      the   DT\n",
       "6      news     news   NN\n",
       "7         ,        ,    ,\n",
       "8   closing  closing  VBG\n",
       "9        up       up   RP\n",
       "10      2.6      2.6   CD\n",
       "11  percent  percent   NN\n",
       "12       at       at   IN\n",
       "13        $        $    $\n",
       "14     5.49     5.49   CD\n",
       "15       on       on   IN\n",
       "16   nasdaq   nasdaq  NNS"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  4\n"
     ]
    }
   ],
   "source": [
    "tkns1 = train_data[\"s1\"]['tokens'][r]\n",
    "lems1 = train_data[\"s1\"]['lemmas'][r]\n",
    "tags1 = train_data[\"s1\"]['tags'][r]\n",
    "tkns2 = train_data[\"s2\"]['tokens'][r]\n",
    "lems2 = train_data[\"s2\"]['lemmas'][r]\n",
    "tags2 = train_data[\"s2\"]['tags'][r]\n",
    "\n",
    "data1 = []\n",
    "data2 = []\n",
    "\n",
    "for i in range(0, len(tkns1)):\n",
    "    data1.append([tkns1[i], lems1[i], tags1[i]])\n",
    "    \n",
    "for i in range(0, len(tkns2)):\n",
    "    data2.append([tkns2[i], lems2[i], tags2[i]])\n",
    "    \n",
    "df1 = pd.DataFrame(\n",
    "    data1, \n",
    "    columns = ['Tokens', 'Lemmas', 'Tags']) \n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "    data2, \n",
    "    columns = ['Tokens', 'Lemmas', 'Tags']) \n",
    "\n",
    "\n",
    "print('ROW {} FROM TEST DATA\\n'.format(r))\n",
    "print('Sentence 1\\n')\n",
    "print('Raw: ', train_data[\"s1\"]['sentences'][r])\n",
    "display(df1)\n",
    "print('Sentence 2\\n')\n",
    "print('Raw: ', train_data[\"s2\"]['sentences'][r])\n",
    "display(df2)\n",
    "print('Score: ', train_data[\"scores\"][r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependency Parsing Sentence 1\n",
      "*****************************\n",
      "\n",
      "I\tPRP\t2\tnsubj\n",
      "love\tVBP\t0\tROOT\n",
      "to\tTO\t4\tmark\n",
      "eat\tVB\t2\txcomp\n",
      "incredibly\tRB\t6\tadvmod\n",
      "large\tJJ\t8\tamod\n",
      "pizza\tNN\t8\tcompound\n",
      "slices\tNNS\t4\tdobj\n",
      ".\t.\t2\tpunct\n",
      "\n",
      "\n",
      "Dependency Parsing Sentence 2\n",
      "*****************************\n",
      "\n",
      "I\tPRP\t2\tnsubj\n",
      "enjoy\tVBP\t0\tROOT\n",
      "stuffing\tVBG\t2\txcomp\n",
      "my\tPRP$\t5\tnmod:poss\n",
      "face\tNN\t3\tdobj\n",
      "with\tIN\t8\tcase\n",
      "pepperoni\tNNS\t8\tcompound\n",
      "pizza\tNN\t3\tnmod\n",
      ".\t.\t2\tpunct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dependency parsing\n",
    "print(\"\\nDependency Parsing Sentence 1\")\n",
    "print(\"*****************************\\n\")\n",
    "\n",
    "dependency_parser = CoreNLPDependencyParser(url=core_nlp_url)\n",
    "parse, = dependency_parser.raw_parse(ts1)\n",
    "print(parse.to_conll(4))\n",
    "\n",
    "print(\"\\nDependency Parsing Sentence 2\")\n",
    "print(\"*****************************\\n\")\n",
    "dependency_parser = CoreNLPDependencyParser(url=core_nlp_url)\n",
    "parse, = dependency_parser.raw_parse(ts2)\n",
    "print(parse.to_conll(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Parsing\n",
    "https://www.nltk.org/api/nltk.parse.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full syntactic parse tree for sentence 1: \n",
      "                    ROOT                               \n",
      "                     |                                  \n",
      "                     S                                 \n",
      "  ___________________|_______________________________   \n",
      " |        VP                                         | \n",
      " |    ____|___                                       |  \n",
      " |   |        S                                      | \n",
      " |   |        |                                      |  \n",
      " |   |        VP                                     | \n",
      " |   |     ___|______                                |  \n",
      " |   |    |          VP                              | \n",
      " |   |    |    ______|_____________                  |  \n",
      " |   |    |   |                    NP                | \n",
      " |   |    |   |               _____|___________      |  \n",
      " NP  |    |   |             ADJP         |     |     | \n",
      " |   |    |   |       _______|_____      |     |     |  \n",
      "PRP VBP   TO  VB     RB            JJ    NN   NNS    . \n",
      " |   |    |   |      |             |     |     |     |  \n",
      " I  love  to eat incredibly      large pizza slices  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# syntactic parsing\n",
    "print(\"\\nFull syntactic parse tree for sentence 1: \")\n",
    "syntactic_parser = CoreNLPParser(url=core_nlp_url)\n",
    "s1_tree = next(syntactic_parser.raw_parse(ts1))\n",
    "s1_tree.pretty_print()\n",
    "\n",
    "f = open(\"./output/s1_parse_tree.txt\", \"w\", encoding=\"utf-8\")\n",
    "s1_tree.pretty_print(stream=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full syntactic parse tree for sentence 1: \n",
      "                            ROOT                             \n",
      "                             |                                \n",
      "                             S                               \n",
      "  ___________________________|_____________________________   \n",
      " |           VP                                            | \n",
      " |     ______|_______________                              |  \n",
      " |    |                      S                             | \n",
      " |    |                      |                             |  \n",
      " |    |                      VP                            | \n",
      " |    |       _______________|____________                 |  \n",
      " |    |      |           |                PP               | \n",
      " |    |      |           |         _______|______          |  \n",
      " NP   |      |           NP       |              NP        | \n",
      " |    |      |       ____|___     |        ______|____     |  \n",
      "PRP  VBP    VBG    PRP$      NN   IN     NNS          NN   . \n",
      " |    |      |      |        |    |       |           |    |  \n",
      " I  enjoy stuffing  my      face with pepperoni     pizza  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# syntactic parsing\n",
    "print(\"\\nFull syntactic parse tree for sentence 1: \")\n",
    "syntactic_parser = CoreNLPParser(url=core_nlp_url)\n",
    "s1_tree = next(syntactic_parser.raw_parse(ts2))\n",
    "s1_tree.pretty_print()\n",
    "\n",
    "f = open(\"./output/s2_parse_tree.txt\", \"w\", encoding=\"utf-8\")\n",
    "s1_tree.pretty_print(stream=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********************************************************************************\n",
      "the DT\n",
      "\n",
      "Synonyms:  []\n",
      "\n",
      "Hypernyms:  []\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "american JJ\n",
      "\n",
      "Synonyms:  ['American', 'American_English', 'American_language']\n",
      "\n",
      "Hypernyms:  ['inhabitant', 'habitant', 'dweller', 'denizen', 'indweller', 'English', 'English_language']\n",
      "\n",
      "Hyponyms:  ['African-American', 'African_American', 'Afro-American', 'Black_American', 'Alabaman', 'Alabamian', 'Alaskan', 'Anglo-American', 'Appalachian', 'Arizonan', 'Arizonian', 'Arkansan', 'Arkansawyer', 'Asian_American', 'Bay_Stater', 'Bostonian', 'Californian', 'Carolinian', 'Coloradan', 'Connecticuter', 'Creole', 'Delawarean', 'Delawarian', 'Floridian', 'Franco-American', 'Georgian', 'German_American', 'Hawaiian', 'Idahoan', 'Illinoisan', 'Indianan', 'Hoosier', 'Iowan', 'Kansan', 'Kentuckian', 'Bluegrass_Stater', 'Louisianan', 'Louisianian', 'Mainer', 'Down_Easter', 'Marylander', 'Michigander', 'Wolverine', 'Minnesotan', 'Gopher', 'Mississippian', 'Missourian', 'Montanan', 'Nebraskan', 'Cornhusker', 'Nevadan', 'New_Englander', 'Yankee', 'New_Hampshirite', 'Granite_Stater', 'New_Jerseyan', 'New_Jerseyite', 'Garden_Stater', 'New_Mexican', 'New_Yorker', 'Nisei', 'North_Carolinian', 'Tarheel', 'North_Dakotan', 'Ohioan', 'Buckeye', 'Oklahoman', 'Sooner', 'Oregonian', 'Beaver', 'Pennsylvanian', 'Keystone_Stater', 'Puerto_Rican', 'Rhode_Islander', 'South_Carolinian', 'South_Dakotan', 'Southerner', 'Spanish_American', 'Hispanic_American', 'Hispanic', 'Tennessean', 'Volunteer', 'Texan', 'Tory', 'Utahan', 'Vermonter', 'Virginian', 'Washingtonian', 'West_Virginian', 'Wisconsinite', 'Badger', 'Wyomingite', 'Yank', 'Northerner', 'Yankee-Doodle', 'African_American_Vernacular_English', 'AAVE', 'African_American_English', 'Black_English', 'Black_English_Vernacular', 'Black_Vernacular', 'Black_Vernacular_English', 'Ebonics', 'Latin_American', 'Latino', 'Mesoamerican', 'North_American', 'South_American', 'West_Indian']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: ['United_States', 'United_States_of_America', 'America', 'the_States', 'US', 'U.S.', 'USA', 'U.S.A.', 'Central_America', 'North_America', 'South_America']\n",
      "\n",
      "***********************************************************************************\n",
      "anglican JJ\n",
      "\n",
      "Synonyms:  ['Anglican']\n",
      "\n",
      "Hypernyms:  ['Protestant']\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: ['Anglican_Church', 'Anglican_Communion', 'Church_of_England']\n",
      "\n",
      "***********************************************************************************\n",
      "council NN\n",
      "\n",
      "Synonyms:  ['council']\n",
      "\n",
      "Hypernyms:  ['administrative_unit', 'administrative_body', 'assembly', 'meeting', 'group_meeting']\n",
      "\n",
      "Hyponyms:  ['city_council', 'Economic_and_Social_Council', 'ECOSOC', 'executive_council', 'North_Atlantic_Council', 'NAC', 'panchayat', 'panchayet', 'punchayet', 'privy_council', 'Sanhedrin', 'Security_Council', 'SC', 'soviet', 'Trusteeship_Council', 'TC', 'works_council', 'world_council', 'Constance', 'Council_of_Constance', 'Constantinople', 'Fourth_Council_of_Constantinople', 'Council_of_Basel-Ferrara-Florence', 'Council_of_Trent', 'ecumenical_council', 'Lateran_Council', 'Lyons', 'Second_Council_of_Lyons', 'First_Council_of_Lyons', 'Vatican_Council', 'Vienne', 'Council_of_Vienne', 'indaba', 'Jirga', 'powwow', 'synod']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      ", ,\n",
      "\n",
      "Synonyms:  []\n",
      "\n",
      "Hypernyms:  []\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "which WDT\n",
      "\n",
      "Synonyms:  []\n",
      "\n",
      "Hypernyms:  []\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "represents VBZ\n",
      "\n",
      "Synonyms:  ['represent', 'stand_for', 'correspond', 'typify', 'symbolize', 'symbolise', 'exemplify', 'constitute', 'make_up', 'comprise', 'be', 'defend', 'interpret', 'act', 'play', 'stage', 'present', 'lay_out', 'map']\n",
      "\n",
      "Hypernyms:  ['equal', 'be', 'mean', 'intend', 'serve', 'express', 'verbalize', 'verbalise', 'utter', 'give_tongue_to', 'embody', 'personify', 're-create', 'describe', 'depict', 'draw', 'remonstrate', 'point_out', 'state', 'say', 'tell', 'permute', 'commute', 'transpose']\n",
      "\n",
      "Hyponyms:  ['embody', 'be', 'personify', 'typify', 'epitomize', 'epitomise', 'instantiate', 'speak_for', 'dramatize', 'dramatise', 'misrepresent', 'belie', 'portray', 'compose', 'fall_into', 'fall_under', 'form', 'constitute', 'make', 'present', 'pose', 'range', 'straddle', 'supplement', 'animalize', 'animalise', 'capture', 'draw', 'graph', 'chart', 'map', 'model', 'mock_up', 'paint', 'picture', 'depict', 'render', 'show', 'limn', 'profile', 'sensualize', 'carnalize', 'silhouette', 'stylize', 'stylise', 'conventionalize', 'act_out', 'emote', 'enact', 'reenact', 'impersonate', 'pretend', 'make_believe', 'parody', 'support', 'set', 'localize', 'localise', 'place', 'actualize', 'actualise', 'symbolize', 'symbolise', 'argue', 'reason', 'indicate', 'spin']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "episcopalian JJ\n",
      "\n",
      "Synonyms:  ['Episcopalian', 'Episcopal']\n",
      "\n",
      "Hypernyms:  ['Protestant']\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: ['Episcopal_Church', 'Protestant_Episcopal_Church']\n",
      "\n",
      "***********************************************************************************\n",
      "conservatives NNS\n",
      "\n",
      "Synonyms:  ['conservative', 'conservativist', 'Conservative']\n",
      "\n",
      "Hypernyms:  ['adult', 'grownup', 'member', 'fellow_member']\n",
      "\n",
      "Hyponyms:  ['capitalist', 'conformist', 'fuddy-duddy', 'hardliner', 'minimalist', 'mossback', 'neoconservative', 'neocon', 'reactionary', 'ultraconservative', 'extreme_right-winger', 'rightist', 'right-winger', 'square', 'square_toes', 'traditionalist', 'diehard']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      ", ,\n",
      "\n",
      "Synonyms:  []\n",
      "\n",
      "Hypernyms:  []\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "said VBD\n",
      "\n",
      "Synonyms:  ['state', 'say', 'tell', 'allege', 'aver', 'suppose', 'read', 'order', 'enjoin', 'pronounce', 'articulate', 'enounce', 'sound_out', 'enunciate', 'aforesaid', 'aforementioned', 'said']\n",
      "\n",
      "Hypernyms:  ['express', 'verbalize', 'verbalise', 'utter', 'give_tongue_to', 'assert', 'asseverate', 'maintain', 'speculate', 'have', 'feature', 'request', 'convey', 'recite', 'read', 'register', 'show', 'record']\n",
      "\n",
      "Hyponyms:  ['add', 'append', 'supply', 'announce', 'declare', 'answer', 'reply', 'respond', 'articulate', 'enunciate', 'vocalize', 'vocalise', 'explain', 'get_out', 'give', 'misstate', 'note', 'observe', 'mention', 'remark', 'precede', 'preface', 'premise', 'introduce', 'present', 'represent', 'lay_out', 'summarize', 'summarise', 'sum', 'sum_up', 'plead', 'call', 'send_for', 'command', 'require', 'direct', 'instruct', 'warn', 'aspirate', 'click', 'devoice', 'drawl', 'explode', 'flap', 'lilt', 'lisp', 'mispronounce', 'misspeak', 'nasalize', 'nasalise', 'palatalize', 'palatalise', 'raise', 'retroflex', 'roll', 'round', 'labialize', 'labialise', 'sibilate', 'stress', 'accent', 'accentuate', 'subvocalize', 'subvocalise', 'syllabize', 'syllabise', 'trill', 'twang', 'vowelize', 'vowelise', 'voice', 'sound']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "it PRP\n",
      "\n",
      "Synonyms:  ['information_technology', 'IT']\n",
      "\n",
      "Hypernyms:  ['engineering', 'engineering_science', 'applied_science', 'technology']\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "will MD\n",
      "\n",
      "Synonyms:  ['volition', 'will', 'testament', 'bequeath', 'leave']\n",
      "\n",
      "Hypernyms:  ['faculty', 'mental_faculty', 'module', 'purpose', 'intent', 'intention', 'aim', 'design', 'legal_document', 'legal_instrument', 'official_document', 'instrument', 'ordain', 'decide', \"make_up_one's_mind\", 'determine', 'give', 'gift', 'present']\n",
      "\n",
      "Hyponyms:  ['velleity', 'devise', 'New_Testament', 'Old_Testament', 'fee-tail', 'entail', 'pass_on', 'remember']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  ['codicil']\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "seek VB\n",
      "\n",
      "Synonyms:  ['seek', 'search', 'look_for', 'try', 'attempt', 'essay', 'assay']\n",
      "\n",
      "Hypernyms:  ['movement', 'motion', 'desire', 'want', 'act', 'move', 'travel', 'go', 'locomote', 'request']\n",
      "\n",
      "Hyponyms:  ['bid', 'quest', 'browse', 'surf', 'divine', 'dredge', 'drag', 'feel', 'finger', 'fish', 'angle', 'gather', 'grope', 'fumble', 'grub', 'hunt', 'leave_no_stone_unturned', 'quest_for', 'go_after', 'quest_after', 'pursue', 'scour', 'seek_out', 'shop', 'want', 'endeavor', 'endeavour', 'strive', 'fight', 'struggle', 'gamble', 'chance', 'risk', 'hazard', 'take_chances', 'adventure', 'run_a_risk', 'take_a_chance', 'give_it_a_whirl', 'give_it_a_try', 'have_a_go', 'put_on_the_line', 'lay_on_the_line', 'take_a_dare', 'pick_up_the_gauntlet']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "authorization NN\n",
      "\n",
      "Synonyms:  ['mandate', 'authorization', 'authorisation', 'authority', 'potency', 'dominance', 'say-so', 'sanction', 'empowerment']\n",
      "\n",
      "Hypernyms:  ['legal_document', 'legal_instrument', 'official_document', 'instrument', 'control', 'permission', 'management', 'direction']\n",
      "\n",
      "Hyponyms:  ['carte_blanche', 'command', 'imperium', 'lordship', 'muscle', 'power_of_appointment', 'sovereignty', 'certification', 'enfranchisement', 'commission', 'commissioning', 'delegating', 'delegation', 'relegating', 'relegation', 'deputation', 'license', 'permission', 'permit', 'loan_approval', 'rubber_stamp', 'sanction']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "to TO\n",
      "\n",
      "Synonyms:  []\n",
      "\n",
      "Hypernyms:  []\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "create VB\n",
      "\n",
      "Synonyms:  ['make', 'create', 'produce']\n",
      "\n",
      "Hypernyms:  ['act', 'move', 'appoint', 'charge', 'make', 'create']\n",
      "\n",
      "Hyponyms:  ['arouse', 'elicit', 'enkindle', 'kindle', 'evoke', 'fire', 'raise', 'provoke', 'assemble', 'piece', 'put_together', 'set_up', 'tack', 'tack_together', 'bear', 'turn_out', 'beat', 'beget', 'get', 'engender', 'father', 'mother', 'sire', 'generate', 'bring_forth', 'blast', 'shell', 'bring', 'work', 'play', 'wreak', 'make_for', 'build', 'establish', 'cause', 'do', 'make', 'chop', 'choreograph', 'clear', 'cleave', 'compose', 'write', 'construct', 'copy', 're-create', 'create', 'create_by_mental_act', 'create_mentally', 'create_from_raw_material', 'create_from_raw_stuff', 'create_verbally', 'cut', 'derive', 'educe', 'direct', 'distill', 'extract', 'distil', 'give', 'film-make', 'film', 'form', 'organize', 'organise', 'froth', 'spume', 'suds', 'yield', 'grind', 'incorporate', 'institute', 'lay_down', 'manufacture', 'offset', 'originate', 'initiate', 'start', 'prepare', 'press', 'produce', 'bring_about', 'give_rise', 'puncture', 'put_on', 'turn_in', 'conjure', 'conjure_up', 'invoke', 'stir', 'call_down', 'bring_up', 'put_forward', 'call_forth', 'realize', 'realise', 'actualize', 'actualise', 'substantiate', 'recreate', 'regenerate', 'reproduce', 'procreate', 'multiply', 'scrape', 'short-circuit', 'short', 'strike', 'style', 'track', 'twine', 'carve_out', 'develop', 'reinvent', 'draw', 'paint', 'design', 'bootleg', 'breed', 'churn_out', 'clap_up', 'clap_together', 'slap_together', 'confect', 'custom-make', 'customize', 'customise', 'tailor-make', 'burn', 'dummy', 'dummy_up', 'elaborate', 'extrude', 'squeeze_out', 'fudge_together', 'throw_together', 'laminate', 'machine', 'output', 'overproduce', 'prefabricate', 'preassemble', 'print', 'publish', 'proof', 'pulse', 'pulsate', 'put_out', 'remake', 'refashion', 'redo', 'make_over', 'render', 'return', 'smelt', 'underproduce']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "a DT\n",
      "\n",
      "Synonyms:  ['angstrom', 'angstrom_unit', 'A', 'vitamin_A', 'antiophthalmic_factor', 'axerophthol', 'deoxyadenosine_monophosphate', 'adenine', 'ampere', 'amp', 'a', 'type_A', 'group_A']\n",
      "\n",
      "Hypernyms:  ['metric_linear_unit', 'fat-soluble_vitamin', 'nucleotide', 'base', 'purine', 'current_unit', 'letter', 'letter_of_the_alphabet', 'alphabetic_character', 'blood_group', 'blood_type']\n",
      "\n",
      "Hyponyms:  ['vitamin_A1', 'retinol', 'vitamin_A2', 'dehydroretinol']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  ['picometer', 'picometre', 'micromicron', 'milliampere', 'mA']\n",
      "\n",
      "Holonyms: ['Roman_alphabet', 'Latin_alphabet']\n",
      "\n",
      "***********************************************************************************\n",
      "separate JJ\n",
      "\n",
      "Synonyms:  ['offprint', 'reprint', 'separate', 'divide', 'disunite', 'part', 'distinguish', 'differentiate', 'secern', 'secernate', 'severalize', 'severalise', 'tell', 'tell_apart', 'split', 'split_up', 'dissever', 'carve_up', 'classify', 'class', 'sort', 'assort', 'sort_out', 'break', 'break_up', 'fall_apart', 'come_apart', 'discriminate', 'single_out', 'branch', 'ramify', 'fork', 'furcate', 'freestanding', 'disjoined']\n",
      "\n",
      "Hypernyms:  ['article', 'garment', 'move', 'displace', 'identify', 'place', 'change_integrity', 'categorize', 'categorise', 'distinguish', 'separate', 'differentiate', 'secern', 'secernate', 'severalize', 'severalise', 'tell', 'tell_apart', 'change', 'diverge']\n",
      "\n",
      "Hyponyms:  ['break', 'compartmentalize', 'compartmentalise', 'cut_up', 'cut', 'disconnect', 'disjoin', 'disjoint', 'disarticulate', 'gin', 'joint', 'polarize', 'polarise', 'sequester', 'sequestrate', 'keep_apart', 'set_apart', 'isolate', 'sever', 'break_up', 'tear', 'rupture', 'snap', 'bust', 'contradistinguish', 'contrast', 'decouple', 'dissociate', 'demarcate', 'discriminate', 'know_apart', 'separate', 'single_out', 'individualize', 'individualise', 'know', 'label', 'severalize', 'severalise', 'sex', 'stratify', 'Balkanize', 'Balkanise', 'canton', 'format', 'initialize', 'initialise', 'lot', 'paragraph', 'parcel', 'sectionalize', 'sectionalise', 'sliver', 'splinter', 'subdivide', 'triangulate', 'unitize', 'unitise', 'avulse', 'decompose', 'break_down', 'dialyse', 'dialyze', 'disperse', 'extract', 'filter', 'filtrate', 'strain', 'separate_out', 'filter_out', 'fractionate', 'macerate', 'peptize', 'peptise', 'sift', 'sieve', 'tease', 'card', 'wash', 'catalogue', 'catalog', 'count', 'number', 'dichotomize', 'dichotomise', 'grade', 'group', 'pigeonhole', 'stereotype', 'stamp', 'reclassify', 'refer', 'size', 'detach', 'dissipate', 'dispel', 'scatter', 'partition', 'zone', 'rail', 'rail_off', 'shut_off', 'close_off', 'break_away', 'break_with', 'disassociate', 'divorce', 'disunite', 'disunify', 'break_apart', 'split_up', 'give_the_axe', 'give_the_bounce', 'give_the_gate', 'secede', 'diffract', 'spread_out', 'fragment', 'fragmentize', 'fragmentise', 'burst', 'split', 'break_open', 'crush', 'ladder', 'run', 'puncture', 'smash', 'crack', 'disadvantage', 'disfavor', 'disfavour', 'hive_off', 'insulate', 'redline', 'segregate', 'calve', 'chip', 'chip_off', 'come_off', 'break_off', 'come_away', 'dismember', 'take_apart', 'discerp', 'gerrymander', 'partition_off', 'reduce', 'segment', 'section', 'arborize', 'arborise', 'bifurcate', 'trifurcate', 'twig']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "province NN\n",
      "\n",
      "Synonyms:  ['state', 'province', 'responsibility']\n",
      "\n",
      "Hypernyms:  ['administrative_district', 'administrative_division', 'territorial_division', 'sphere', 'domain', 'area', 'orbit', 'field', 'arena']\n",
      "\n",
      "Hyponyms:  ['American_state', 'Australian_state', 'Canadian_province', 'commonwealth', 'eparchy', 'Italian_region', 'Soviet_Socialist_Republic', 'ecclesiastical_province']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: ['country', 'state', 'land']\n"
     ]
    }
   ],
   "source": [
    "# 3 has holonyms and meronyms which are relatively rare./\n",
    "r = 3\n",
    "for tk, tg in zip(train_data[\"s2\"]['tokens'][r], train_data[\"s1\"]['tags'][r]):\n",
    "    \n",
    "    print('\\n***********************************************************************************')\n",
    "    print(tk, tg)\n",
    "    synonyms = []\n",
    "    hypernyms = []\n",
    "    hyponyms = []\n",
    "    substance_meronyms = []\n",
    "    part_meronyms = []\n",
    "    holonyms = []\n",
    "\n",
    "    for syn in wn.synsets(tk):\n",
    "        # Synonyms\n",
    "        for l in syn.lemmas():\n",
    "            if l.name() not in synonyms:\n",
    "                synonyms.append(l.name())\n",
    "\n",
    "        # Hypernyms\n",
    "        for hpr in syn.hypernyms():\n",
    "            for l in hpr.lemmas():\n",
    "                if l.name() not in hypernyms:\n",
    "                    hypernyms.append(l.name())\n",
    "\n",
    "        # Hyponyms\n",
    "        for hpo in syn.hyponyms():\n",
    "            for l in hpo.lemmas():\n",
    "                if l.name() not in hyponyms:\n",
    "                    hyponyms.append(l.name())\n",
    "\n",
    "        # Substance Meronyms\n",
    "        for mrn in syn.substance_meronyms():\n",
    "            for l in mrn.lemmas():\n",
    "                if l.name() not in substance_meronyms:\n",
    "                    substance_meronyms.append(l.name())\n",
    "\n",
    "        # Part Meronyms\n",
    "        for mrn in syn.part_meronyms():\n",
    "            for l in mrn.lemmas():\n",
    "                if l.name() not in part_meronyms:\n",
    "                    part_meronyms.append(l.name())\n",
    "\n",
    "        # Holonyms\n",
    "        for hol in syn.member_holonyms():\n",
    "            for l in hol.lemmas():\n",
    "                if l.name() not in holonyms:\n",
    "                    holonyms.append(l.name())\n",
    "\n",
    "    print('\\nSynonyms: ', synonyms)\n",
    "    print('\\nHypernyms: ', hypernyms)\n",
    "    print('\\nHyponyms: ', hyponyms)\n",
    "    print('\\nMeronyms (substance): ', substance_meronyms)\n",
    "    print('\\nMeronyms (part): ', part_meronyms)\n",
    "    print('\\nHolonyms:', holonyms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3\n",
    "\n",
    "In our model we used 11 features for each pair of sentences to build our Machine Learning Model:\n",
    "- Cosine Similarity\n",
    "- Spacy (Cosine) Similarity \n",
    "- SIF Similarity\n",
    "- Word Overlap\n",
    "- Normalized Word Overlap\n",
    "- Lemma Overlap\n",
    "- Normalized Lemma Overlap\n",
    "- Synset Overlap\n",
    "- Normalized Synset Overlap\n",
    "- Path Similarity \n",
    "- Named Entity Overlap\n",
    "\n",
    "In the following cells we show some examples of these in use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test sentences for the following demonstrations.\n",
    "\n",
    "# Similar Sentences\n",
    "s1 = 'I enjoy eating apples.'\n",
    "s1_tok = ['I', 'enjoy', 'eating', 'apples']\n",
    "s2 = 'I like munching red apples'\n",
    "s2_tok = ['I', 'like', 'munching', 'red', 'apples']\n",
    "\n",
    "# Dissimilar Sentences\n",
    "s3 = 'My final exam was very difficult.'\n",
    "s3_tok = ['My', 'final', 'exam', 'was', 'very', 'difficult']\n",
    "s4 = 'Your mother smelled of elderberries.'\n",
    "s4_tok = ['Your', 'mother', 'smelled', 'of', 'elderberries']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "Cosine of embedding vectors in 3D Space. 0-1 value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar:        0.171     \n",
      "dissimilar:     0.0       \n",
      "same:           1.0       \n"
     ]
    }
   ],
   "source": [
    "def calc_cosine_similarity(s1, s2):\n",
    "\n",
    "    # remove the stopwords, transform into TF-IDF matrix, then\n",
    "    tfidf_matrix = TfidfVectorizer(\n",
    "        stop_words=\"english\").fit_transform([s1, s2])\n",
    "    \n",
    "    cos_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # print(tfidf_matrix.toarray())\n",
    "\n",
    "    cos_sim = cos_sim_matrix[0][1]\n",
    "\n",
    "    return cos_sim\n",
    "\n",
    "# Close Example\n",
    "print('{:15} {:<10.3}'.format('similar:', calc_cosine_similarity(s1, s2)) )\n",
    "\n",
    "# Different Example\n",
    "print('{:15} {:<10.3}'.format('dissimilar:', calc_cosine_similarity(s3, s4)) )\n",
    "\n",
    "# Same Example\n",
    "print('{:15} {:<10.3}'.format('same:', calc_cosine_similarity(s3, s3)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy Cosine Similarity\n",
    "Cosine similarity calculated with the Spacy embeddings (large file)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar:        0.853     \n",
      "dissimilar:     0.637     \n",
      "same:           1.0       \n"
     ]
    }
   ],
   "source": [
    "def calc_spacy_sim(s1, s2):\n",
    "    s2 = nlp(s2)\n",
    "    s1 = nlp(s1)\n",
    "    return s1.similarity(s2)\n",
    "\n",
    "# Close Example\n",
    "print('{:15} {:<10.3}'.format('similar:', calc_spacy_sim(s1, s2)) )\n",
    "\n",
    "# Different Example\n",
    "print('{:15} {:<10.3}'.format('dissimilar:', calc_spacy_sim(s3, s4)) )\n",
    "\n",
    "# Same Example\n",
    "print('{:15} {:<10.3}'.format('same:', calc_spacy_sim(s3, s3)) )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smooth Inverse Frequency (SIF) Similarity\n",
    "Smooth Inverse Frequency is a weighted average of word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar:        0.707     \n",
      "dissimilar:     0.0       \n",
      "same:           1.0       \n"
     ]
    }
   ],
   "source": [
    "def frequency_distribution(corpus):\n",
    "    s1_toks = corpus['s1']['tokens']\n",
    "    s2_toks = corpus['s2']['tokens']    \n",
    "    freq_dist = FreqDist()\n",
    "    for i in range(len(s1_toks)):\n",
    "        for token in (s1_toks[i] + s2_toks[i]):\n",
    "            freq_dist[token.lower()] += 1\n",
    "    return freq_dist\n",
    "\n",
    "freq_dist = frequency_distribution(train_data)\n",
    "\n",
    "\n",
    "def calc_sif_similarity(s1, s2, a = .001):\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform([s1, s2])\n",
    "    X_arr = X.toarray()\n",
    "    sif_matrix = []\n",
    "    for i in range(0, len(X_arr)):\n",
    "        sif_arr = []\n",
    "        for j in range(0, len(X_arr[i])):\n",
    "            word = vectorizer.get_feature_names()[j]\n",
    "            w = a / (a + freq_dist[word])\n",
    "            v = X_arr[i][j]\n",
    "            sif_arr.append(v*w)\n",
    "        sif_matrix.append(sif_arr)\n",
    "    sif_cos_sim_matrix = cosine_similarity(sif_matrix, sif_matrix)\n",
    "    sif_cos_sim = sif_cos_sim_matrix[0][1]\n",
    "    return sif_cos_sim\n",
    "\n",
    "# Close Example\n",
    "print('{:15} {:<10.3}'.format('similar:', calc_sif_similarity(s1, s2)) )\n",
    "\n",
    "# Different Example\n",
    "print('{:15} {:<10.3}'.format('dissimilar:', calc_sif_similarity(s3, s4)) )\n",
    "\n",
    "# Same Example\n",
    "print('{:15} {:<10.3}'.format('same:', calc_sif_similarity(s3, s3)) ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Word Overlap (Raw and Normalized)\n",
    "How many words do the two sentences have in common? This doesn't count stopwords or duplicates.\n",
    "\n",
    "## Simple Lemma Overlap (Raw and Normalized)\n",
    "How many lemmas do the two sentences have in common? This doesn't count stopwords or duplicates.\n",
    "\n",
    "This is the same function as simple word overlap, except that it takes in lemmas vs tokens and thus would likely have more overlap.\n",
    "\n",
    "*Note* This method takes tokenized sentences so I've hard coded those in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar:        2     0.444     \n",
      "dessimilar:     0     0.0       \n",
      "same:           4     1.0       \n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_sentence_list = train_data['s1']['tokens']+train_data['s2']['tokens']\n",
    "words_filtered = []\n",
    "\n",
    "# print(words)\n",
    "\n",
    "# looking through I've noticed there are a number of stop-words that can be added to the set\n",
    "stop_words.add(',')\n",
    "stop_words.add('``')\n",
    "stop_words.add(\"n't\")\n",
    "\n",
    "for tsl in tokenized_sentence_list:\n",
    "    for w in tsl:\n",
    "        if w not in stop_words and w not in words_filtered:\n",
    "            words_filtered.append(w)\n",
    "\n",
    "def remove_duplicate_tokens(token_list):\n",
    "    blank_list = []\n",
    "    for w in token_list:\n",
    "        if w not in blank_list:\n",
    "            blank_list.append(w)\n",
    "    return blank_list\n",
    "\n",
    "def remove_stopwords(token_list):\n",
    "    blank_list = []\n",
    "    for w in token_list:\n",
    "        if w not in stop_words:\n",
    "            blank_list.append(w)\n",
    "    return blank_list\n",
    "\n",
    "def calc_basic_overlap(s1_tokens, s2_tokens):\n",
    "    s1_tokens = remove_stopwords(s1_tokens)\n",
    "    s1_tokens = remove_duplicate_tokens(s1_tokens)\n",
    "\n",
    "    s2_tokens = remove_stopwords(s2_tokens)\n",
    "    s2_tokens = remove_duplicate_tokens(s2_tokens)\n",
    "        \n",
    "    overlap = 0\n",
    "    encountered_words = []\n",
    "    for word in (s1_tokens+s2_tokens):\n",
    "        try:\n",
    "            if word in encountered_words: # we know we have found an overlap\n",
    "                overlap += 1\n",
    "            encountered_words.append(word)\n",
    "        except ValueError:\n",
    "            # print(word + ' not found in lexicon. Skipping...')\n",
    "            continue\n",
    "\n",
    "    avg_sentence_len = len(s1_tokens+s2_tokens) / 2\n",
    "    \n",
    "    overlap_normlalized = overlap / avg_sentence_len\n",
    "    return overlap, overlap_normlalized\n",
    "\n",
    "s1s2_word_raw, s1s2_word_norm = calc_basic_overlap(s1_tok, s2_tok)\n",
    "s3s4_word_raw, s3s4_word_norm = calc_basic_overlap(s3_tok, s4_tok)\n",
    "s3s3_word_raw, s3s3_word_norm = calc_basic_overlap(s3_tok, s3_tok)\n",
    "\n",
    "# Close Example\n",
    "print('{:15} {:<5} {:<10.3}'.format('similar:',  s1s2_word_raw, s1s2_word_norm))\n",
    "\n",
    "# Different Example\n",
    "print('{:15} {:<5} {:<10.3}'.format('dessimilar:',  s3s4_word_raw, s3s4_word_norm))\n",
    "\n",
    "# Same Example\n",
    "print('{:15} {:<5} {:<10.3}'.format('same:',  s3s3_word_raw, s3s3_word_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synset Overlap\n",
    "\n",
    "\n",
    "*NOTE* This feature also ingests tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar:        13    0.306     \n",
      "dissimilar:     0     0.0       \n",
      "same:           13    1.0       \n"
     ]
    }
   ],
   "source": [
    "def calc_synset_overlap(s1_tokens, s2_tokens):\n",
    "    s1_tokens = remove_stopwords(s1_tokens)\n",
    "    s1_tokens = remove_duplicate_tokens(s1_tokens)\n",
    "\n",
    "    s2_tokens = remove_stopwords(s2_tokens)\n",
    "    s2_tokens = remove_duplicate_tokens(s2_tokens)\n",
    "    \n",
    "#     print(s2_tokens)\n",
    "#     print(s1_tokens)\n",
    "\n",
    "    s1_spread = []\n",
    "    s2_spread = []\n",
    "    \n",
    "    for word in s1_tokens:\n",
    "        for synset in wn.synsets(word):\n",
    "            for i in range(0, len(synset.lemmas())):\n",
    "                syn_word = synset.lemmas()[i].name()\n",
    "                if syn_word not in s1_spread:\n",
    "                    s1_spread.append(syn_word)\n",
    "\n",
    "    for word in s2_tokens:\n",
    "        for synset in wn.synsets(word):\n",
    "            for i in range(0, len(synset.lemmas())):\n",
    "                syn_word = synset.lemmas()[i].name()\n",
    "                if syn_word not in s2_spread:\n",
    "                    s2_spread.append(syn_word)         \n",
    "    \n",
    "    return calc_basic_overlap(s1_spread, s2_spread)\n",
    "    \n",
    "s1s2_syn_raw, s1s2_syn_norm = calc_synset_overlap(s1_tok, s2_tok)\n",
    "s3s4_syn_raw, s3s4_syn_norm = calc_synset_overlap(s3_tok, s4_tok)\n",
    "s3s3_syn_raw, s3s3_syn_norm = calc_synset_overlap(s3_tok, s3_tok)\n",
    "\n",
    "# Close Example\n",
    "print('{:15} {:<5} {:<10.3}'.format('similar:',  s1s2_syn_raw, s1s2_syn_norm))\n",
    "\n",
    "# Different Example\n",
    "print('{:15} {:<5} {:<10.3}'.format('dissimilar:',  s3s4_syn_raw, s3s4_syn_norm))\n",
    "\n",
    "# Same Example\n",
    "print('{:15} {:<5} {:<10.3}'.format('same:',  s3s3_syn_raw, s3s3_syn_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path Similarity\n",
    "\n",
    "Path similarity scores how similar two word senses are by computing  shortest number of edges from one word sense to another word sense, assuming a hierarchical structure like WordNet  in the is-a (hypernym/hypnoym) taxonomy. In general, word senses which have a longer path distance are less similar than those with a very short path distance. Path similarity is not commutative by design, hence we have defined another function for computing a symmetric similarity which takes the average of two cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar:        0.0       \n",
      "dissimilar:     0.0       \n",
      "same:           0.0       \n"
     ]
    }
   ],
   "source": [
    "def get_synsets(sentence1, sentence2):\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tag_to_synset(sentence1, *tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tag_to_synset(sentence2, *tagged_word) for tagged_word in sentence2]\n",
    "    \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [syn for syn in synsets1 if syn]\n",
    "    synsets2 = [syn for syn in synsets2 if syn]\n",
    "\n",
    "    return synsets1, synsets2\n",
    "\n",
    "def postag_to_synsettag(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if tag.startswith('V') or tag == \"MD\":\n",
    "        return 'v'\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return\n",
    "\n",
    "def tag_to_synset(sent, word, tag):\n",
    "    wn_tag = postag_to_synsettag(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            return lesk(sent, word, wn_tag)\n",
    "        except:\n",
    "            return None\n",
    "\n",
    "\n",
    "def sentence_path_similarity(sentence1, sentence2):\n",
    "    synsets1, synsets2 = get_synsets(sentence1, sentence2)\n",
    "    score, count = 0.0, 0\n",
    "    best = 0.0\n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        for synset2 in synsets2:\n",
    "            if synset.path_similarity(synset2) is not None and synset.path_similarity(synset2) > best:\n",
    "                try:\n",
    "                    best = synset.path_similarity(synset2)\n",
    "                except TypeError:\n",
    "                    continue\n",
    "        # Check that the similarity could have been computed\n",
    "        if best is not None:\n",
    "            score += best\n",
    "            count += 1\n",
    "    # Average the values\n",
    "    try:\n",
    "        score /= count\n",
    "    except:\n",
    "        return 0.0\n",
    "    return score\n",
    "\n",
    "\n",
    "def symmetric_sentence_path_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\n",
    "    return (sentence_path_similarity(sentence1, sentence2) + sentence_path_similarity(sentence2, sentence1)) / 2     \n",
    "\n",
    "\n",
    "\n",
    "sps_ts1 = \"The American Anglican Council, which represents Episcopalian conservatives, said it will seek authorization to create a separate group.\"\n",
    "sps_ts2 = \"The American Anglican Council, which represents Episcopalian conservatives, said it will seek authorization to create a separate province in North America because of last week's actions.\"\n",
    "\n",
    "\n",
    "# Close Example\n",
    "print('{:15} {:<10.3}'.format('similar:', symmetric_sentence_path_similarity(sps_ts2, sps_ts2) ))\n",
    "\n",
    "# Different Example\n",
    "print('{:15} {:<10.3}'.format('dissimilar:', symmetric_sentence_path_similarity(s3, s4)) )\n",
    "\n",
    "# Same Example\n",
    "print('{:15} {:<10.3}'.format('same:', symmetric_sentence_path_similarity(s3, s3)) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similar:        0.0       \n",
      "dissimilar:     0.0       \n",
      "same:           0.0       \n"
     ]
    }
   ],
   "source": [
    "def named_entity_overlap(s1, s2):    \n",
    "  \n",
    "    sentence_nlp1= nlp(s1)\n",
    "    ner1= [(word.text, word.ent_type_) for word in sentence_nlp1 if word.ent_type_]\n",
    "    \n",
    "    sentence_nlp2=nlp(s2)\n",
    "    ner2 = [(word.text, word.ent_type_) for word in sentence_nlp2 if word.ent_type_]\n",
    "\n",
    "    overlap = []\n",
    "\n",
    "    \n",
    "    da = {k:v for k,v in ner1}\n",
    "    db = {k:v for k,v in ner2}\n",
    "    total_length = len(set(ner1+ner2))\n",
    "    temp = []\n",
    "    for a in da.keys():\n",
    "        for b in db.keys():\n",
    "            if a==b:\n",
    "                temp.insert(len(temp), a)\n",
    "    if total_length != 0:\n",
    "        overlap =len(temp)/total_length\n",
    "    else:\n",
    "        overlap = 0.0\n",
    "\n",
    "    return overlap\n",
    "\n",
    "# Close Example\n",
    "print('{:15} {:<10.3}'.format('similar:', named_entity_overlap(s1, s2)) )\n",
    "\n",
    "# Different Example\n",
    "print('{:15} {:<10.3}'.format('dissimilar:', named_entity_overlap(s3, s4)) )\n",
    "\n",
    "# Same Example\n",
    "print('{:15} {:<10.3}'.format('same:', named_entity_overlap(s3, s3)) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4\n",
    "\n",
    "- Load in model with Pickle Dump\n",
    "- Preprocess test data\n",
    "- Run it through the model\n",
    "- Output a file of our models guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filename = \"./model/random_forest_46.pkl\"\n",
    "test_data = preprocess(\"./data/test-set.txt\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daytonpe/anaconda3/lib/python3.7/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 0.19.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n",
      "/Users/daytonpe/anaconda3/lib/python3.7/site-packages/sklearn/base.py:251: UserWarning: Trying to unpickle estimator RandomForestClassifier from version 0.19.1 when using version 0.20.1. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "with open(pkl_filename, 'rb') as file:\n",
    "    pickle_model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes about 30 seconds to preprocess the test data\n",
    "def pipeline(corpus):\n",
    "   \n",
    "    s1_array = corpus['s1']['sentences']\n",
    "    s2_array = corpus['s2']['sentences']\n",
    "    s1_tokens = corpus['s1']['tokens']\n",
    "    s2_tokens = corpus['s2']['tokens']\n",
    "    s1_lemmas = corpus['s1']['lemmas']\n",
    "    s2_lemmas = corpus['s2']['lemmas']\n",
    "    s1_ls = corpus['s2']['ls']\n",
    "    s2_ls = corpus['s2']['ls']\n",
    "    \n",
    "    data = []\n",
    "    for i in range(0, len(s1_array)):\n",
    "        cos_sim = calc_cosine_similarity(s1_array[i], s2_array[i])\n",
    "        sif_sim = calc_sif_similarity(s1_array[i], s2_array[i])\n",
    "        w_overlap, w_norm_overlap = calc_basic_overlap(s1_tokens[i], s2_tokens[i])\n",
    "        l_overlap, l_norm_overlap = calc_basic_overlap(s1_lemmas[i], s2_lemmas[i])\n",
    "        spacy_sim = calc_spacy_sim(s1_array[i], s2_array[i])\n",
    "        syn_overlap, normalized_syn_overlap = calc_synset_overlap(s1_tokens[i], s2_tokens[i])\n",
    "        path_similarity = symmetric_sentence_path_similarity(s1_ls[i], s2_ls[i]) \n",
    "        ne_overlap = named_entity_overlap(s1_array[i], s2_array[i])\n",
    "        data.insert(len(data),[len(s1_tokens), len(s2_tokens), w_norm_overlap, l_norm_overlap, spacy_sim, sif_sim, cos_sim, syn_overlap, normalized_syn_overlap, path_similarity, ne_overlap, len(s1_tokens[i]), len(s2_tokens[i])])\n",
    "\n",
    "    return data\n",
    "\n",
    "test_input = pipeline(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = pickle_model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "751\n"
     ]
    }
   ],
   "source": [
    "# f= open(\"D:\\\\UTD\\\\NLPProject\\\\v3\\\\semantic-textual-similarity\\\\dev-set-predicted-answers.txt\",\"w+\") # shruti\n",
    "f= open(\"./output/test-set-predicted-answers.txt\",\"w+\") # pat\n",
    "\n",
    "f.write(\"id\\tGoldTag\\n\")\n",
    "i=1\n",
    "for x in test_predictions:\n",
    "    f.write(\"p_%d\\t%d\\n\" % (i,x))\n",
    "    i+=1\n",
    "print(i)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
