{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 6320 Natural Language Processing\n",
    "## Shruti Agrawal & Pat Dayton\n",
    "\n",
    "This notebook demos our code for Tasks 1 & 2 of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.corpus import wordnet as wn\n",
    "import pandas as pd\n",
    "core_nlp_url = 'http://localhost:9000'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the Stanford CoreNLP Server\n",
    "In another console run the script below in order to start the Stanford CoreNLP Server on port 9000. We will hit this API in Part 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#java -mx4g -cp \"./corenlp/*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 9000 -timeout 15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Parse the Corpus\n",
    "First read in the corpus and do basic parsing to split out the first sentence, second sentence, and score for each line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readData(fileName):\n",
    "    \"\"\"Read in the file, strip out sentence 1, sentence 2, and score\"\"\"\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    score = []\n",
    "    file = open(fileName, encoding=\"utf8\")\n",
    "    text = file.readline()\n",
    "    text = file.read()\n",
    "    \n",
    "    # loop to extract a set of two sentences\n",
    "    for sentence in text.split('\\n'):\n",
    "\n",
    "        # creating two separate lists of the sentences\n",
    "        # '.rstrip('.') only removes the last period in the sentence\n",
    "        \n",
    "        s1.insert(len(s1), (sentence.split('\\t')[1].lower()).rstrip('.'))\n",
    "        s2.insert(len(s1), (sentence.split('\\t')[2].lower()).rstrip('.'))\n",
    "        \n",
    "        # inserting the score as a separate lists\n",
    "        score.insert(len(s1), (sentence.split('\\t')[3]))\n",
    "\n",
    "    # print(s1)\n",
    "    return s1, s2, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(fileName):\n",
    "\n",
    "    s1, s2, scores = readData(fileName)\n",
    "    s1_toks = []\n",
    "    s2_toks = []\n",
    "\n",
    "    # tokenizing and tagging\n",
    "    s1_tags = []\n",
    "    s2_tags = []\n",
    "\n",
    "    for sentence in s1:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        s1_toks.insert(len(s1_toks), tokens)\n",
    "        s1_tags.insert(\n",
    "            len(s1_tags), nltk.pos_tag(tokens))\n",
    "\n",
    "    for sentence in s2:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        s2_toks.insert(len(s2_toks), tokens)\n",
    "        s2_tags.insert(\n",
    "            len(s2_tags), nltk.pos_tag(tokens))\n",
    "    \n",
    "    # Remove the unnecessary tuple and keep just the tags\n",
    "    for i, tag_list in enumerate(s1_tags):\n",
    "        s1_tags[i] = [tup[1] for tup in tag_list]\n",
    "    for i, tag_list in enumerate(s2_tags):\n",
    "        s2_tags[i] = [tup[1] for tup in tag_list]\n",
    "\n",
    "    # lemmatizing\n",
    "    s1_lemmas = []\n",
    "    s2_lemmas = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for sentence in s1_toks:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        s1_lemmas.insert(\n",
    "            len(s1_lemmas), sentence_components)\n",
    "\n",
    "    for sentence in s2_toks:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        s2_lemmas.insert(\n",
    "            len(s2_lemmas), sentence_components)\n",
    "\n",
    "        \n",
    "    # Zipping it all together into one object for each word\n",
    "    s1_word_lists = []\n",
    "    s2_word_lists = []\n",
    "    \n",
    "    for tok_list, lem_list, tag_list in zip(s1_toks, s1_lemmas, s1_tags):\n",
    "        sentence_words = []\n",
    "        for tok, lem, tag in zip(tok_list, lem_list, tag_list):\n",
    "            word = {}\n",
    "            word['tok'] = tok\n",
    "            word['lem'] = lem\n",
    "            word['tag'] = tag\n",
    "            sentence_words.append(word)\n",
    "        s1_word_lists.append(sentence_words) \n",
    "        \n",
    "    for tok_list, lem_list, tag_list in zip(s2_toks, s2_lemmas, s2_tags):\n",
    "        sentence_words = []\n",
    "        for tok, lem, tag in zip(tok_list, lem_list, tag_list):\n",
    "            word = {}\n",
    "            word['tok'] = tok\n",
    "            word['lem'] = lem\n",
    "            word['tag'] = tag\n",
    "            sentence_words.append(word)\n",
    "        s2_word_lists.append(sentence_words)  \n",
    "              \n",
    "    \n",
    "    # Create a corpus object to represent our corpus\n",
    "    corpus = {}\n",
    "    corpus[\"s1\"] = {}\n",
    "    corpus[\"s2\"] = {}\n",
    "    corpus['scores'] = [int(i) for i in scores]\n",
    "    \n",
    "    corpus[\"s1\"][\"sentences\"] = s1\n",
    "    corpus[\"s2\"][\"sentences\"] = s2\n",
    "    \n",
    "    corpus[\"s1\"][\"tokens\"] = s1_toks\n",
    "    corpus[\"s2\"][\"tokens\"] = s2_toks\n",
    "    \n",
    "    corpus[\"s1\"][\"lemmas\"] = s1_lemmas\n",
    "    corpus[\"s2\"][\"lemmas\"] = s2_lemmas\n",
    "    \n",
    "    corpus[\"s1\"][\"tags\"] = s1_tags\n",
    "    corpus[\"s2\"][\"tags\"] = s2_tags\n",
    "    \n",
    "    corpus[\"s1\"][\"words\"] = s1_word_lists\n",
    "    corpus[\"s2\"][\"words\"] = s2_word_lists\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 Example Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess(\"./data/train-set.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1484\n",
      "1484\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data[\"s1\"]['sentences']))\n",
    "print(len(train_data[\"s2\"]['sentences']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROW 286 FROM TEST DATA\n",
      "\n",
      "Sentence 1\n",
      "\n",
      "Raw:  gemstar's shares gathered up 2.6 percent, adding 14 cents to $5.49 at the close\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemstar</td>\n",
       "      <td>gemstar</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'s</td>\n",
       "      <td>'s</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shares</td>\n",
       "      <td>share</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gathered</td>\n",
       "      <td>gathered</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>RP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>percent</td>\n",
       "      <td>percent</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>adding</td>\n",
       "      <td>adding</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cents</td>\n",
       "      <td>cent</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>to</td>\n",
       "      <td>to</td>\n",
       "      <td>TO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5.49</td>\n",
       "      <td>5.49</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>close</td>\n",
       "      <td>close</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tokens    Lemmas Tags\n",
       "0    gemstar   gemstar   NN\n",
       "1         's        's  POS\n",
       "2     shares     share  NNS\n",
       "3   gathered  gathered  VBD\n",
       "4         up        up   RP\n",
       "5        2.6       2.6   CD\n",
       "6    percent   percent   NN\n",
       "7          ,         ,    ,\n",
       "8     adding    adding  VBG\n",
       "9         14        14   CD\n",
       "10     cents      cent  NNS\n",
       "11        to        to   TO\n",
       "12         $         $    $\n",
       "13      5.49      5.49   CD\n",
       "14        at        at   IN\n",
       "15       the       the   DT\n",
       "16     close     close   NN"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 2\n",
      "\n",
      "Raw:  gemstar shares moved higher on the news, closing up 2.6 percent at $5.49 on nasdaq\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>Tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gemstar</td>\n",
       "      <td>gemstar</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shares</td>\n",
       "      <td>share</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>moved</td>\n",
       "      <td>moved</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>higher</td>\n",
       "      <td>higher</td>\n",
       "      <td>RBR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>news</td>\n",
       "      <td>news</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "      <td>,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>closing</td>\n",
       "      <td>closing</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>up</td>\n",
       "      <td>up</td>\n",
       "      <td>RP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.6</td>\n",
       "      <td>2.6</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>percent</td>\n",
       "      <td>percent</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>at</td>\n",
       "      <td>at</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "      <td>$</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.49</td>\n",
       "      <td>5.49</td>\n",
       "      <td>CD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>on</td>\n",
       "      <td>on</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nasdaq</td>\n",
       "      <td>nasdaq</td>\n",
       "      <td>NNS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Tokens   Lemmas Tags\n",
       "0   gemstar  gemstar   NN\n",
       "1    shares    share  NNS\n",
       "2     moved    moved  VBD\n",
       "3    higher   higher  RBR\n",
       "4        on       on   IN\n",
       "5       the      the   DT\n",
       "6      news     news   NN\n",
       "7         ,        ,    ,\n",
       "8   closing  closing  VBG\n",
       "9        up       up   RP\n",
       "10      2.6      2.6   CD\n",
       "11  percent  percent   NN\n",
       "12       at       at   IN\n",
       "13        $        $    $\n",
       "14     5.49     5.49   CD\n",
       "15       on       on   IN\n",
       "16   nasdaq   nasdaq  NNS"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  4\n"
     ]
    }
   ],
   "source": [
    "r=286\n",
    "\n",
    "tkns1 = train_data[\"s1\"]['tokens'][r]\n",
    "lems1 = train_data[\"s1\"]['lemmas'][r]\n",
    "tags1 = train_data[\"s1\"]['tags'][r]\n",
    "tkns2 = train_data[\"s2\"]['tokens'][r]\n",
    "lems2 = train_data[\"s2\"]['lemmas'][r]\n",
    "tags2 = train_data[\"s2\"]['tags'][r]\n",
    "\n",
    "data1 = []\n",
    "data2 = []\n",
    "\n",
    "for i in range(0, len(tkns1)):\n",
    "    data1.append([tkns1[i], lems1[i], tags1[i]])\n",
    "    \n",
    "for i in range(0, len(tkns2)):\n",
    "    data2.append([tkns2[i], lems2[i], tags2[i]])\n",
    "    \n",
    "df1 = pd.DataFrame(\n",
    "    data1, \n",
    "    columns = ['Tokens', 'Lemmas', 'Tags']) \n",
    "\n",
    "df2 = pd.DataFrame(\n",
    "    data2, \n",
    "    columns = ['Tokens', 'Lemmas', 'Tags']) \n",
    "\n",
    "\n",
    "print('ROW {} FROM TEST DATA\\n'.format(r))\n",
    "print('Sentence 1\\n')\n",
    "print('Raw: ', train_data[\"s1\"]['sentences'][r])\n",
    "display(df1)\n",
    "print('Sentence 2\\n')\n",
    "print('Raw: ', train_data[\"s2\"]['sentences'][r])\n",
    "display(df2)\n",
    "print('Score: ', train_data[\"scores\"][r])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dependency Parsing Sentence 1\n",
      "\n",
      "gemstar\tNN\t3\tnmod:poss\n",
      "'s\tPOS\t1\tcase\n",
      "shares\tNNS\t4\tnsubj\n",
      "gathered\tVBD\t0\tROOT\n",
      "up\tRP\t4\tcompound:prt\n",
      "2.6\tCD\t7\tnummod\n",
      "percent\tNN\t4\tdobj\n",
      ",\t,\t4\tpunct\n",
      "adding\tVBG\t4\tadvcl\n",
      "14\tCD\t11\tnummod\n",
      "cents\tNNS\t9\tdobj\n",
      "to\tTO\t14\tcase\n",
      "$\t$\t14\tdep\n",
      "5.49\tCD\t9\tnmod\n",
      "at\tIN\t17\tcase\n",
      "the\tDT\t17\tdet\n",
      "close\tNN\t9\tnmod\n",
      "\n",
      "\n",
      "Dependency Parsing Sentence 2\n",
      "\n",
      "gemstar\tJJ\t2\tamod\n",
      "shares\tNNS\t3\tnsubj\n",
      "moved\tVBD\t0\tROOT\n",
      "higher\tRBR\t3\tadvmod\n",
      "on\tIN\t7\tcase\n",
      "the\tDT\t7\tdet\n",
      "news\tNN\t3\tnmod\n",
      ",\t,\t3\tpunct\n",
      "closing\tVBG\t3\tadvcl\n",
      "up\tRP\t9\tcompound:prt\n",
      "2.6\tCD\t12\tnummod\n",
      "percent\tNN\t9\tdobj\n",
      "at\tIN\t15\tcase\n",
      "$\t$\t15\tdep\n",
      "5.49\tCD\t9\tnmod\n",
      "on\tIN\t17\tcase\n",
      "nasdaq\tNN\t9\tnmod\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dependency parsing\n",
    "print(\"\\nDependency Parsing Sentence 1\\n\")\n",
    "dependency_parser = CoreNLPDependencyParser(url=core_nlp_url)\n",
    "parse, = dependency_parser.raw_parse(train_data[\"s1\"]['sentences'][r])\n",
    "print(parse.to_conll(4))\n",
    "\n",
    "print(\"\\nDependency Parsing Sentence 2\\n\")\n",
    "dependency_parser = CoreNLPDependencyParser(url=core_nlp_url)\n",
    "parse, = dependency_parser.raw_parse(train_data[\"s2\"]['sentences'][r])\n",
    "print(parse.to_conll(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Syntactic Parsing\n",
    "https://www.nltk.org/api/nltk.parse.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full syntactic parse tree for sentence 1: \n",
      "                                                         ROOT                                                  \n",
      "                                                          |                                                     \n",
      "                                                          S                                                    \n",
      "              ____________________________________________|_____                                                \n",
      "             |                                                  VP                                             \n",
      "             |             _____________________________________|_________________                              \n",
      "             |            |      |       |           |                            S                            \n",
      "             |            |      |       |           |                            |                             \n",
      "             |            |      |       |           |                            VP                           \n",
      "             |            |      |       |           |     _______________________|_______                      \n",
      "             |            |      |       |           |    |         |                     PP                   \n",
      "             |            |      |       |           |    |         |          ___________|____                 \n",
      "             |            |      |       |           |    |         |         |                NP              \n",
      "             |            |      |       |           |    |         |         |        ________|___             \n",
      "             NP           |      |       |           |    |         |         |       |            PP          \n",
      "          ___|____        |      |       |           |    |         |         |       |         ___|___         \n",
      "         NP       |       |     PRT      NP          |    |         NP        |       NP       |       NP      \n",
      "    _____|___     |       |      |    ___|_____      |    |      ___|____     |    ___|___     |    ___|____    \n",
      "   NN       POS  NNS     VBD     RP  CD        NN    ,   VBG    CD      NNS   TO  $       CD   IN  DT       NN \n",
      "   |         |    |       |      |   |         |     |    |     |        |    |   |       |    |   |        |   \n",
      "gemstar      's shares gathered  up 2.6     percent  ,  adding  14     cents  to  $      5.49  at the     close\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# syntactic parsing\n",
    "print(\"\\nFull syntactic parse tree for sentence 1: \")\n",
    "syntactic_parser = CoreNLPParser(url=core_nlp_url)\n",
    "s1_tree = next(syntactic_parser.raw_parse(train_data[\"s1\"]['sentences'][r]))\n",
    "s1_tree.pretty_print()\n",
    "\n",
    "# type(s1_tree)\n",
    "# s1_parse_tree_file = open(\"./output/s1_parse_tree.txt\", \"w\") \n",
    "# s1_parse_tree_file.write(str(s1_tree))\n",
    "# s1_parse_tree_file.close()\n",
    "\n",
    "f = open(\"./output/s1_parse_tree.txt\", \"w\", encoding=\"utf-8\")\n",
    "s1_tree.pretty_print(stream=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full syntactic parse tree for sentence 1: \n",
      "                                                       ROOT                                                         \n",
      "                                                        |                                                            \n",
      "                                                        S                                                           \n",
      "          ______________________________________________|_____                                                       \n",
      "         |                                                    VP                                                    \n",
      "         |            ________________________________________|___________________                                   \n",
      "         |           |     |         |            |                               S                                 \n",
      "         |           |     |         |            |                               |                                  \n",
      "         |           |     |         |            |                               VP                                \n",
      "         |           |     |         |            |      _________________________|___                               \n",
      "         |           |     |         |            |     |     |                       NP                            \n",
      "         |           |     |         |            |     |     |        _______________|_______                       \n",
      "         |           |     |         |            |     |     |       |                       PP                    \n",
      "         |           |     |         |            |     |     |       |            ___________|____                  \n",
      "         |           |     |         |            |     |     |       |           |                NP               \n",
      "         |           |     |         |            |     |     |       |           |        ________|_______          \n",
      "         |           |     |         PP           |     |     |       |           |       |                PP       \n",
      "         |           |     |      ___|___         |     |     |       |           |       |             ___|____     \n",
      "         NP          |    ADVP   |       NP       |     |    PRT      NP          |       NP           |        NP  \n",
      "    _____|____       |     |     |    ___|___     |     |     |    ___|_____      |    ___|___         |        |    \n",
      "   JJ        NNS    VBD   RBR    IN  DT      NN   ,    VBG    RP  CD        NN    IN  $       CD       IN       NN  \n",
      "   |          |      |     |     |   |       |    |     |     |   |         |     |   |       |        |        |    \n",
      "gemstar     shares moved higher  on the     news  ,  closing  up 2.6     percent  at  $      5.49      on     nasdaq\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# syntactic parsing\n",
    "print(\"\\nFull syntactic parse tree for sentence 1: \")\n",
    "syntactic_parser = CoreNLPParser(url=core_nlp_url)\n",
    "s1_tree = next(syntactic_parser.raw_parse(train_data[\"s2\"]['sentences'][r]))\n",
    "s1_tree.pretty_print()\n",
    "\n",
    "f = open(\"./output/s2_parse_tree.txt\", \"w\", encoding=\"utf-8\")\n",
    "s1_tree.pretty_print(stream=f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordnet Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***********************************************************************************\n",
      "gemstar NN\n",
      "\n",
      "Synonyms:  []\n",
      "\n",
      "Hypernyms:  []\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "shares NN\n",
      "\n",
      "Synonyms:  ['share', 'portion', 'part', 'percentage', 'parcel', 'contribution', 'plowshare', 'ploughshare', 'partake', 'partake_in', 'divvy_up', 'portion_out', 'apportion', 'deal']\n",
      "\n",
      "Hypernyms:  ['assets', 'stock_certificate', 'stock', 'allotment', 'apportionment', 'apportioning', 'allocation', 'parceling', 'parcelling', 'assignation', 'attempt', 'effort', 'endeavor', 'endeavour', 'try', 'wedge', 'overlap', 'use', 'utilize', 'utilise', 'apply', 'employ', 'get', 'acquire', 'distribute', 'give_out', 'hand_out', 'pass_out', 'communicate', 'intercommunicate']\n",
      "\n",
      "Hyponyms:  ['allotment', 'allocation', 'allowance', 'cut', 'dispensation', 'dole', 'interest', 'stake', 'profit_sharing', 'ration', 'slice', 'piece', 'split', 'tranche', 'way', 'end', 'osculate', 'partake', 'communalize', 'communalise', 'double_up', 'pool', 'cut_in']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "moved NN\n",
      "\n",
      "Synonyms:  ['travel', 'go', 'move', 'locomote', 'displace', 'proceed', 'be_active', 'act', 'affect', 'impress', 'strike', 'motivate', 'actuate', 'propel', 'prompt', 'incite', 'run', 'make_a_motion', 'moved', 'affected', 'stirred', 'touched']\n",
      "\n",
      "Hypernyms:  ['act', 'move', 'change', 'alter', 'vary', 'cause', 'do', 'make', 'affect', 'impress', 'strike', 'sell', 'live', 'propose', 'suggest', 'advise']\n",
      "\n",
      "Hyponyms:  ['accompany', 'advance', 'progress', 'pass_on', 'move_on', 'march_on', 'go_on', 'angle', 'ascend', 'go_up', 'automobile', 'back', 'bang', 'beetle', 'betake_oneself', 'billow', 'bounce', 'jounce', 'breeze', 'caravan', 'career', 'carry', 'circle', 'circulate', 'circuit', 'come', 'come_up', 'crawl', 'creep', 'cruise', 'derail', 'jump', 'descend', 'fall', 'go_down', 'come_down', 'do', 'drag', 'draw', 'drive', 'motor', 'ease', 'ferry', 'float', 'drift', 'be_adrift', 'blow', 'swim', 'flock', 'fly', 'wing', 'follow', 'travel_along', 'forge', 'spurt', 'spirt', 'get_around', 'get_about', 'ghost', 'glide', 'go_around', 'spread', 'hiss', 'whoosh', 'hurtle', 'island_hop', 'lance', 'lurch', 'outflank', 'pace', 'pan', 'pass', 'go_through', 'go_across', 'pass_over', 'overfly', 'play', 'plow', 'plough', 'prance', 'precede', 'lead', 'precess', 'proceed', 'go_forward', 'continue', 'propagate', 'pursue', 'push', 'raft', 'repair', 'resort', 'retreat', 'retrograde', 'return', 'ride', 'sit', 'rise', 'lift', 'arise', 'move_up', 'uprise', 'roll', 'wander', 'swan', 'stray', 'tramp', 'roam', 'cast', 'ramble', 'rove', 'range', 'vagabond', 'round', 'run', 'rush', 'hotfoot', 'hasten', 'hie', 'speed', 'race', 'pelt_along', 'rush_along', 'cannonball_along', 'bucket_along', 'belt_along', 'step_on_it', 'scramble', 'seek', 'shuttle', 'sift', 'ski', 'slice_into', 'slice_through', 'slither', 'slide', 'snowshoe', 'steamer', 'steam', 'step', 'tread', 'err', 'swap', 'swash', 'swing', 'taxi', 'trail', 'shack', 'tram', 'transfer', 'change', 'travel', 'journey', 'move_around', 'travel_by', 'pass_by', 'surpass', 'go_past', 'go_by', 'travel_purposefully', 'travel_rapidly', 'hurry', 'zip', 'trundle', 'turn', 'walk', 'take_the_air', 'weave', 'wind', 'thread', 'meander', 'wend', 'wheel', 'whine', 'whish', 'whisk', 'whistle', 'withdraw', 'pull_away', 'draw_back', 'recede', 'pull_back', 'retire', 'move_back', 'zigzag', 'crank', 'zoom', 'bring_forward', 'agitate', 'vex', 'disturb', 'commove', 'shake_up', 'stir_up', 'raise_up', 'beat', 'flap', 'brandish', 'flourish', 'wave', 'center', 'centre', 'change_hands', 'change_owners', 'chase_away', 'drive_out', 'turn_back', 'drive_away', 'dispel', 'drive_off', 'run_off', 'pass_around', 'distribute', 'dandle', 'disarrange', 'dislocate', 'luxate', 'splay', 'slip', 'displace', 'drop', 'engage', 'mesh', 'lock', 'operate', 'expel', 'throw_out', 'kick_out', 'exteriorize', 'bring_outside', 'flick', 'ruffle', 'riffle', 'fluctuate', 'funnel', 'herd', 'crowd', 'hit', 'strike', 'hustle', 'jar', 'bump_around', 'lateralize', 'launch', 'set_in_motion', 'raise', 'lower', 'take_down', 'let_down', 'get_down', 'bring_down', 'mobilize', 'mobilise', 'overturn', 'tip_over', 'turn_over', 'upset', 'knock_over', 'bowl_over', 'tump_over', 'pour', 'press_down', 'depress', 'propel', 'impel', 'pull', 'force', 'pulse', 'pump', 'put', 'set', 'place', 'pose', 'position', 'lay', 'elevate', 'get_up', 'bring_up', 'rake', 'relocate', 'rock', 'sway', 'revolve', 'rout_out', 'force_out', 'rouse', 'saltate', 'scan', 'send', 'direct', 'separate', 'disunite', 'divide', 'part', 'shift', 'dislodge', 'reposition', 'singsong', 'sink', 'sling', 'spill', 'slop', 'splatter', 'shed', 'disgorge', 'station', 'post', 'stir', 'take_back', 'translate', 'transmit', 'transport', 'channel', 'channelize', 'channelise', 'ship', 'tug', 'unseat', 'unwind', 'wind_off', 'unroll', 'uproot', 'extirpate', 'deracinate', 'root_out', 'upstage', 'wash', 'wedge', 'squeeze', 'wrap', 'twine', 'woosh', 'work', 'arouse', 'assume', 'take', 'take_up', 'pound', 'thump', 'bob', 'bolt', 'brush', 'sweep', 'buck', 'jerk', 'hitch', 'bustle', 'bustle_about', 'cant', 'cant_over', 'tilt', 'slant', 'pitch', 'careen', 'wobble', 'chop', 'churn', 'boil', 'moil', 'roil', 'climb', 'close', 'come_together', 'crash', 'cut', 'cut_to', 'dance', 'trip_the_light_fantastic', 'trip_the_light_fantastic_toe', 'diverge', 'dodge', 'drop_back', 'duck', 'exit', 'go_out', 'get_out', 'leave', 'falter', 'waver', 'fidget', 'flex', 'bend', 'flinch', 'squinch', 'funk', 'cringe', 'shrink', 'wince', 'recoil', 'quail', 'fling', 'flip', 'twitch', 'flow', 'flux', 'flurry', 'grab', 'gravitate', 'heave', 'hit_the_dirt', 'hit_the_deck', 'hop', 'hop_on', 'mount', 'mount_up', 'get_on', 'jump_on', 'climb_on', 'bestride', 'jolt', 'leap', 'bound', 'spring', 'jump_off', 'linger', 'dawdle', 'list', 'lean', 'lunge', 'hurl', 'thrust', 'make_way', 'mill', 'mill_about', 'mill_around', 'mope', 'mope_around', 'move_back_and_forth', 'move_involuntarily', 'move_reflexively', 'move_over', 'give_way', 'give', 'ease_up', 'yield', 'nod', 'pulsate', 'quiver', 'putter', 'potter', 'potter_around', 'putter_around', 'quicken', 'reach', 'reach_out', 'reciprocate', 'undulate', 'feed', 'course', 'seesaw', 'split', 'shake', 'sidle', 'sashay', 'snap', 'click', 'startle', 'start', 'steal', 'budge', 'streak', 'stretch', 'stretch_out', 'strike_out', 'stumble', 'trip', 'sail', 'swoop', 'teeter', 'totter', 'throw', 'thunder', 'vibrate', 'wallow', 'welter', 'wamble', 'waggle', 'whirl', 'tumble', 'whirl_around', 'coggle', 'writhe', 'wrestle', 'wriggle', 'worm', 'squirm', 'twist', 'evacuate', 'migrate', 'transmigrate', 'move_in', 'move_out', 'steamroller', 'steamroll', 'venture', 'embark', 'bestir', 'scroll', 'lapse', 'act_on', 'alternate', 'take_turns', 'antagonize', 'antagonise', 'counteract', 'anticipate', 'foresee', 'forestall', 'counter', 'attack', 'aggress', 'begin', 'behave', 'acquit', 'bear', 'deport', 'conduct', 'comport', 'coact', 'come_close', 'come_to_the_fore', 'step_forward', 'come_forward', 'step_up', 'step_to_the_fore', 'come_out', 'condescend', 'deign', 'stoop', 'lower_oneself', 'go_along', 'keep', 'persist_in', 'cope', 'get_by', 'make_out', 'make_do', 'contend', 'grapple', 'deal', 'manage', 'court', 'create', 'dally', 'toy', 'flirt', 'dare', 'dispatch', 'do_well', 'had_best', 'effect', 'egotrip', 'evade', 'exert', 'finish_up', 'land_up', 'fetch_up', 'end_up', 'wind_up', 'finish', 'get_around_to', 'go', 'move', 'go_ahead', 'plow_ahead', 'go_off_half-cocked', 'go_off_at_half-cock', 'guard', 'interact', 'interrupt', 'lord_it_over', 'queen_it_over', 'put_on_airs', 'act_superior', 'make_a_point', 'make_sure', 'make_bold', 'presume', 'maneuver', 'manoeuver', 'manoeuvre', 'misbehave', 'misconduct', 'misdemean', 'participate', 'take_part', 'partner', 'perform', 'perpetrate', 'commit', 'play_it_by_ear', 'prosecute', 'rampage', 'react', 'respond', 'oppose', 'repeat', 'take_over', 'reward', 'repay', 'pay_back', 'look_sharp', 'festinate', 'satisfice', 'satisfise', 'set_about', 'go_about', 'approach', 'sneak', 'stampede', 'surprise', 'take_care', 'take_time_by_the_forelock', 'try', 'attempt', 'essay', 'assay', 'use', 'volunteer', 'offer', 'wait', 'hold_off', 'hold_back', 'woo', 'romance', 'solicit', 'alienate', 'awaken', 'cloud', 'trouble', 'engrave', 'hit_home', 'strike_home', 'strike_a_chord', 'strike_a_note', 'impress', 'ingrain', 'instill', 'infect', 'pierce', 'sadden', 'smite', 'strike_dumb', 'sweep_away', 'sweep_off', 'touch', 'zap', 'bluff', 'bluff_out', 'castle', 'check', 'open', 'serve', 'stalemate', 'trump', 'ruff']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "higher NN\n",
      "\n",
      "Synonyms:  ['higher', 'high', 'eminent', 'high-pitched', 'in_high_spirits', 'gamey', 'gamy', 'mellow']\n",
      "\n",
      "Hypernyms:  []\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "on NN\n",
      "\n",
      "Synonyms:  ['on', 'along']\n",
      "\n",
      "Hypernyms:  []\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "the NN\n",
      "\n",
      "Synonyms:  []\n",
      "\n",
      "Hypernyms:  []\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "news NN\n",
      "\n",
      "Synonyms:  ['news', 'intelligence', 'tidings', 'word', 'news_program', 'news_show', 'newsworthiness']\n",
      "\n",
      "Hypernyms:  ['information', 'info', 'broadcast', 'program', 'programme', 'interest', 'interestingness']\n",
      "\n",
      "Hyponyms:  ['good_word', 'latest', 'update', 'business_news', 'coverage', 'reporting', 'reportage', 'hard_news', 'newscast', 'report', 'news_report', 'story', 'account', 'write_up', 'soft_news', 'stop_press', 'television_news']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      ", NN\n",
      "\n",
      "Synonyms:  []\n",
      "\n",
      "Hypernyms:  []\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "closing NN\n",
      "\n",
      "Synonyms:  ['shutting', 'closing', 'conclusion', 'end', 'close', 'ending', 'closure', 'closedown', 'shutdown', 'completion', 'culmination', 'windup', 'mop_up', 'shut', 'close_up', 'fold', 'shut_down', 'close_down', 'conclude', 'come_together', 'fill_up']\n",
      "\n",
      "Hypernyms:  ['motion', 'movement', 'move', 'motility', 'section', 'subdivision', 'approach', 'approaching', 'coming', 'termination', 'ending', 'conclusion', 'change_state', 'turn', 'end', 'terminate', 'stop', 'finish', 'cease', 'trade', 'prosecute', 'engage', 'pursue', 'near', 'come_on', 'go_up', 'draw_near', 'draw_close', 'come_near', 'join', 'bring_together', 'barricade', 'block', 'blockade', 'block_off', 'block_up', 'bar', 'fill', 'complete']\n",
      "\n",
      "Hyponyms:  ['anticlimax', 'bathos', 'epilogue', 'epilog', 'finale', 'coda', 'peroration', 'bank_closing', 'layoff', 'plant_closing', 'consummation', 'finalization', 'finalisation', 'finish', 'finishing', 'follow-through', 'graduation', 'bung', 'draw', 'roll_up', 'seal', 'seal_off', 'shutter', 'slam', 'bang', 'slat', 'snap', 'adjourn', 'withdraw', 'retire', 'coapt', 'conglutinate', 'plug', 'stop_up', 'secure']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "up NN\n",
      "\n",
      "Synonyms:  ['up', 'astir', 'improving', 'upward', 'upwards', 'upwardly']\n",
      "\n",
      "Hypernyms:  ['increase']\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "2.6 NN\n",
      "\n",
      "Synonyms:  []\n",
      "\n",
      "Hypernyms:  []\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "percent NN\n",
      "\n",
      "Synonyms:  ['percentage', 'percent', 'per_centum', 'pct']\n",
      "\n",
      "Hypernyms:  ['proportion']\n",
      "\n",
      "Hyponyms:  ['absentee_rate', 'occupancy_rate', 'unemployment_rate', 'vacancy_rate']\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "at NN\n",
      "\n",
      "Synonyms:  ['astatine', 'At', 'atomic_number_85', 'at']\n",
      "\n",
      "Hypernyms:  ['chemical_element', 'element', 'halogen', 'Laotian_monetary_unit']\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "$ NN\n",
      "\n",
      "Synonyms:  []\n",
      "\n",
      "Hypernyms:  []\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "5.49 NN\n",
      "\n",
      "Synonyms:  []\n",
      "\n",
      "Hypernyms:  []\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "on NN\n",
      "\n",
      "Synonyms:  ['on', 'along']\n",
      "\n",
      "Hypernyms:  []\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n",
      "\n",
      "***********************************************************************************\n",
      "nasdaq NN\n",
      "\n",
      "Synonyms:  ['National_Association_of_Securities_Dealers_Automated_Quotations', 'NASDAQ']\n",
      "\n",
      "Hypernyms:  []\n",
      "\n",
      "Hyponyms:  []\n",
      "\n",
      "Meronyms (substance):  []\n",
      "\n",
      "Meronyms (part):  []\n",
      "\n",
      "Holonyms: []\n"
     ]
    }
   ],
   "source": [
    "for tk, tg in zip(train_data[\"s2\"]['tokens'][r], train_data[\"s1\"]['tags'][r]):\n",
    "    \n",
    "    print('\\n***********************************************************************************')\n",
    "    print(tk, ind)\n",
    "    synonyms = []\n",
    "    hypernyms = []\n",
    "    hyponyms = []\n",
    "    substance_meronyms = []\n",
    "    part_meronyms = []\n",
    "    holonyms = []\n",
    "\n",
    "    for syn in wn.synsets(tk):\n",
    "        # Synonyms\n",
    "        for l in syn.lemmas():\n",
    "            if l.name() not in synonyms:\n",
    "                synonyms.append(l.name())\n",
    "\n",
    "        # Hypernyms\n",
    "        for hpr in syn.hypernyms():\n",
    "            for l in hpr.lemmas():\n",
    "                if l.name() not in hypernyms:\n",
    "                    hypernyms.append(l.name())\n",
    "\n",
    "        # Hyponyms\n",
    "        for hpo in syn.hyponyms():\n",
    "            for l in hpo.lemmas():\n",
    "                if l.name() not in hyponyms:\n",
    "                    hyponyms.append(l.name())\n",
    "\n",
    "        # Substance Meronyms\n",
    "        for mrn in syn.substance_meronyms():\n",
    "            for l in mrn.lemmas():\n",
    "                if l.name() not in substance_meronyms:\n",
    "                    substance_meronyms.append(l.name())\n",
    "\n",
    "        # Part Meronyms\n",
    "        for mrn in syn.part_meronyms():\n",
    "            for l in mrn.lemmas():\n",
    "                if l.name() not in part_meronyms:\n",
    "                    part_meronyms.append(l.name())\n",
    "\n",
    "        # Holonyms\n",
    "        for hol in syn.member_holonyms():\n",
    "            for l in hol.lemmas():\n",
    "                if l.name() not in holonyms:\n",
    "                    holonyms.append(l.name())\n",
    "\n",
    "    print('\\nSynonyms: ', synonyms)\n",
    "    print('\\nHypernyms: ', hypernyms)\n",
    "    print('\\nHyponyms: ', hyponyms)\n",
    "    print('\\nMeronyms (substance): ', substance_meronyms)\n",
    "    print('\\nMeronyms (part): ', part_meronyms)\n",
    "    print('\\nHolonyms:', holonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Add some examples of each of our features. (3 or 4 of the more interesting ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4: Use Pickle Dump here to Run the Model for given sentences or text file outputting in the correct order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
