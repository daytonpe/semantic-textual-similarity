{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# XGBOOST\n",
    "import xgboost as xgb\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# SPACY IMPORT\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same readData from STS.py\n",
    "def readData(fileName):\n",
    "\n",
    "    first_sentence = []\n",
    "    second_sentence = []\n",
    "    score = []\n",
    "    file = open(fileName, encoding=\"utf8\")\n",
    "    text = file.readline()\n",
    "    text = file.read()\n",
    "    # loop to extract a set of two sentences\n",
    "    for sentence in text.split('\\n'):\n",
    "        # creating two separate lists of the sentences\n",
    "        # '.rstrip('.') only removes the last period in the sentence\n",
    "        first_sentence.insert(len(first_sentence),\n",
    "                              (sentence.split('\\t')[1].lower()).rstrip('.'))\n",
    "        second_sentence.insert(len(first_sentence),\n",
    "                               (sentence.split('\\t')[2].lower()).rstrip('.'))\n",
    "        # inserting the score as a separate lists\n",
    "        score.insert(len(first_sentence), (sentence.split('\\t')[3]))\n",
    "\n",
    "    # print(first_sentence)\n",
    "    return first_sentence, second_sentence, score\n",
    "\n",
    "\n",
    "def preprocess(fileName):\n",
    "\n",
    "    first_sentence, second_sentence, score = readData(fileName)\n",
    "    first_sentence_tokens = []\n",
    "    second_sentence_tokens = []\n",
    "\n",
    "    # tokenizing and tagging\n",
    "    first_sentence_tags = []\n",
    "    second_sentence_tags = []\n",
    "\n",
    "    for sentence in first_sentence:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        first_sentence_tokens.insert(len(first_sentence_tokens), tokens)\n",
    "        first_sentence_tags.insert(\n",
    "            len(first_sentence_tags), nltk.pos_tag(tokens))\n",
    "        # print(first_sentence_tokens)\n",
    "\n",
    "    for sentence in second_sentence:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        second_sentence_tokens.insert(len(second_sentence_tokens), tokens)\n",
    "        second_sentence_tags.insert(\n",
    "            len(second_sentence_tags), nltk.pos_tag(tokens))\n",
    "\n",
    "        # print(second_sentence_tokens)\n",
    "\n",
    "    # lemmatizing\n",
    "    first_sentence_lemmas = []\n",
    "    second_sentence_lemmas = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for sentence in first_sentence_tokens:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        first_sentence_lemmas.insert(\n",
    "            len(first_sentence_lemmas), sentence_components)\n",
    "\n",
    "    for sentence in second_sentence_tokens:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        second_sentence_lemmas.insert(\n",
    "            len(second_sentence_lemmas), sentence_components)\n",
    "\n",
    "    return first_sentence, second_sentence, score, first_sentence_tokens, second_sentence_tokens, first_sentence_lemmas, second_sentence_lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_arr_train, s2_arr_train, scores_train, s1_tokens_train, s2_tokens_train, s1_lemmas_train,s2_lemmas_train = preprocess(\"./data/train-set.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s1_arr_train[0])\n",
    "print(s2_arr_train[0])\n",
    "print(scores_train[0])\n",
    "print(s1_tokens_train[0])\n",
    "print(s2_tokens_train[0])\n",
    "print(s1_lemmas_train[0])\n",
    "print(s2_lemmas_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list = [0,0,0,0,0,0]\n",
    "for s in scores_train:\n",
    "    score_list[int(s)] += 1\n",
    "for i in range(0, len(score_list)):\n",
    "#     print(i, ': ', score_list[i], round(score_list[i]/len(score_list), 1), '%')\n",
    "    print(\"% 1d: % 4d % 6.2f per\" %(i, score_list[i], 100*score_list[i]/len(scores_train))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This section includes all the code/functions to create features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cosine_similarity(sentence1, sentence2):\n",
    "\n",
    "    # remove the stopwords, transform into TF-IDF matrix, then\n",
    "    tfidf_matrix = TfidfVectorizer(\n",
    "        stop_words=\"english\").fit_transform([sentence1, sentence2])\n",
    "    cos_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "#     print(tfidf_matrix.toarray())\n",
    "\n",
    "    cos_sim = cos_sim_matrix[0][1]\n",
    "\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth Inverse Frequency (SIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_distribution(s1_tokens_array, s2_tokens_array):\n",
    "    freq_dist = FreqDist()\n",
    "    for i in range(len(s1_tokens_array)):\n",
    "        for token in (s1_tokens_array[i] + s2_tokens_array[i]):\n",
    "            freq_dist[token.lower()] += 1\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_dist = frequency_distribution(s1_tokens_train, s2_tokens_train)\n",
    "print(freq_dist.most_common(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sif_similarity(s1, s2, a = .001):\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform([s1, s2])\n",
    "    X_arr = X.toarray()\n",
    "    sif_matrix = []\n",
    "    for i in range(0, len(X_arr)):\n",
    "        sif_arr = []\n",
    "        for j in range(0, len(X_arr[i])):\n",
    "            word = vectorizer.get_feature_names()[j]\n",
    "            w = a / (a + freq_dist[word])\n",
    "            v = X_arr[i][j]\n",
    "            sif_arr.append(v*w)\n",
    "        sif_matrix.append(sif_arr)\n",
    "    sif_cos_sim_matrix = cosine_similarity(sif_matrix, sif_matrix)\n",
    "    sif_cos_sim = sif_cos_sim_matrix[0][1]\n",
    "    return sif_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_sif_similarity('I like some apples', 'I like the pears')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Overlap\n",
    "\n",
    "Unique words that are in both sentences divided by the total number of words in both sentences. Does not include stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_sentence_list = s1_tokens_train+s2_tokens_train\n",
    "words_filtered = []\n",
    "\n",
    "# print(words)\n",
    "\n",
    "# looking through I've noticed there are a number of stop-words that can be added to the set\n",
    "stop_words.add(',')\n",
    "stop_words.add('``')\n",
    "stop_words.add(\"n't\")\n",
    "\n",
    "for tsl in tokenized_sentence_list:\n",
    "    for w in tsl:\n",
    "        if w not in stop_words and w not in words_filtered:\n",
    "            words_filtered.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(token_list):\n",
    "    blank_list = []\n",
    "    for w in token_list:\n",
    "        if w not in stop_words:\n",
    "            blank_list.append(w)\n",
    "    return blank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_tokens(token_list):\n",
    "    blank_list = []\n",
    "    for w in token_list:\n",
    "        if w not in blank_list:\n",
    "            blank_list.append(w)\n",
    "    return blank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_basic_overlap(s1_tokens, s2_tokens):\n",
    "    s1_tokens = remove_stopwords(s1_tokens)\n",
    "    s1_tokens = remove_duplicate_tokens(s1_tokens)\n",
    "\n",
    "    s2_tokens = remove_stopwords(s2_tokens)\n",
    "    s2_tokens = remove_duplicate_tokens(s2_tokens)\n",
    "    \n",
    "    overlap = 0\n",
    "    encountered_indexes = []\n",
    "    for word in (s1_tokens+s2_tokens):\n",
    "        try:\n",
    "            word_index = words_filtered.index(word)\n",
    "            if word_index in encountered_indexes: # we know we have found an overlap\n",
    "                overlap += 1\n",
    "            encountered_indexes.append(word_index)\n",
    "        except ValueError:\n",
    "            # print(word + ' not found in lexicon. Skipping...')\n",
    "            continue\n",
    "\n",
    "    avg_sentence_len = len(s1_tokens+s2_tokens) / 2\n",
    "    \n",
    "    overlap_normlalized = overlap / avg_sentence_len\n",
    "    return overlap, overlap_normlalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synset Overlap\n",
    "\n",
    "#### TODO\n",
    "\n",
    "We may be able to incorporate POS and dependency parsing here as right now i'm just taking the first synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_synset_overlap(s1_tokens, s2_tokens):\n",
    "    s1_tokens = remove_stopwords(s1_tokens)\n",
    "    s1_tokens = remove_duplicate_tokens(s1_tokens)\n",
    "\n",
    "    s2_tokens = remove_stopwords(s2_tokens)\n",
    "    s2_tokens = remove_duplicate_tokens(s2_tokens)\n",
    "    \n",
    "    print(s2_tokens)\n",
    "    print(s1_tokens)\n",
    "\n",
    "    s1_spread = []\n",
    "    s2_spread = []\n",
    "    \n",
    "    for word in s1_tokens:\n",
    "        for synset in wn.synsets(word):\n",
    "            for i in range(0, len(synset.lemmas())):\n",
    "                syn_word = synset.lemmas()[i].name()\n",
    "                if syn_word not in s1_spread:\n",
    "                    s1_spread.append(syn_word)\n",
    "\n",
    "    for word in s2_tokens:\n",
    "        for synset in wn.synsets(word):\n",
    "            for i in range(0, len(synset.lemmas())):\n",
    "                syn_word = synset.lemmas()[i].name()\n",
    "                if syn_word not in s2_spread:\n",
    "                    s2_spread.append(syn_word)         \n",
    "    \n",
    "    return calc_basic_overlap(s1_spread, s2_spread)\n",
    "    \n",
    "calc_synset_overlap(s1_tokens_train[0], s2_tokens_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_spacy_sim(s1, s2):\n",
    "    s2 = nlp(s2)\n",
    "    s1 = nlp(s1)\n",
    "    return s1.similarity(s2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "In this section we run the data through the pipeline to get it into the form necessary to create our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(s1_array, s2_array, s1_tokens, s2_tokens, s1_lemmas, s2_lemmas):\n",
    "    # TODO add a check to ensure the lengths of these arrays are the same\n",
    "    # or add the basic processing to pipeline\n",
    "    data = []\n",
    "    for i in range(0, len(s1_array)):\n",
    "        cos_sim = calc_cosine_similarity(s1_array[i], s2_array[i])\n",
    "        sif_sim = calc_sif_similarity(s1_array[i], s2_array[i])\n",
    "        w_overlap, w_norm_overlap = calc_basic_overlap(s1_tokens[i], s2_tokens[i])\n",
    "        l_overlap, l_norm_overlap = calc_basic_overlap(s1_lemmas[i], s2_lemmas[i])\n",
    "        spacy_sim = calc_spacy_sim(s1_array[i], s2_array[i])\n",
    "        syn_overlap, normalized_syn_overlap = calc_synset_overlap(s1_tokens[i], s2_tokens[i])\n",
    "        data.append([i, w_norm_overlap, l_norm_overlap, sif_sim, cos_sim, spacy_sim, syn_overlap, normalized_syn_overlap])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data = pipeline(s1_arr_train, s2_arr_train, s1_tokens_train, s2_tokens_train, s1_lemmas_train, s2_lemmas_train)\n",
    "print(data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "In this section we fit our feature set to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_classifier = DecisionTreeClassifier(random_state=14, max_depth=8)\n",
    "dt_classifier.fit(data,scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Nodes: {dt_classifier.tree_.node_count}')\n",
    "print(f'Max Depth: {dt_classifier.tree_.max_depth}')\n",
    "print(f'Accuracy: {dt_classifier.score(data, scores_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = XGBClassifier(booster='gbtree', \n",
    "                       n_estimators=1000,\n",
    "                       n_jobs=4,\n",
    "                       learning_rate=.05,\n",
    "                       max_depth=4,\n",
    "                       random_state=42,\n",
    "                       gamma=.05,\n",
    "                       early_stopping_rounds = 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = XGBClassifier(n_jobs=4, n_estimators=1000, gamma=.05, random_state=42)\n",
    "xgboost_model.fit(np.asarray(data), np.asarray(scores_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=14, n_estimators=100)\n",
    "rf_classifier.fit(data,scores_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_arr_dev, s2_arr_dev, scores_dev, s1_tokens_dev, s2_tokens_dev, s1_lemmas_dev,s2_lemmas_dev = preprocess(\"./data/dev-set.txt\")\n",
    "dev_data = pipeline(s1_arr_dev, s2_arr_dev, s1_tokens_dev, s2_tokens_dev, s1_lemmas_dev,s2_lemmas_dev)\n",
    "dt_dev_predictions = dt_classifier.predict(dev_data)\n",
    "rf_dev_predictions = rf_classifier.predict(dev_data)\n",
    "xgb_dev_predictions = xgboost_model.predict(np.asarray(dev_data))\n",
    "dev_data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure our lengths match up\n",
    "print(len(scores_dev))\n",
    "print(len(dt_dev_predictions))\n",
    "print(len(rf_dev_predictions))\n",
    "print(len(xgb_dev_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_metrics(name, predictions, scores):\n",
    "    correct = 0\n",
    "    total_error = 0\n",
    "    for i in range(0, len(predictions)): \n",
    "        if predictions[i] == scores[i]:\n",
    "            correct += 1\n",
    "        total_error += abs(int(scores[i]) - int(predictions[i]))\n",
    "    acc = correct / len(predictions) \n",
    "    avg_err = total_error / len(predictions)\n",
    "    \n",
    "    f1 = f1_score(scores, predictions, average='weighted')\n",
    "    return name, acc, avg_err, f1\n",
    "\n",
    "\n",
    "# Ensure all our arrays are full of ints for metrics.\n",
    "scores_dev = [int(i) for i in scores_dev] \n",
    "rf_dev_predictions = [int(i) for i in rf_dev_predictions] \n",
    "xgb_dev_predictions = [int(i) for i in xgb_dev_predictions] \n",
    "dt_dev_predictions = [int(i) for i in dt_dev_predictions] \n",
    "\n",
    "\n",
    "\n",
    "xgb_metrics = get_metrics('XGBoost', xgb_dev_predictions, scores_dev)\n",
    "dt_metrics  = get_metrics('Decision Tree', dt_dev_predictions, scores_dev)\n",
    "rf_metrics = get_metrics('Random Forest', rf_dev_predictions, scores_dev)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [dt_metrics, rf_metrics, xgb_metrics], \n",
    "    columns = ['Model', 'Accuracy', 'Avg Error', 'F-Score']) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
