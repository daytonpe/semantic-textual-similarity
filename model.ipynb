{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# import xgboost as xgb\n",
    "\n",
    "# SPACY IMPORT\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same readData from STS.py\n",
    "def readData(fileName):\n",
    "\n",
    "    first_sentence = []\n",
    "    second_sentence = []\n",
    "    score = []\n",
    "    file = open(fileName, encoding=\"utf8\")\n",
    "    text = file.readline()\n",
    "    text = file.read()\n",
    "    # loop to extract a set of two sentences\n",
    "    for sentence in text.split('\\n'):\n",
    "        # creating two separate lists of the sentences\n",
    "        # '.rstrip('.') only removes the last period in the sentence\n",
    "        first_sentence.insert(len(first_sentence),\n",
    "                              (sentence.split('\\t')[1].lower()).rstrip('.'))\n",
    "        second_sentence.insert(len(first_sentence),\n",
    "                               (sentence.split('\\t')[2].lower()).rstrip('.'))\n",
    "        # inserting the score as a separate lists\n",
    "        score.insert(len(first_sentence), (sentence.split('\\t')[3]))\n",
    "\n",
    "    # print(first_sentence)\n",
    "    return first_sentence, second_sentence, score\n",
    "\n",
    "\n",
    "def preprocess(fileName):\n",
    "\n",
    "    first_sentence, second_sentence, score = readData(fileName)\n",
    "    first_sentence_tokens = []\n",
    "    second_sentence_tokens = []\n",
    "\n",
    "    # tokenizing and tagging\n",
    "    first_sentence_tags = []\n",
    "    second_sentence_tags = []\n",
    "\n",
    "    for sentence in first_sentence:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        first_sentence_tokens.insert(len(first_sentence_tokens), tokens)\n",
    "        first_sentence_tags.insert(\n",
    "            len(first_sentence_tags), nltk.pos_tag(tokens))\n",
    "        # print(first_sentence_tokens)\n",
    "\n",
    "    for sentence in second_sentence:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        second_sentence_tokens.insert(len(second_sentence_tokens), tokens)\n",
    "        second_sentence_tags.insert(\n",
    "            len(second_sentence_tags), nltk.pos_tag(tokens))\n",
    "\n",
    "        # print(second_sentence_tokens)\n",
    "\n",
    "    # lemmatizing\n",
    "    first_sentence_lemmas = []\n",
    "    second_sentence_lemmas = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for sentence in first_sentence_tokens:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        first_sentence_lemmas.insert(\n",
    "            len(first_sentence_lemmas), sentence_components)\n",
    "\n",
    "    for sentence in second_sentence_tokens:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        second_sentence_lemmas.insert(\n",
    "            len(second_sentence_lemmas), sentence_components)\n",
    "\n",
    "    return first_sentence, second_sentence, score, first_sentence_tokens, second_sentence_tokens, first_sentence_lemmas, second_sentence_lemmas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_arr_train, s2_arr_train, scores_train, s1_tokens_train, s2_tokens_train, s1_lemmas_train,s2_lemmas_train = preprocess(\"./data/train-set.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0:    8   0.54 per\n",
      " 1:   37   2.49 per\n",
      " 2:   95   6.40 per\n",
      " 3:  310  20.89 per\n",
      " 4:  616  41.51 per\n",
      " 5:  418  28.17 per\n"
     ]
    }
   ],
   "source": [
    "score_list = [0,0,0,0,0,0]\n",
    "for s in scores_train:\n",
    "    score_list[int(s)] += 1\n",
    "for i in range(0, len(score_list)):\n",
    "#     print(i, ': ', score_list[i], round(score_list[i]/len(score_list), 1), '%')\n",
    "    print(\"% 1d: % 4d % 6.2f per\" %(i, score_list[i], 100*score_list[i]/len(scores_train))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This section includes all the code/functions to create features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cosine_similarity(sentence1, sentence2):\n",
    "\n",
    "    # remove the stopwords, transform into TF-IDF matrix, then\n",
    "    tfidf_matrix = TfidfVectorizer(\n",
    "        stop_words=\"english\").fit_transform([sentence1, sentence2])\n",
    "    cos_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "#     print(tfidf_matrix.toarray())\n",
    "\n",
    "    cos_sim = cos_sim_matrix[0][1]\n",
    "\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth Inverse Frequency (SIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_distribution(s1_tokens_array, s2_tokens_array):\n",
    "    freq_dist = FreqDist()\n",
    "    for i in range(len(s1_tokens_array)):\n",
    "        for token in (s1_tokens_array[i] + s2_tokens_array[i]):\n",
    "            freq_dist[token.lower()] += 1\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 5169), (',', 3690), ('of', 2497), ('to', 2133), ('and', 1716), ('a', 1615), ('in', 1573), ('is', 891), ('that', 831), ('on', 820), ('for', 756), ('it', 587), ('this', 579), ('we', 531), ('with', 464), ('be', 459), ('by', 443), ('i', 425), ('which', 403), ('have', 384), ('not', 366), ('at', 343), ('as', 334), ('are', 333), ('has', 319), ('said', 316), ('was', 304), ('european', 287), (\"'s\", 280), ('from', 261), ('``', 252), (\"''\", 242), ('will', 233), ('.', 229), ('also', 223), ('its', 194), ('but', 193), ('would', 191), ('all', 188), ('percent', 187)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist = frequency_distribution(s1_tokens_train, s2_tokens_train)\n",
    "print(freq_dist.most_common(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sif_similarity(s1, s2, a = .001):\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform([s1, s2])\n",
    "    X_arr = X.toarray()\n",
    "    sif_matrix = []\n",
    "    for i in range(0, len(X_arr)):\n",
    "        sif_arr = []\n",
    "        for j in range(0, len(X_arr[i])):\n",
    "            word = vectorizer.get_feature_names()[j]\n",
    "            w = a / (a + freq_dist[word])\n",
    "            v = X_arr[i][j]\n",
    "            sif_arr.append(v*w)\n",
    "        sif_matrix.append(sif_arr)\n",
    "    sif_cos_sim_matrix = cosine_similarity(sif_matrix, sif_matrix)\n",
    "    sif_cos_sim = sif_cos_sim_matrix[0][1]\n",
    "    return sif_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4515545128534995e-10"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_sif_similarity('I like some apples', 'I like the pears')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Overlap\n",
    "\n",
    "Unique words that are in both sentences divided by the total number of words in both sentences. Does not include stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_sentence_list = s1_tokens_train+s2_tokens_train\n",
    "words_filtered = []\n",
    "\n",
    "# print(words)\n",
    "\n",
    "# looking through I've noticed there are a number of stop-words that can be added to the set\n",
    "stop_words.add(',')\n",
    "stop_words.add('``')\n",
    "stop_words.add(\"n't\")\n",
    "\n",
    "for tsl in tokenized_sentence_list:\n",
    "    for w in tsl:\n",
    "        if w not in stop_words and w not in words_filtered:\n",
    "            words_filtered.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(token_list):\n",
    "    blank_list = []\n",
    "    for w in token_list:\n",
    "        if w not in stop_words:\n",
    "            blank_list.append(w)\n",
    "    return blank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_tokens(token_list):\n",
    "    blank_list = []\n",
    "    for w in token_list:\n",
    "        if w not in blank_list:\n",
    "            blank_list.append(w)\n",
    "    return blank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_basic_overlap(s1_tokens, s2_tokens):\n",
    "    s1_tokens = remove_stopwords(s1_tokens)\n",
    "    s1_tokens = remove_duplicate_tokens(s1_tokens)\n",
    "\n",
    "    s2_tokens = remove_stopwords(s2_tokens)\n",
    "    s2_tokens = remove_duplicate_tokens(s2_tokens)\n",
    "    \n",
    "    overlap = 0\n",
    "    encountered_indexes = []\n",
    "    for word in (s1_tokens+s2_tokens):\n",
    "        try:\n",
    "            word_index = words_filtered.index(word)\n",
    "            if word_index in encountered_indexes: # we know we have found an overlap\n",
    "                overlap += 1\n",
    "            encountered_indexes.append(word_index)\n",
    "        except ValueError:\n",
    "            # print(word + ' not found in lexicon. Skipping...')\n",
    "            continue\n",
    "\n",
    "    avg_sentence_len = len(s1_tokens+s2_tokens) / 2\n",
    "    \n",
    "    overlap_normlalized = overlap / avg_sentence_len\n",
    "    return overlap, overlap_normlalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synset Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sources', 'close', 'sale', 'said', 'vivendi', 'keeping', 'door', 'open', 'bids', 'next', 'day', 'two']\n",
      "['sources', 'close', 'sale', 'said', 'vivendi', 'keeping', 'door', 'open', 'bids', 'hoped', 'see', 'bidders', 'interested', 'individual', 'assets', 'team']\n",
      "beginning not found in lexicon. Skipping...\n",
      "root not found in lexicon. Skipping...\n",
      "rootage not found in lexicon. Skipping...\n",
      "source not found in lexicon. Skipping...\n",
      "seed not found in lexicon. Skipping...\n",
      "germ not found in lexicon. Skipping...\n",
      "generator not found in lexicon. Skipping...\n",
      "reservoir not found in lexicon. Skipping...\n",
      "stopping_point not found in lexicon. Skipping...\n",
      "finale not found in lexicon. Skipping...\n",
      "finis not found in lexicon. Skipping...\n",
      "finish not found in lexicon. Skipping...\n",
      "conclusion not found in lexicon. Skipping...\n",
      "closing_curtain not found in lexicon. Skipping...\n",
      "close_up not found in lexicon. Skipping...\n",
      "shut_down not found in lexicon. Skipping...\n",
      "close_down not found in lexicon. Skipping...\n",
      "come_together not found in lexicon. Skipping...\n",
      "fill_up not found in lexicon. Skipping...\n",
      "nigh not found in lexicon. Skipping...\n",
      "faithful not found in lexicon. Skipping...\n",
      "tight not found in lexicon. Skipping...\n",
      "confining not found in lexicon. Skipping...\n",
      "airless not found in lexicon. Skipping...\n",
      "stuffy not found in lexicon. Skipping...\n",
      "unaired not found in lexicon. Skipping...\n",
      "snug not found in lexicon. Skipping...\n",
      "close-fitting not found in lexicon. Skipping...\n",
      "cheeseparing not found in lexicon. Skipping...\n",
      "penny-pinching not found in lexicon. Skipping...\n",
      "skinny not found in lexicon. Skipping...\n",
      "closelipped not found in lexicon. Skipping...\n",
      "closemouthed not found in lexicon. Skipping...\n",
      "secretive not found in lexicon. Skipping...\n",
      "tightlipped not found in lexicon. Skipping...\n",
      "closely not found in lexicon. Skipping...\n",
      "cut-rate_sale not found in lexicon. Skipping...\n",
      "sales_event not found in lexicon. Skipping...\n",
      "sales_agreement not found in lexicon. Skipping...\n",
      "allege not found in lexicon. Skipping...\n",
      "aver not found in lexicon. Skipping...\n",
      "suppose not found in lexicon. Skipping...\n",
      "enjoin not found in lexicon. Skipping...\n",
      "pronounce not found in lexicon. Skipping...\n",
      "articulate not found in lexicon. Skipping...\n",
      "enounce not found in lexicon. Skipping...\n",
      "sound_out not found in lexicon. Skipping...\n",
      "enunciate not found in lexicon. Skipping...\n",
      "aforesaid not found in lexicon. Skipping...\n",
      "aforementioned not found in lexicon. Skipping...\n",
      "guardianship not found in lexicon. Skipping...\n",
      "safekeeping not found in lexicon. Skipping...\n",
      "retention not found in lexicon. Skipping...\n",
      "holding not found in lexicon. Skipping...\n",
      "go_on not found in lexicon. Skipping...\n",
      "proceed not found in lexicon. Skipping...\n",
      "go_along not found in lexicon. Skipping...\n",
      "hold_on not found in lexicon. Skipping...\n",
      "observe not found in lexicon. Skipping...\n",
      "keep_on not found in lexicon. Skipping...\n",
      "sustain not found in lexicon. Skipping...\n",
      "stay_fresh not found in lexicon. Skipping...\n",
      "restrain not found in lexicon. Skipping...\n",
      "keep_back not found in lexicon. Skipping...\n",
      "hold_back not found in lexicon. Skipping...\n",
      "preserve not found in lexicon. Skipping...\n",
      "keep_open not found in lexicon. Skipping...\n",
      "hold_open not found in lexicon. Skipping...\n",
      "doorway not found in lexicon. Skipping...\n",
      "room_access not found in lexicon. Skipping...\n",
      "threshold not found in lexicon. Skipping...\n",
      "outdoors not found in lexicon. Skipping...\n",
      "out-of-doors not found in lexicon. Skipping...\n",
      "open_air not found in lexicon. Skipping...\n",
      "surface not found in lexicon. Skipping...\n",
      "open_up not found in lexicon. Skipping...\n",
      "unfold not found in lexicon. Skipping...\n",
      "spread_out not found in lexicon. Skipping...\n",
      "afford not found in lexicon. Skipping...\n",
      "unfastened not found in lexicon. Skipping...\n",
      "exposed not found in lexicon. Skipping...\n",
      "assailable not found in lexicon. Skipping...\n",
      "undefendable not found in lexicon. Skipping...\n",
      "undefended not found in lexicon. Skipping...\n",
      "loose not found in lexicon. Skipping...\n",
      "undetermined not found in lexicon. Skipping...\n",
      "unresolved not found in lexicon. Skipping...\n",
      "receptive not found in lexicon. Skipping...\n",
      "overt not found in lexicon. Skipping...\n",
      "capable not found in lexicon. Skipping...\n",
      "candid not found in lexicon. Skipping...\n",
      "heart-to-heart not found in lexicon. Skipping...\n",
      "bidding not found in lexicon. Skipping...\n",
      "dictation not found in lexicon. Skipping...\n",
      "beseech not found in lexicon. Skipping...\n",
      "entreat not found in lexicon. Skipping...\n",
      "adjure not found in lexicon. Skipping...\n",
      "conjure not found in lexicon. Skipping...\n",
      "desire not found in lexicon. Skipping...\n",
      "go_for not found in lexicon. Skipping...\n",
      "hop not found in lexicon. Skipping...\n",
      "skip not found in lexicon. Skipping...\n",
      "hop-skip not found in lexicon. Skipping...\n",
      "realise not found in lexicon. Skipping...\n",
      "visualize not found in lexicon. Skipping...\n",
      "visualise not found in lexicon. Skipping...\n",
      "envision not found in lexicon. Skipping...\n",
      "fancy not found in lexicon. Skipping...\n",
      "picture not found in lexicon. Skipping...\n",
      "image not found in lexicon. Skipping...\n",
      "reckon not found in lexicon. Skipping...\n",
      "get_word not found in lexicon. Skipping...\n",
      "get_wind not found in lexicon. Skipping...\n",
      "pick_up not found in lexicon. Skipping...\n",
      "find_out not found in lexicon. Skipping...\n",
      "get_a_line not found in lexicon. Skipping...\n",
      "discover not found in lexicon. Skipping...\n",
      "catch not found in lexicon. Skipping...\n",
      "take_in not found in lexicon. Skipping...\n",
      "run_into not found in lexicon. Skipping...\n",
      "encounter not found in lexicon. Skipping...\n",
      "run_across not found in lexicon. Skipping...\n",
      "come_across not found in lexicon. Skipping...\n",
      "check not found in lexicon. Skipping...\n",
      "ascertain not found in lexicon. Skipping...\n",
      "insure not found in lexicon. Skipping...\n",
      "see_to_it not found in lexicon. Skipping...\n",
      "assure not found in lexicon. Skipping...\n",
      "take_care not found in lexicon. Skipping...\n",
      "go_steady not found in lexicon. Skipping...\n",
      "go_out not found in lexicon. Skipping...\n",
      "examine not found in lexicon. Skipping...\n",
      "go_through not found in lexicon. Skipping...\n",
      "escort not found in lexicon. Skipping...\n",
      "interpret not found in lexicon. Skipping...\n",
      "construe not found in lexicon. Skipping...\n",
      "bidder not found in lexicon. Skipping...\n",
      "concern not found in lexicon. Skipping...\n",
      "occupy not found in lexicon. Skipping...\n",
      "worry not found in lexicon. Skipping...\n",
      "matter_to not found in lexicon. Skipping...\n",
      "somebody not found in lexicon. Skipping...\n",
      "mortal not found in lexicon. Skipping...\n",
      "soul not found in lexicon. Skipping...\n",
      "case-by-case not found in lexicon. Skipping...\n",
      "item-by-item not found in lexicon. Skipping...\n",
      "asset not found in lexicon. Skipping...\n",
      "plus not found in lexicon. Skipping...\n",
      "team_up not found in lexicon. Skipping...\n",
      "beginning not found in lexicon. Skipping...\n",
      "root not found in lexicon. Skipping...\n",
      "rootage not found in lexicon. Skipping...\n",
      "source not found in lexicon. Skipping...\n",
      "seed not found in lexicon. Skipping...\n",
      "germ not found in lexicon. Skipping...\n",
      "generator not found in lexicon. Skipping...\n",
      "reservoir not found in lexicon. Skipping...\n",
      "stopping_point not found in lexicon. Skipping...\n",
      "finale not found in lexicon. Skipping...\n",
      "finis not found in lexicon. Skipping...\n",
      "finish not found in lexicon. Skipping...\n",
      "conclusion not found in lexicon. Skipping...\n",
      "closing_curtain not found in lexicon. Skipping...\n",
      "close_up not found in lexicon. Skipping...\n",
      "shut_down not found in lexicon. Skipping...\n",
      "close_down not found in lexicon. Skipping...\n",
      "come_together not found in lexicon. Skipping...\n",
      "fill_up not found in lexicon. Skipping...\n",
      "nigh not found in lexicon. Skipping...\n",
      "faithful not found in lexicon. Skipping...\n",
      "tight not found in lexicon. Skipping...\n",
      "confining not found in lexicon. Skipping...\n",
      "airless not found in lexicon. Skipping...\n",
      "stuffy not found in lexicon. Skipping...\n",
      "unaired not found in lexicon. Skipping...\n",
      "snug not found in lexicon. Skipping...\n",
      "close-fitting not found in lexicon. Skipping...\n",
      "cheeseparing not found in lexicon. Skipping...\n",
      "penny-pinching not found in lexicon. Skipping...\n",
      "skinny not found in lexicon. Skipping...\n",
      "closelipped not found in lexicon. Skipping...\n",
      "closemouthed not found in lexicon. Skipping...\n",
      "secretive not found in lexicon. Skipping...\n",
      "tightlipped not found in lexicon. Skipping...\n",
      "closely not found in lexicon. Skipping...\n",
      "cut-rate_sale not found in lexicon. Skipping...\n",
      "sales_event not found in lexicon. Skipping...\n",
      "sales_agreement not found in lexicon. Skipping...\n",
      "allege not found in lexicon. Skipping...\n",
      "aver not found in lexicon. Skipping...\n",
      "suppose not found in lexicon. Skipping...\n",
      "enjoin not found in lexicon. Skipping...\n",
      "pronounce not found in lexicon. Skipping...\n",
      "articulate not found in lexicon. Skipping...\n",
      "enounce not found in lexicon. Skipping...\n",
      "sound_out not found in lexicon. Skipping...\n",
      "enunciate not found in lexicon. Skipping...\n",
      "aforesaid not found in lexicon. Skipping...\n",
      "aforementioned not found in lexicon. Skipping...\n",
      "guardianship not found in lexicon. Skipping...\n",
      "safekeeping not found in lexicon. Skipping...\n",
      "retention not found in lexicon. Skipping...\n",
      "holding not found in lexicon. Skipping...\n",
      "go_on not found in lexicon. Skipping...\n",
      "proceed not found in lexicon. Skipping...\n",
      "go_along not found in lexicon. Skipping...\n",
      "hold_on not found in lexicon. Skipping...\n",
      "observe not found in lexicon. Skipping...\n",
      "keep_on not found in lexicon. Skipping...\n",
      "sustain not found in lexicon. Skipping...\n",
      "stay_fresh not found in lexicon. Skipping...\n",
      "restrain not found in lexicon. Skipping...\n",
      "keep_back not found in lexicon. Skipping...\n",
      "hold_back not found in lexicon. Skipping...\n",
      "preserve not found in lexicon. Skipping...\n",
      "keep_open not found in lexicon. Skipping...\n",
      "hold_open not found in lexicon. Skipping...\n",
      "doorway not found in lexicon. Skipping...\n",
      "room_access not found in lexicon. Skipping...\n",
      "threshold not found in lexicon. Skipping...\n",
      "outdoors not found in lexicon. Skipping...\n",
      "out-of-doors not found in lexicon. Skipping...\n",
      "open_air not found in lexicon. Skipping...\n",
      "surface not found in lexicon. Skipping...\n",
      "open_up not found in lexicon. Skipping...\n",
      "unfold not found in lexicon. Skipping...\n",
      "spread_out not found in lexicon. Skipping...\n",
      "afford not found in lexicon. Skipping...\n",
      "unfastened not found in lexicon. Skipping...\n",
      "exposed not found in lexicon. Skipping...\n",
      "assailable not found in lexicon. Skipping...\n",
      "undefendable not found in lexicon. Skipping...\n",
      "undefended not found in lexicon. Skipping...\n",
      "loose not found in lexicon. Skipping...\n",
      "undetermined not found in lexicon. Skipping...\n",
      "unresolved not found in lexicon. Skipping...\n",
      "receptive not found in lexicon. Skipping...\n",
      "overt not found in lexicon. Skipping...\n",
      "capable not found in lexicon. Skipping...\n",
      "candid not found in lexicon. Skipping...\n",
      "heart-to-heart not found in lexicon. Skipping...\n",
      "bidding not found in lexicon. Skipping...\n",
      "dictation not found in lexicon. Skipping...\n",
      "beseech not found in lexicon. Skipping...\n",
      "entreat not found in lexicon. Skipping...\n",
      "adjure not found in lexicon. Skipping...\n",
      "conjure not found in lexicon. Skipping...\n",
      "adjacent not found in lexicon. Skipping...\n",
      "side_by_side not found in lexicon. Skipping...\n",
      "succeeding not found in lexicon. Skipping...\n",
      "twenty-four_hours not found in lexicon. Skipping...\n",
      "twenty-four_hour_period not found in lexicon. Skipping...\n",
      "24-hour_interval not found in lexicon. Skipping...\n",
      "solar_day not found in lexicon. Skipping...\n",
      "mean_solar_day not found in lexicon. Skipping...\n",
      "daytime not found in lexicon. Skipping...\n",
      "daylight not found in lexicon. Skipping...\n",
      "sidereal_day not found in lexicon. Skipping...\n",
      "Day not found in lexicon. Skipping...\n",
      "Clarence_Day not found in lexicon. Skipping...\n",
      "Clarence_Shepard_Day_Jr. not found in lexicon. Skipping...\n",
      "II not found in lexicon. Skipping...\n",
      "deuce not found in lexicon. Skipping...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(46, 0.23173803526448364)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_synset_overlap(s1_tokens, s2_tokens):\n",
    "    s1_tokens = remove_stopwords(s1_tokens)\n",
    "    s1_tokens = remove_duplicate_tokens(s1_tokens)\n",
    "\n",
    "    s2_tokens = remove_stopwords(s2_tokens)\n",
    "    s2_tokens = remove_duplicate_tokens(s2_tokens)\n",
    "    \n",
    "    print(s2_tokens)\n",
    "    print(s1_tokens)\n",
    "\n",
    "    s1_spread = []\n",
    "    s2_spread = []\n",
    "    \n",
    "    for word in s1_tokens:\n",
    "        for synset in wn.synsets(word):\n",
    "            for i in range(0, len(synset.lemmas())):\n",
    "                syn_word = synset.lemmas()[i].name()\n",
    "                if syn_word not in s1_spread:\n",
    "                    s1_spread.append(syn_word)\n",
    "\n",
    "    for word in s2_tokens:\n",
    "        for synset in wn.synsets(word):\n",
    "            for i in range(0, len(synset.lemmas())):\n",
    "                syn_word = synset.lemmas()[i].name()\n",
    "                if syn_word not in s2_spread:\n",
    "                    s2_spread.append(syn_word)         \n",
    "    \n",
    "    return calc_basic_overlap(s1_spread, s2_spread)\n",
    "    \n",
    "calc_synset_overlap(s1_tokens_train[0], s2_tokens_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_spacy_sim(s1, s2):\n",
    "    s2 = nlp(s2)\n",
    "    s1 = nlp(s1)\n",
    "    return s1.similarity(s2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "In this section we run the data through the pipeline to get it into the form necessary to create our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(s1_array, s2_array, s1_tokens, s2_tokens, s1_lemmas, s2_lemmas):\n",
    "    # TODO add a check to ensure the lengths of these arrays are the same\n",
    "    # or add the basic processing to pipeline\n",
    "    data = []\n",
    "    for i in range(0, len(s1_array)):\n",
    "        cos_sim = calc_cosine_similarity(s1_array[i], s2_array[i])\n",
    "        sif_sim = calc_sif_similarity(s1_array[i], s2_array[i])\n",
    "        w_overlap, w_norm_overlap = calc_basic_overlap(s1_tokens[i], s2_tokens[i])\n",
    "        l_overlap, l_norm_overlap = calc_basic_overlap(s1_lemmas[i], s2_lemmas[i])\n",
    "        spacy_sim = calc_spacy_sim(s1_array[i], s2_array[i])\n",
    "#         syn_overlap, normalized_syn_overlap = calc_synset_overlap(s1_tokens[i], s2_tokens[i])\n",
    "        data.append([i, w_norm_overlap, l_norm_overlap, sif_sim, cos_sim, spacy_sim])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0.6428571428571429, 0.6, 0.4043188683415115, 0.5949218057093537, 0.9728363156062543], [1, 0.631578947368421, 0.6, 0.37040524322972224, 0.474330706497194, 0.9396972779230695], [2, 0.5, 0.5, 0.1358693286767868, 0.392181175971253, 0.9275870020498693], [3, 0.7333333333333333, 0.6666666666666666, 0.6935512636502701, 0.668348418668298, 0.9731175952414417], [4, 0.24, 0.24, 4.979960298599938e-10, 0.12170566815950139, 0.8512029318853689]]\n"
     ]
    }
   ],
   "source": [
    "data = pipeline(s1_arr_train, s2_arr_train, s1_tokens_train, s2_tokens_train, s1_lemmas_train, s2_lemmas_train)\n",
    "print(data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', '4', '3', '3', '2']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "In this section we fit our feature set to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=8,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=14,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier(random_state=14, max_depth=8)\n",
    "dt_classifier.fit(data,scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 237\n",
      "Max Depth: 8\n",
      "Accuracy: 0.7028301886792453\n"
     ]
    }
   ],
   "source": [
    "print(f'Nodes: {dt_classifier.tree_.node_count}')\n",
    "print(f'Max Depth: {dt_classifier.tree_.max_depth}')\n",
    "print(f'Accuracy: {dt_classifier.score(data, scores_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-c6d6ad15771c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms1_arr_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_arr_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1_tokens_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_tokens_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/dev-set.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdev_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_arr_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_arr_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdev_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m xgboost_model = xgb.XGBRegressor(booster='gbtree', \n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 5)"
     ]
    }
   ],
   "source": [
    "s1_arr_dev, s2_arr_dev, scores_dev, s1_tokens_dev, s2_tokens_dev = preprocess(\"./data/dev-set.txt\")\n",
    "dev_data = pipeline(s1_arr_dev, s2_arr_dev)\n",
    "dev_predictions = dt_classifier.predict(dev_data)\n",
    "\n",
    "xgboost_model = xgb.XGBRegressor(booster='gbtree', \n",
    "                       n_estimators=1000,\n",
    "                       n_jobs=4,\n",
    "                       learning_rate=.05,\n",
    "                       max_depth=3,\n",
    "                       random_state=42,\n",
    "                       gamma=.05,\n",
    "                       early_stopping_rounds = 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model.fit(data, scores_train,\n",
    "#                   eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "                  eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=14, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=14, n_estimators=100)\n",
    "rf_classifier.fit(data,scores_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  0.42105263157894735,\n",
       "  0.42105263157894735,\n",
       "  0.0001332665010484381,\n",
       "  0.4111217451482082,\n",
       "  0.9552089581976497],\n",
       " [1,\n",
       "  0.5714285714285714,\n",
       "  0.8571428571428571,\n",
       "  0.8164963796577869,\n",
       "  0.7168117414430619,\n",
       "  0.9945601722568282],\n",
       " [2,\n",
       "  0.2857142857142857,\n",
       "  0.2857142857142857,\n",
       "  0.707106779665172,\n",
       "  0.34464214103805474,\n",
       "  0.9519508734395983],\n",
       " [3, 0.6, 0.6, 9.042867432144338e-05, 0.4112070550676187, 0.9244398882914221],\n",
       " [4, 0.25, 0.25, 1.0, 1.0, 1.0]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_arr_dev, s2_arr_dev, scores_dev, s1_tokens_dev, s2_tokens_dev, s1_lemmas_dev,s2_lemmas_dev = preprocess(\"./data/dev-set.txt\")\n",
    "dev_data = pipeline(s1_arr_dev, s2_arr_dev, s1_tokens_dev, s2_tokens_dev, s1_lemmas_dev,s2_lemmas_dev)\n",
    "dt_dev_predictions = dt_classifier.predict(dev_data)\n",
    "rf_dev_predictions = rf_classifier.predict(dev_data)\n",
    "dev_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1209\n",
      "1209\n",
      "1209\n"
     ]
    }
   ],
   "source": [
    "# make sure our lengths match up\n",
    "print(len(scores_dev))\n",
    "print(len(dt_dev_predictions))\n",
    "print(len(rf_dev_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE:  0.22001654259718775\n",
      "RANDOM FOREST:  0.2878411910669975\n"
     ]
    }
   ],
   "source": [
    "def calc_accuracy(predictions, scores):\n",
    "    correct = 0\n",
    "    for i in range(0, len(predictions)): \n",
    "        if predictions[i] == scores[i]:\n",
    "            correct += 1\n",
    "    return correct / len(predictions) \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "print('DECISION TREE: ', calc_accuracy(dt_dev_predictions, scores_dev))\n",
    "print('RANDOM FOREST: ', calc_accuracy(rf_dev_predictions, scores_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
