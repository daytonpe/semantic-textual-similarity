{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# XGBOOST\n",
    "import xgboost as xgb\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# SPACY IMPORT\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data and PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same readData from STS.py\n",
    "def readData(fileName):\n",
    "\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    score = []\n",
    "    file = open(fileName, encoding=\"utf8\")\n",
    "    text = file.readline()\n",
    "    text = file.read()\n",
    "    \n",
    "    # loop to extract a set of two sentences\n",
    "    for sentence in text.split('\\n'):\n",
    "\n",
    "        # creating two separate lists of the sentences\n",
    "        # '.rstrip('.') only removes the last period in the sentence\n",
    "        \n",
    "        s1.insert(len(s1), (sentence.split('\\t')[1].lower()).rstrip('.'))\n",
    "        s2.insert(len(s1), (sentence.split('\\t')[2].lower()).rstrip('.'))\n",
    "        \n",
    "        # inserting the score as a separate lists\n",
    "        score.insert(len(s1), (sentence.split('\\t')[3]))\n",
    "\n",
    "    # print(s1)\n",
    "    return s1, s2, score\n",
    "\n",
    "\n",
    "def preprocess(fileName):\n",
    "\n",
    "    s1, s2, scores = readData(fileName)\n",
    "    s1_toks = []\n",
    "    s2_toks = []\n",
    "\n",
    "    # tokenizing and tagging\n",
    "    s1_tags = []\n",
    "    s2_tags = []\n",
    "\n",
    "    for sentence in s1:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        s1_toks.insert(len(s1_toks), tokens)\n",
    "        s1_tags.insert(\n",
    "            len(s1_tags), nltk.pos_tag(tokens))\n",
    "\n",
    "    for sentence in s2:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        s2_toks.insert(len(s2_toks), tokens)\n",
    "        s2_tags.insert(\n",
    "            len(s2_tags), nltk.pos_tag(tokens))\n",
    "    \n",
    "    # Remove the unnecessary tuple and keep just the tags\n",
    "    for i, tag_list in enumerate(s1_tags):\n",
    "        s1_tags[i] = [tup[1] for tup in tag_list]\n",
    "    for i, tag_list in enumerate(s2_tags):\n",
    "        s2_tags[i] = [tup[1] for tup in tag_list]\n",
    "\n",
    "    # lemmatizing\n",
    "    s1_lemmas = []\n",
    "    s2_lemmas = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for sentence in s1_toks:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        s1_lemmas.insert(\n",
    "            len(s1_lemmas), sentence_components)\n",
    "\n",
    "    for sentence in s2_toks:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        s2_lemmas.insert(\n",
    "            len(s2_lemmas), sentence_components)\n",
    "\n",
    "        \n",
    "    # Zipping it all together into one object for each word\n",
    "    s1_word_lists = []\n",
    "    s2_word_lists = []\n",
    "    \n",
    "    for tok_list, lem_list, tag_list in zip(s1_toks, s1_lemmas, s1_tags):\n",
    "        sentence_words = []\n",
    "        for tok, lem, tag in zip(tok_list, lem_list, tag_list):\n",
    "            word = {}\n",
    "            word['tok'] = tok\n",
    "            word['lem'] = lem\n",
    "            word['tag'] = tag\n",
    "            sentence_words.append(word)\n",
    "        s1_word_lists.append(sentence_words) \n",
    "        \n",
    "    for tok_list, lem_list, tag_list in zip(s2_toks, s2_lemmas, s2_tags):\n",
    "        sentence_words = []\n",
    "        for tok, lem, tag in zip(tok_list, lem_list, tag_list):\n",
    "            word = {}\n",
    "            word['tok'] = tok\n",
    "            word['lem'] = lem\n",
    "            word['tag'] = tag\n",
    "            sentence_words.append(word)\n",
    "        s2_word_lists.append(sentence_words)  \n",
    "              \n",
    "    \n",
    "    # Create a corpus object to represent our corpus\n",
    "    corpus = {}\n",
    "    corpus[\"s1\"] = {}\n",
    "    corpus[\"s2\"] = {}\n",
    "    corpus['scores'] = [int(i) for i in scores]\n",
    "    \n",
    "    corpus[\"s1\"][\"sentences\"] = s1\n",
    "    corpus[\"s2\"][\"sentences\"] = s2\n",
    "    \n",
    "    corpus[\"s1\"][\"tokens\"] = s1_toks\n",
    "    corpus[\"s2\"][\"tokens\"] = s2_toks\n",
    "    \n",
    "    corpus[\"s1\"][\"lemmas\"] = s1_lemmas\n",
    "    corpus[\"s2\"][\"lemmas\"] = s2_lemmas\n",
    "    \n",
    "    corpus[\"s1\"][\"tags\"] = s1_tags\n",
    "    corpus[\"s2\"][\"tags\"] = s2_tags\n",
    "    \n",
    "    corpus[\"s1\"][\"words\"] = s1_word_lists\n",
    "    corpus[\"s2\"][\"words\"] = s2_word_lists\n",
    "    \n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = preprocess(\"./data/train-set.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tok': 'but', 'lem': 'but', 'tag': 'CC'},\n",
       " {'tok': 'other', 'lem': 'other', 'tag': 'JJ'},\n",
       " {'tok': 'sources', 'lem': 'source', 'tag': 'NNS'},\n",
       " {'tok': 'close', 'lem': 'close', 'tag': 'RB'},\n",
       " {'tok': 'to', 'lem': 'to', 'tag': 'TO'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['s2']['words'][0][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_sentence_list = train_data['s1']['tokens']+train_data['s2']['tokens']\n",
    "words_filtered = []\n",
    "\n",
    "# print(words)\n",
    "\n",
    "# looking through I've noticed there are a number of stop-words that can be added to the set\n",
    "stop_words.add(',')\n",
    "stop_words.add('``')\n",
    "stop_words.add(\"n't\")\n",
    "\n",
    "for tsl in tokenized_sentence_list:\n",
    "    for w in tsl:\n",
    "        if w not in stop_words and w not in words_filtered:\n",
    "            words_filtered.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_distribution(corpus):\n",
    "    s1_toks = corpus['s1']['tokens']\n",
    "    s2_toks = corpus['s2']['tokens']    \n",
    "    freq_dist = FreqDist()\n",
    "    for i in range(len(s1_toks)):\n",
    "        for token in (s1_toks[i] + s2_toks[i]):\n",
    "            freq_dist[token.lower()] += 1\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 5169), (',', 3690), ('of', 2497), ('to', 2133), ('and', 1716), ('a', 1615), ('in', 1573), ('is', 891), ('that', 831), ('on', 820), ('for', 756), ('it', 587), ('this', 579), ('we', 531), ('with', 464), ('be', 459), ('by', 443), ('i', 425), ('which', 403), ('have', 384), ('not', 366), ('at', 343), ('as', 334), ('are', 333), ('has', 319), ('said', 316), ('was', 304), ('european', 287), (\"'s\", 280), ('from', 261), ('``', 252), (\"''\", 242), ('will', 233), ('.', 229), ('also', 223), ('its', 194), ('but', 193), ('would', 191), ('all', 188), ('percent', 187)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist = frequency_distribution(train_data)\n",
    "\n",
    "print(freq_dist.most_common(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Distribution\n",
    "\n",
    "# TODO\n",
    "this data is super imbalanced. we likely need to balance it in preprocessing. `https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0:    8   0.54 per\n",
      " 1:   37   2.49 per\n",
      " 2:   95   6.40 per\n",
      " 3:  310  20.89 per\n",
      " 4:  616  41.51 per\n",
      " 5:  418  28.17 per\n"
     ]
    }
   ],
   "source": [
    "score_list = [0,0,0,0,0,0]\n",
    "for s in train_data['scores']:\n",
    "    score_list[int(s)] += 1\n",
    "for i in range(0, len(score_list)):\n",
    "    print(\"% 1d: % 4d % 6.2f per\" %(i, score_list[i], 100*score_list[i]/len(train_data['scores']))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This section includes all the code/functions to create features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_tokens(token_list):\n",
    "    blank_list = []\n",
    "    for w in token_list:\n",
    "        if w not in blank_list:\n",
    "            blank_list.append(w)\n",
    "    return blank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(token_list):\n",
    "    blank_list = []\n",
    "    for w in token_list:\n",
    "        if w not in stop_words:\n",
    "            blank_list.append(w)\n",
    "    return blank_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity (TF-IDF)\n",
    "\n",
    "This is the same as the spacy similarity. This one is probably less accurate though as I don't believe it's trained from the GloVe w2v model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cosine_similarity(s1, s2):\n",
    "\n",
    "    # remove the stopwords, transform into TF-IDF matrix, then\n",
    "    tfidf_matrix = TfidfVectorizer(\n",
    "        stop_words=\"english\").fit_transform([s1, s2])\n",
    "    \n",
    "    cos_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # print(tfidf_matrix.toarray())\n",
    "\n",
    "    cos_sim = cos_sim_matrix[0][1]\n",
    "\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33609692727625756"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_cos_sim('I like some apples', 'I like the pears')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_spacy_sim(s1, s2):\n",
    "    s2 = nlp(s2)\n",
    "    s1 = nlp(s1)\n",
    "    return s1.similarity(s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9361166303666173"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_spacy_sim('I like some apples', 'I like the pears')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth Inverse Frequency (SIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sif_sim(s1, s2, a = .001):\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform([s1, s2])\n",
    "    X_arr = X.toarray()\n",
    "    sif_matrix = []\n",
    "    for i in range(0, len(X_arr)):\n",
    "        sif_arr = []\n",
    "        for j in range(0, len(X_arr[i])):\n",
    "            word = vectorizer.get_feature_names()[j]\n",
    "            w = a / (a + freq_dist[word])\n",
    "            v = X_arr[i][j]\n",
    "            sif_arr.append(v*w)\n",
    "        sif_matrix.append(sif_arr)\n",
    "    sif_cos_sim_matrix = cosine_similarity(sif_matrix, sif_matrix)\n",
    "    sif_cos_sim = sif_cos_sim_matrix[0][1]\n",
    "    return sif_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4515545128534995e-10"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_sif_sim('I like some apples', 'I like the pears')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Overlap\n",
    "\n",
    "Unique words that are in both sentences divided by the total number of words in both sentences. Does not include stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_basic_overlap(s1_tokens, s2_tokens):\n",
    "    s1_tokens = remove_stopwords(s1_tokens)\n",
    "    s1_tokens = remove_duplicate_tokens(s1_tokens)\n",
    "\n",
    "    s2_tokens = remove_stopwords(s2_tokens)\n",
    "    s2_tokens = remove_duplicate_tokens(s2_tokens)\n",
    "    \n",
    "    overlap = 0\n",
    "    encountered_indexes = []\n",
    "    for word in (s1_tokens+s2_tokens):\n",
    "        try:\n",
    "            word_index = words_filtered.index(word)\n",
    "            if word_index in encountered_indexes: # we know we have found an overlap\n",
    "                overlap += 1\n",
    "            encountered_indexes.append(word_index)\n",
    "        except ValueError:\n",
    "            # print(word + ' not found in lexicon. Skipping...')\n",
    "            continue\n",
    "\n",
    "    avg_sentence_len = len(s1_tokens+s2_tokens) / 2\n",
    "    \n",
    "    overlap_normlalized = overlap / avg_sentence_len\n",
    "    return overlap, overlap_normlalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synset Overlap\n",
    "\n",
    "# TODO\n",
    "\n",
    "We may be able to incorporate POS and dependency parsing here as right now i'm just taking the first synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 0.23173803526448364)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_synset_overlap(s1_tokens, s2_tokens):\n",
    "    s1_tokens = remove_stopwords(s1_tokens)\n",
    "    s1_tokens = remove_duplicate_tokens(s1_tokens)\n",
    "\n",
    "    s2_tokens = remove_stopwords(s2_tokens)\n",
    "    s2_tokens = remove_duplicate_tokens(s2_tokens)\n",
    "    \n",
    "#     print(s2_tokens)\n",
    "#     print(s1_tokens)\n",
    "\n",
    "    s1_spread = []\n",
    "    s2_spread = []\n",
    "    \n",
    "    for word in s1_tokens:\n",
    "        for synset in wn.synsets(word):\n",
    "            for i in range(0, len(synset.lemmas())):\n",
    "                syn_word = synset.lemmas()[i].name()\n",
    "                if syn_word not in s1_spread:\n",
    "                    s1_spread.append(syn_word)\n",
    "\n",
    "    for word in s2_tokens:\n",
    "        for synset in wn.synsets(word):\n",
    "            for i in range(0, len(synset.lemmas())):\n",
    "                syn_word = synset.lemmas()[i].name()\n",
    "                if syn_word not in s2_spread:\n",
    "                    s2_spread.append(syn_word)         \n",
    "    \n",
    "    return calc_basic_overlap(s1_spread, s2_spread)\n",
    "    \n",
    "calc_synset_overlap(train_data['s1']['tokens'][0], train_data['s2']['tokens'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "In this section we run the data through the pipeline to get it into the form necessary to create our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(corpus):\n",
    "    # TODO add a check to ensure the lengths of these arrays are the same\n",
    "    # or add the basic processing to pipeline\n",
    "    \n",
    "    s1_array = corpus['s1']['sentences']\n",
    "    s2_array = corpus['s2']['sentences']\n",
    "    s1_tokens = corpus['s1']['tokens']\n",
    "    s2_tokens = corpus['s2']['tokens']\n",
    "    s1_lemmas = corpus['s1']['lemmas']\n",
    "    s2_lemmas = corpus['s2']['lemmas']\n",
    "    \n",
    "    data = []\n",
    "    for i in range(0, len(s1_array)):\n",
    "        cos_sim = calc_cosine_similarity(s1_array[i], s2_array[i])\n",
    "        sif_sim = calc_sif_similarity(s1_array[i], s2_array[i])\n",
    "        w_overlap, w_norm_overlap = calc_basic_overlap(s1_tokens[i], s2_tokens[i])\n",
    "        l_overlap, l_norm_overlap = calc_basic_overlap(s1_lemmas[i], s2_lemmas[i])\n",
    "        spacy_sim = calc_spacy_sim(s1_array[i], s2_array[i])\n",
    "        syn_overlap, normalized_syn_overlap = calc_synset_overlap(s1_tokens[i], s2_tokens[i])\n",
    "        data.append([w_norm_overlap, l_norm_overlap, sif_sim, cos_sim, spacy_sim, syn_overlap, normalized_syn_overlap])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0.6428571428571429, 0.6, 0.4043188683415115, 0.5949218057093537, 0.9699586985720816, 46, 0.23173803526448364], [1, 0.631578947368421, 0.6, 0.37040524322972224, 0.474330706497194, 0.9316565540040632, 24, 0.22018348623853212], [2, 0.5, 0.5, 0.1358693286767868, 0.392181175971253, 0.9247478261787359, 32, 0.2098360655737705], [3, 0.7333333333333333, 0.6666666666666666, 0.6935512636502701, 0.668348418668298, 0.9677497361187998, 28, 0.20363636363636364], [4, 0.24, 0.24, 4.979960298599938e-10, 0.12170566815950139, 0.8618764553778161, 22, 0.21674876847290642]]\n"
     ]
    }
   ],
   "source": [
    "train_input = pipeline(train_data)\n",
    "print(data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 3, 3, 2]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['scores'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "In this section we fit our feature set to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=8,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=14,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier(random_state=14, max_depth=8)\n",
    "dt_classifier.fit(train_input,train_data['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 265\n",
      "Max Depth: 8\n",
      "Accuracy: 0.6967654986522911\n"
     ]
    }
   ],
   "source": [
    "print(f'Nodes: {dt_classifier.tree_.node_count}')\n",
    "print(f'Max Depth: {dt_classifier.tree_.max_depth}')\n",
    "\n",
    "training_scores = train_data['scores']\n",
    "print(f'Accuracy: {dt_classifier.score(train_input, training_scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=False, random_state=14, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=14, n_estimators=100)\n",
    "rf_classifier.fit(train_input, train_data['scores'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model = XGBClassifier(booster='gbtree', \n",
    "                       n_estimators=1000,\n",
    "                       n_jobs=4,\n",
    "                       learning_rate=.05,\n",
    "                       max_depth=7,\n",
    "                       random_state=42,\n",
    "                       #gamma=.05,\n",
    "                       early_stopping_rounds = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0.05,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=2000, n_jobs=4,\n",
       "       nthread=None, objective='multi:softprob', random_state=42,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_model = XGBClassifier(n_jobs=4, n_estimators=2000, gamma=.05, random_state=42)\n",
    "xgboost_model.fit(np.asarray(train_input), np.asarray(train_data['scores']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data = preprocess(\"./data/dev-set.txt\")\n",
    "dev_input = pipeline(dev_data)\n",
    "\n",
    "dt_dev_predictions = dt_classifier.predict(dev_input)\n",
    "rf_dev_predictions = rf_classifier.predict(dev_input)\n",
    "xgb_dev_predictions = xgboost_model.predict(np.asarray(dev_input))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1209\n",
      "1209\n",
      "1209\n",
      "1209\n"
     ]
    }
   ],
   "source": [
    "# make sure our lengths match up\n",
    "print(len(dev_data['scores']))\n",
    "print(len(dt_dev_predictions))\n",
    "print(len(rf_dev_predictions))\n",
    "print(len(xgb_dev_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Avg Error</th>\n",
       "      <th>F-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.318445</td>\n",
       "      <td>1.040529</td>\n",
       "      <td>0.327107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.386270</td>\n",
       "      <td>0.878412</td>\n",
       "      <td>0.393129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.394541</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.403824</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Avg Error   F-Score\n",
       "0  Decision Tree  0.318445   1.040529  0.327107\n",
       "1  Random Forest  0.386270   0.878412  0.393129\n",
       "2        XGBoost  0.394541   0.897436  0.403824"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_metrics(name, predictions, scores):\n",
    "    correct = 0\n",
    "    total_error = 0\n",
    "    for i in range(0, len(predictions)): \n",
    "        if predictions[i] == scores[i]:\n",
    "            correct += 1\n",
    "        total_error += abs(int(scores[i]) - int(predictions[i]))\n",
    "    acc = correct / len(predictions) \n",
    "    avg_err = total_error / len(predictions)\n",
    "    \n",
    "    f1 = f1_score(scores, predictions, average='weighted')\n",
    "    return name, acc, avg_err, f1\n",
    "\n",
    "\n",
    "# Ensure all our arrays are full of ints for metrics.\n",
    "dev_data['scores'] = [int(i) for i in dev_data['scores']] \n",
    "rf_dev_predictions = [int(i) for i in rf_dev_predictions] \n",
    "xgb_dev_predictions = [int(i) for i in xgb_dev_predictions] \n",
    "dt_dev_predictions = [int(i) for i in dt_dev_predictions] \n",
    "\n",
    "\n",
    "\n",
    "xgb_metrics = get_metrics('XGBoost', xgb_dev_predictions, dev_data['scores'])\n",
    "dt_metrics  = get_metrics('Decision Tree', dt_dev_predictions, dev_data['scores'])\n",
    "rf_metrics = get_metrics('Random Forest', rf_dev_predictions, dev_data['scores'])\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [dt_metrics, rf_metrics, xgb_metrics], \n",
    "    columns = ['Model', 'Accuracy', 'Avg Error', 'F-Score']) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
