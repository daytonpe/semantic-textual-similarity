{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import pickle\n",
    "\n",
    "# XGBOOST\n",
    "import xgboost as xgb\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# SPACY IMPORT\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from spacy.symbols import nsubj, VERB\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data and PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same readData from STS.py\n",
    "def readData(fileName):\n",
    "\n",
    "    s1 = []\n",
    "    s2 = []\n",
    "    score = []\n",
    "    file = open(fileName, encoding=\"utf8\")\n",
    "    text = file.readline()\n",
    "    text = file.read()\n",
    "    \n",
    "    # loop to extract a set of two sentences\n",
    "    for sentence in text.split('\\n'):\n",
    "\n",
    "        # creating two separate lists of the sentences\n",
    "        # '.rstrip('.') only removes the last period in the sentence\n",
    "        \n",
    "        s1.insert(len(s1), (sentence.split('\\t')[1].lower()).rstrip('.'))\n",
    "        s2.insert(len(s1), (sentence.split('\\t')[2].lower()).rstrip('.'))\n",
    "        \n",
    "        # inserting the score as a separate lists\n",
    "        score.insert(len(s1), (sentence.split('\\t')[3]))\n",
    "\n",
    "    # print(s1)\n",
    "    return s1, s2, score\n",
    "\n",
    "\n",
    "def preprocess(fileName):\n",
    "\n",
    "    s1, s2, scores = readData(fileName)\n",
    "    s1_toks = []\n",
    "    s2_toks = []\n",
    "\n",
    "    # tokenizing and tagging\n",
    "    s1_tags = []\n",
    "    s2_tags = []\n",
    "\n",
    "    for sentence in s1:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        s1_toks.insert(len(s1_toks), tokens)\n",
    "        s1_tags.insert(\n",
    "            len(s1_tags), nltk.pos_tag(tokens))\n",
    "\n",
    "    for sentence in s2:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        s2_toks.insert(len(s2_toks), tokens)\n",
    "        s2_tags.insert(\n",
    "            len(s2_tags), nltk.pos_tag(tokens))\n",
    "    \n",
    "    # Remove the unnecessary tuple and keep just the tags\n",
    "    for i, tag_list in enumerate(s1_tags):\n",
    "        s1_tags[i] = [tup[1] for tup in tag_list]\n",
    "    for i, tag_list in enumerate(s2_tags):\n",
    "        s2_tags[i] = [tup[1] for tup in tag_list]\n",
    "\n",
    "    # lemmatizing\n",
    "    s1_lemmas = []\n",
    "    s2_lemmas = []\n",
    "    s1_ls = []\n",
    "    s2_ls = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for sentence in s1_toks:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        s1_lemmas.insert(\n",
    "            len(s1_lemmas), sentence_components)\n",
    "        s1_ls.insert(len(s1_ls), ' '.join(word for word in sentence_components))\n",
    "    \n",
    "    print(s1_ls[2])\n",
    "\n",
    "    for sentence in s2_toks:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        s2_lemmas.insert(\n",
    "            len(s2_lemmas), sentence_components)\n",
    "        s2_ls.insert(len(s2_ls), ' '.join(word for word in sentence_components))\n",
    "\n",
    "\n",
    "        \n",
    "    # Zipping it all together into one object for each word\n",
    "    s1_word_lists = []\n",
    "    s2_word_lists = []\n",
    "    \n",
    "    for tok_list, lem_list, tag_list in zip(s1_toks, s1_lemmas, s1_tags):\n",
    "        sentence_words = []\n",
    "        for tok, lem, tag in zip(tok_list, lem_list, tag_list):\n",
    "            word = {}\n",
    "            word['tok'] = tok\n",
    "            word['lem'] = lem\n",
    "            word['tag'] = tag\n",
    "            sentence_words.append(word)\n",
    "        s1_word_lists.append(sentence_words) \n",
    "        \n",
    "    for tok_list, lem_list, tag_list in zip(s2_toks, s2_lemmas, s2_tags):\n",
    "        sentence_words = []\n",
    "        for tok, lem, tag in zip(tok_list, lem_list, tag_list):\n",
    "            word = {}\n",
    "            word['tok'] = tok\n",
    "            word['lem'] = lem\n",
    "            word['tag'] = tag\n",
    "            sentence_words.append(word)\n",
    "        s2_word_lists.append(sentence_words)  \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Create a corpus object to represent our corpus\n",
    "    corpus = {}\n",
    "    corpus[\"s1\"] = {}\n",
    "    corpus[\"s2\"] = {}\n",
    "    corpus['scores'] = [int(i) for i in scores]\n",
    "    \n",
    "    corpus[\"s1\"][\"sentences\"] = s1\n",
    "    corpus[\"s2\"][\"sentences\"] = s2\n",
    "    \n",
    "    corpus[\"s1\"][\"tokens\"] = s1_toks\n",
    "    corpus[\"s2\"][\"tokens\"] = s2_toks\n",
    "    \n",
    "    corpus[\"s1\"][\"lemmas\"] = s1_lemmas\n",
    "    corpus[\"s2\"][\"lemmas\"] = s2_lemmas\n",
    "    \n",
    "    corpus[\"s1\"][\"tags\"] = s1_tags\n",
    "    corpus[\"s2\"][\"tags\"] = s2_tags\n",
    "    \n",
    "    corpus[\"s1\"][\"words\"] = s1_word_lists\n",
    "    corpus[\"s2\"][\"words\"] = s2_word_lists\n",
    "\n",
    "    corpus[\"s1\"][\"ls\"] = s1_ls\n",
    "    corpus[\"s2\"][\"ls\"] = s2_ls\n",
    "    \n",
    "    return corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the fine are part of failed republican effort to force or entice the democrat to return\n"
     ]
    }
   ],
   "source": [
    "#train_data = preprocess(\"./data/train-set.txt\")\n",
    "train_data = preprocess(\"D:\\\\UTD\\\\NLPProject\\\\v2\\\\semantic-textual-similarity\\\\data\\\\train-set.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'lem': 'but', 'tag': 'CC', 'tok': 'but'},\n",
       " {'lem': 'other', 'tag': 'JJ', 'tok': 'other'},\n",
       " {'lem': 'source', 'tag': 'NNS', 'tok': 'sources'},\n",
       " {'lem': 'close', 'tag': 'RB', 'tok': 'close'},\n",
       " {'lem': 'to', 'tag': 'TO', 'tok': 'to'}]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['s2']['words'][0][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_sentence_list = train_data['s1']['tokens']+train_data['s2']['tokens']\n",
    "words_filtered = []\n",
    "\n",
    "# print(words)\n",
    "\n",
    "# looking through I've noticed there are a number of stop-words that can be added to the set\n",
    "stop_words.add(',')\n",
    "stop_words.add('``')\n",
    "stop_words.add(\"n't\")\n",
    "\n",
    "for tsl in tokenized_sentence_list:\n",
    "    for w in tsl:\n",
    "        if w not in stop_words and w not in words_filtered:\n",
    "            words_filtered.append(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_distribution(corpus):\n",
    "    s1_toks = corpus['s1']['tokens']\n",
    "    s2_toks = corpus['s2']['tokens']    \n",
    "    freq_dist = FreqDist()\n",
    "    for i in range(len(s1_toks)):\n",
    "        for token in (s1_toks[i] + s2_toks[i]):\n",
    "            freq_dist[token.lower()] += 1\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 5169), (',', 3690), ('of', 2497), ('to', 2133), ('and', 1716), ('a', 1615), ('in', 1573), ('is', 891), ('that', 831), ('on', 820), ('for', 756), ('it', 587), ('this', 579), ('we', 531), ('with', 464), ('be', 459), ('by', 443), ('i', 425), ('which', 403), ('have', 384), ('not', 366), ('at', 343), ('as', 334), ('are', 333), ('has', 319), ('said', 316), ('was', 304), ('european', 287), (\"'s\", 280), ('from', 261), ('``', 252), (\"''\", 242), ('will', 233), ('.', 229), ('also', 223), ('its', 194), ('but', 193), ('would', 191), ('all', 188), ('percent', 187)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist = frequency_distribution(train_data)\n",
    "\n",
    "print(freq_dist.most_common(40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Distribution\n",
    "\n",
    "# TODO\n",
    "this data is super imbalanced. we likely need to balance it in preprocessing. `https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0:    0   0.00 per\n",
      " 1:   45   3.03 per\n",
      " 2:   95   6.40 per\n",
      " 3:  310  20.89 per\n",
      " 4:  616  41.51 per\n",
      " 5:  418  28.17 per\n"
     ]
    }
   ],
   "source": [
    "score_list = [0,0,0,0,0,0]\n",
    "for s in train_data['scores']:\n",
    "    score_list[int(s)] += 1\n",
    "for i in range(0, len(score_list)):\n",
    "    print(\"% 1d: % 4d % 6.2f per\" %(i, score_list[i], 100*score_list[i]/len(train_data['scores']))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This section includes all the code/functions to create features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_tokens(token_list):\n",
    "    blank_list = []\n",
    "    for w in token_list:\n",
    "        if w not in blank_list:\n",
    "            blank_list.append(w)\n",
    "    return blank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(token_list):\n",
    "    blank_list = []\n",
    "    for w in token_list:\n",
    "        if w not in stop_words:\n",
    "            blank_list.append(w)\n",
    "    return blank_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity (TF-IDF)\n",
    "\n",
    "This is the same as the spacy similarity. This one is probably less accurate though as I don't believe it's trained from the GloVe w2v model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cosine_similarity(s1, s2):\n",
    "\n",
    "    # remove the stopwords, transform into TF-IDF matrix, then\n",
    "    tfidf_matrix = TfidfVectorizer(\n",
    "        stop_words=\"english\").fit_transform([s1, s2])\n",
    "    \n",
    "    cos_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "    \n",
    "    # print(tfidf_matrix.toarray())\n",
    "\n",
    "    cos_sim = cos_sim_matrix[0][1]\n",
    "\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33609692727625756"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_cosine_similarity('I like some apples', 'I like the pears')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spacy Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_spacy_sim(s1, s2):\n",
    "    s2 = nlp(s2)\n",
    "    s1 = nlp(s1)\n",
    "    return s1.similarity(s2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9361166303666173"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_spacy_sim('I like some apples', 'I like the pears')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth Inverse Frequency (SIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sif_similarity(s1, s2, a = .001):\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform([s1, s2])\n",
    "    X_arr = X.toarray()\n",
    "    sif_matrix = []\n",
    "    for i in range(0, len(X_arr)):\n",
    "        sif_arr = []\n",
    "        for j in range(0, len(X_arr[i])):\n",
    "            word = vectorizer.get_feature_names()[j]\n",
    "            w = a / (a + freq_dist[word])\n",
    "            v = X_arr[i][j]\n",
    "            sif_arr.append(v*w)\n",
    "        sif_matrix.append(sif_arr)\n",
    "    sif_cos_sim_matrix = cosine_similarity(sif_matrix, sif_matrix)\n",
    "    sif_cos_sim = sif_cos_sim_matrix[0][1]\n",
    "    return sif_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4515545128534995e-10"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_sif_similarity('I like some apples', 'I like the pears')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Overlap\n",
    "\n",
    "Unique words that are in both sentences divided by the total number of words in both sentences. Does not include stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_basic_overlap(s1_tokens, s2_tokens):\n",
    "    s1_tokens = remove_stopwords(s1_tokens)\n",
    "    s1_tokens = remove_duplicate_tokens(s1_tokens)\n",
    "\n",
    "    s2_tokens = remove_stopwords(s2_tokens)\n",
    "    s2_tokens = remove_duplicate_tokens(s2_tokens)\n",
    "    \n",
    "    overlap = 0\n",
    "    encountered_words = []\n",
    "    for word in (s1_tokens+s2_tokens):\n",
    "        try:\n",
    "            if word in encountered_words: # we know we have found an overlap\n",
    "                overlap += 1\n",
    "            encountered_words.append(word)\n",
    "        except ValueError:\n",
    "            # print(word + ' not found in lexicon. Skipping...')\n",
    "            continue\n",
    "\n",
    "    avg_sentence_len = len(s1_tokens+s2_tokens) / 2\n",
    "    \n",
    "    overlap_normlalized = overlap / avg_sentence_len\n",
    "    return overlap, overlap_normlalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def named_entity_overlap(s1, s2):    \n",
    "\n",
    "\n",
    "    # print(s1)\n",
    "    \n",
    "    sentence_nlp1= nlp(s1)\n",
    "    ner1= [(word.text, word.ent_type_) for word in sentence_nlp1 if word.ent_type_]\n",
    "    \n",
    "    sentence_nlp2=nlp(s2)\n",
    "    ner2 = [(word.text, word.ent_type_) for word in sentence_nlp2 if word.ent_type_]\n",
    "    \n",
    "    #list_ner = [list(a) for a in zip(ner1, ner2)]\n",
    "    \n",
    "    #print(s1[0:5])\n",
    "    #print(s2[0:5])\n",
    "    \n",
    "    #print(ner1)\n",
    "    #print()\n",
    "    #print(ner2)\n",
    "    #print()\n",
    "    #print(list_ner[0:5])\n",
    "    overlap = []\n",
    "\n",
    "    \n",
    "    da = {k:v for k,v in ner1}\n",
    "    db = {k:v for k,v in ner2}\n",
    "    total_length = len(set(ner1+ner2))\n",
    "    temp = []\n",
    "    for a in da.keys():\n",
    "        for b in db.keys():\n",
    "            if a==b:\n",
    "                temp.insert(len(temp), a)\n",
    "    if total_length != 0:\n",
    "        overlap =len(temp)/total_length\n",
    "    else:\n",
    "        overlap = 0.0\n",
    "    #overlap.insert(len(overlap),[k  for k in da.keys() and db.keys() if da[k] == db[k]] else continue)\n",
    "    #print()\n",
    "   # print(overlap[0:5]) #total common named entities / total unique named entities in both sentences\n",
    "\n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Head Verb Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_verb_overlap(s1, s2):\n",
    "    s1_nlp = nlp(s1)\n",
    "    s2_nlp = nlp(s2)\n",
    "\n",
    "# Finding a verb with a subject from below — good\n",
    "    verbs1 = []\n",
    "    for possible_subject in s1_nlp:\n",
    "        if possible_subject.dep == nsubj and possible_subject.head.pos == VERB and possible_subject.head.text not in verbs1:\n",
    "            verbs1.append(possible_subject.head.text)\n",
    "    verbs2 = []\n",
    "    for possible_subject in s2_nlp:\n",
    "        if possible_subject.dep == nsubj and possible_subject.head.pos == VERB and possible_subject.head.text not in verbs2:\n",
    "            verbs2.append(possible_subject.head.text)\n",
    "    \n",
    "    overlap = [value for value in verbs1 if value in verbs2]\n",
    "    verb_overlap = len(overlap)\n",
    "    \n",
    "    total_verbs = len(list(set(verbs1+verbs2)))\n",
    "    if total_verbs == 0:\n",
    "        return 0 \n",
    "    else:\n",
    "        return verb_overlap/total_verbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synset Overlap\n",
    "\n",
    "# TODO\n",
    "\n",
    "We may be able to incorporate POS and dependency parsing here as right now i'm just taking the first synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(144, 0.72544080604534)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calc_synset_overlap(s1_tokens, s2_tokens):\n",
    "    s1_tokens = remove_stopwords(s1_tokens)\n",
    "    s1_tokens = remove_duplicate_tokens(s1_tokens)\n",
    "\n",
    "    s2_tokens = remove_stopwords(s2_tokens)\n",
    "    s2_tokens = remove_duplicate_tokens(s2_tokens)\n",
    "    \n",
    "#     print(s2_tokens)\n",
    "#     print(s1_tokens)\n",
    "\n",
    "    s1_spread = []\n",
    "    s2_spread = []\n",
    "    \n",
    "    for word in s1_tokens:\n",
    "        for synset in wn.synsets(word):\n",
    "            for i in range(0, len(synset.lemmas())):\n",
    "                syn_word = synset.lemmas()[i].name()\n",
    "                if syn_word not in s1_spread:\n",
    "                    s1_spread.append(syn_word)\n",
    "\n",
    "    for word in s2_tokens:\n",
    "        for synset in wn.synsets(word):\n",
    "            for i in range(0, len(synset.lemmas())):\n",
    "                syn_word = synset.lemmas()[i].name()\n",
    "                if syn_word not in s2_spread:\n",
    "                    s2_spread.append(syn_word)         \n",
    "    \n",
    "    return calc_basic_overlap(s1_spread, s2_spread)\n",
    "    \n",
    "calc_synset_overlap(train_data['s1']['tokens'][0], train_data['s2']['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path Similarity: \n",
      "Dolphins are swimming mammals. Dolphins can swim. 1.0\n",
      "LCH Similarity: \n",
      "Dolphins are swimming mammals. Dolphins can swim. 3.6375861597263857\n",
      "LIN Similarity: \n",
      "Dolphins are swimming mammals. Dolphins can swim. 1.0\n",
      "JCN Similarity: \n",
      "Dolphins are swimming mammals. Dolphins can swim. 1e+300\n",
      "RES Similarity: \n",
      "Dolphins are swimming mammals. Dolphins can swim. 1e+300\n",
      "WUP Similarity: \n",
      "Dolphins are swimming mammals. Dolphins can swim. 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.wsd import lesk\n",
    "\n",
    "\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "\n",
    " \n",
    "def postag_to_synsettag(tag):\n",
    "    #print(tag)\n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    if tag.startswith('V') or tag == \"MD\":\n",
    "        return 'v'\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    "    return\n",
    "\n",
    "def tag_to_synset(sent, word, tag):\n",
    "    wn_tag = postag_to_synsettag(tag)\n",
    "    if wn_tag is None:\n",
    "        #print(1)\n",
    "        return None\n",
    "    else:\n",
    "        try:\n",
    "            #print(2)\n",
    "            return lesk(sent, word, wn_tag)\n",
    "        except:\n",
    "            #print(3)\n",
    "            return None\n",
    "\n",
    "def get_synsets(sentence1, sentence2):\n",
    "    sentence1 = pos_tag(word_tokenize(sentence1))\n",
    "    sentence2 = pos_tag(word_tokenize(sentence2))\n",
    " \n",
    "    # Get the synsets for the tagged words\n",
    "    synsets1 = [tag_to_synset(sentence1, *tagged_word) for tagged_word in sentence1]\n",
    "    synsets2 = [tag_to_synset(sentence2, *tagged_word) for tagged_word in sentence2]\n",
    "    #print(synsets1)\n",
    "    #print(synsets2)\n",
    "    # Filter out the Nones\n",
    "    synsets1 = [syn for syn in synsets1 if syn]\n",
    "    synsets2 = [syn for syn in synsets2 if syn]\n",
    "    #print(synsets1)\n",
    "    #print(synsets2)\n",
    "    return synsets1, synsets2\n",
    "\n",
    "def max_similarity_values():\n",
    "    t1 = \"hello\"\n",
    "    t2 = \"hello\"\n",
    "    s1, s2 = get_synsets(t1,t2)\n",
    "    semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "    jcn_maxVal = s1[0].jcn_similarity(s2[0], semcor_ic)\n",
    "    path_maxVal = s1[0].path_similarity(s2[0])\n",
    "    lin_maxVal = s1[0].lin_similarity(s2[0], semcor_ic)\n",
    "    res_maxVal = s1[0].res_similarity(s2[0], semcor_ic)\n",
    "    wup_maxVal = s1[0].wup_similarity(s2[0])\n",
    "    lch_maxVal = s1[0].lch_similarity(s2[0])\n",
    "    \n",
    "    return path_maxVal, wup_maxVal, lin_maxVal, lch_maxVal, res_maxVal, jcn_maxVal\n",
    "\n",
    "\n",
    "def sentence_path_similarity(sentence1, sentence2):\n",
    "    synsets1, synsets2 = get_synsets(sentence1, sentence2)\n",
    "    score, count = 0.0, 0\n",
    "    best = 0.0\n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        for synset2 in synsets2:\n",
    "            if synset.path_similarity(synset2) is not None and synset.path_similarity(synset2) > best:\n",
    "                try:\n",
    "                    best = synset.path_similarity(synset2)\n",
    "                except TypeError:\n",
    "                    continue\n",
    "        # Check that the similarity could have been computed\n",
    "        if best is not None:\n",
    "            score += best\n",
    "            count += 1\n",
    "    # Average the values\n",
    "    try:\n",
    "        score /= count\n",
    "    except:\n",
    "        return 0.0\n",
    "    return score\n",
    "\n",
    "def sentence_wup_similarity(sentence1, sentence2):\n",
    "    synsets1, synsets2 = get_synsets(sentence1, sentence2)\n",
    "    score, count = 0.0, 0\n",
    "    best = 0.0\n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        for synset2 in synsets2:\n",
    "            if synset.wup_similarity(synset2) is not None and synset.wup_similarity(synset2) > best:\n",
    "                try:\n",
    "                    best = synset.wup_similarity(synset2)\n",
    "                except TypeError:\n",
    "                    continue\n",
    "        # Check that the similarity could have been computed\n",
    "        if best is not None:\n",
    "            score += best\n",
    "            count += 1\n",
    "    # Average the values\n",
    "    try:\n",
    "        score /= count\n",
    "    except:\n",
    "        return 0.0\n",
    "    return score\n",
    "def sentence_lin_similarity(sentence1, sentence2):\n",
    "    synsets1, synsets2 = get_synsets(sentence1, sentence2)\n",
    "    score, count = 0.0, 0\n",
    "    best = 0.0\n",
    "    semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        for synset2 in synsets2:\n",
    "            try:\n",
    "                if synset.lin_similarity(synset2, semcor_ic) is not None and synset.lin_similarity(synset2, semcor_ic) > best:\n",
    "                    try:\n",
    "                        best = synset.lin_similarity(synset2, semcor_ic)\n",
    "                    except TypeError:\n",
    "                        continue\n",
    "            except:\n",
    "                continue\n",
    "        # Check that the similarity could have been computed\n",
    "        if best is not None:\n",
    "            score += best\n",
    "            count += 1\n",
    "    # Average the values\n",
    "    try:\n",
    "        score /= count\n",
    "    except:\n",
    "        return 0.0\n",
    "    return score\n",
    "\n",
    "def sentence_res_similarity(sentence1, sentence2):\n",
    "    synsets1, synsets2 = get_synsets(sentence1, sentence2)\n",
    "    score, count = 0.0, 0\n",
    "    best = 0.0\n",
    "    semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        for synset2 in synsets2:\n",
    "            try:\n",
    "                if synset.res_similarity(synset2, semcor_ic) is not None and synset.res_similarity(synset2, semcor_ic) > best:\n",
    "                    try:\n",
    "                        best = synset.res_similarity(synset2, semcor_ic)\n",
    "                    except TypeError:\n",
    "                        continue\n",
    "            except:\n",
    "                continue\n",
    "        # Check that the similarity could have been computed\n",
    "        if best is not None:\n",
    "            score += best\n",
    "            count += 1\n",
    "    # Average the values\n",
    "    try:\n",
    "        score /= count\n",
    "    except:\n",
    "        return 0.0\n",
    "    return score\n",
    "def sentence_lch_similarity(sentence1, sentence2):\n",
    "    synsets1, synsets2 = get_synsets(sentence1, sentence2)\n",
    "    score, count = 0.0, 0\n",
    "    best = 0.0\n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        for synset2 in synsets2:\n",
    "            try:\n",
    "                if synset.lch_similarity(synset2) is not None and synset.lch_similarity(synset2) > best:\n",
    "                    try:\n",
    "                        best = synset.lch_similarity(synset2)\n",
    "                    except TypeError:\n",
    "                        continue\n",
    "            except:\n",
    "                continue\n",
    "        # Check that the similarity could have been computed\n",
    "        if best is not None:\n",
    "            score += best\n",
    "            count += 1\n",
    "    # Average the values\n",
    "    try:\n",
    "        score /= count\n",
    "    except:\n",
    "        return 0.0\n",
    "    return score\n",
    "\n",
    "def sentence_jcn_similarity(sentence1, sentence2):\n",
    "    synsets1, synsets2 = get_synsets(sentence1, sentence2)\n",
    "    score, count = 0.0, 0\n",
    "    best = 0.0\n",
    "    semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "    # For each word in the first sentence\n",
    "    for synset in synsets1:\n",
    "        # Get the similarity value of the most similar word in the other sentence\n",
    "        for synset2 in synsets2:\n",
    "            try:\n",
    "                if synset.jcn_similarity(synset2, semcor_ic) is not None and synset.jcn_similarity(synset2, semcor_ic) > best:\n",
    "                    try:\n",
    "                        best = synset.jcn_similarity(synset2, semcor_ic)\n",
    "                    except TypeError:\n",
    "                        continue\n",
    "            except:\n",
    "                continue\n",
    "        # Check that the similarity could have been computed\n",
    "        if best is not None:\n",
    "            score += best\n",
    "            count += 1\n",
    "    # Average the values\n",
    "    try:\n",
    "          score /= count\n",
    "    except:\n",
    "          return 0.0\n",
    "    return score\n",
    "\n",
    "def symmetric_sentence_path_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\n",
    "    return (sentence_path_similarity(sentence1, sentence2) + sentence_path_similarity(sentence2, sentence1)) / 2 \n",
    "def symmetric_sentence_wup_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\n",
    "    return (sentence_wup_similarity(sentence1, sentence2) + sentence_wup_similarity(sentence2, sentence1)) / 2 \n",
    "def symmetric_sentence_res_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\n",
    "    return (sentence_res_similarity(sentence1, sentence2) + sentence_res_similarity(sentence2, sentence1)) / 2 \n",
    "\n",
    "def symmetric_sentence_lch_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\n",
    "    return (sentence_lch_similarity(sentence1, sentence2) + sentence_lch_similarity(sentence2, sentence1)) / 2 \n",
    "def symmetric_sentence_lin_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\n",
    "    return (sentence_lin_similarity(sentence1, sentence2) + sentence_lin_similarity(sentence2, sentence1)) / 2 \n",
    "\n",
    "def symmetric_sentence_jcn_similarity(sentence1, sentence2):\n",
    "    \"\"\" compute the symmetric sentence similarity using Wordnet \"\"\"\n",
    "    return (sentence_jcn_similarity(sentence1, sentence2) + sentence_jcn_similarity(sentence2, sentence1)) / 2 \n",
    "focus_sentence = \"Dolphins are swimming mammals.\"\n",
    "sentence = \"Dolphins can swim.\"\n",
    "print(\"Path Similarity: \")\n",
    "print(focus_sentence, sentence, symmetric_sentence_path_similarity(focus_sentence, sentence))\n",
    "print(\"LCH Similarity: \")\n",
    "print(focus_sentence, sentence, symmetric_sentence_lch_similarity(focus_sentence, sentence))\n",
    "print(\"LIN Similarity: \")\n",
    "print(focus_sentence, sentence, symmetric_sentence_lin_similarity(focus_sentence, sentence))\n",
    "print(\"JCN Similarity: \")\n",
    "print(focus_sentence, sentence, symmetric_sentence_jcn_similarity(focus_sentence, sentence))\n",
    "print(\"RES Similarity: \")\n",
    "print(focus_sentence, sentence, symmetric_sentence_res_similarity(focus_sentence, sentence))\n",
    "print(\"WUP Similarity: \")\n",
    "print(focus_sentence, sentence, symmetric_sentence_wup_similarity(focus_sentence, sentence))\n",
    "print() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "In this section we run the data through the pipeline to get it into the form necessary to create our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(corpus):\n",
    "    # TODO add a check to ensure the lengths of these arrays are the same\n",
    "    # or add the basic processing to pipeline\n",
    "    \n",
    "    s1_array = corpus['s1']['sentences']\n",
    "    s2_array = corpus['s2']['sentences']\n",
    "    s1_tokens = corpus['s1']['tokens']\n",
    "    s2_tokens = corpus['s2']['tokens']\n",
    "    s1_lemmas = corpus['s1']['lemmas']\n",
    "    s2_lemmas = corpus['s2']['lemmas']\n",
    "    s1_ls = corpus['s2']['ls']\n",
    "    s2_ls = corpus['s2']['ls']\n",
    "    #path_maxVal, wup_maxVal, lin_maxVal, lch_maxVal, res_maxVal, jcn_maxVal = max_similarity_values()\n",
    "\n",
    "    #print(s1_lemmas[0])\n",
    "    \n",
    "    data = []\n",
    "    for i in range(0, len(s1_array)):\n",
    "        cos_sim = calc_cosine_similarity(s1_array[i], s2_array[i])\n",
    "        sif_sim = calc_sif_similarity(s1_array[i], s2_array[i])\n",
    "        w_overlap, w_norm_overlap = calc_basic_overlap(s1_tokens[i], s2_tokens[i])\n",
    "        l_overlap, l_norm_overlap = calc_basic_overlap(s1_lemmas[i], s2_lemmas[i])\n",
    "        spacy_sim = calc_spacy_sim(s1_array[i], s2_array[i])\n",
    "        syn_overlap, normalized_syn_overlap = calc_synset_overlap(s1_tokens[i], s2_tokens[i])\n",
    "        path_similarity = symmetric_sentence_path_similarity(s1_ls[i], s2_ls[i]) \n",
    "        #wup_similarity = symmetric_sentence_wup_similarity(s1_array[i], s2_array[i]) / wup_maxVal\n",
    "        #jcn_similarity = symmetric_sentence_jcn_similarity(s1_array[i], s2_array[i]) / jcn_maxVal\n",
    "        #lch_similarity = symmetric_sentence_lch_similarity(s1_array[i], s2_array[i]) / lch_maxVal\n",
    "        #lin_similarity = symmetric_sentence_lin_similarity(s1_array[i], s2_array[i]) / lin_maxVal\n",
    "        #res_similarity = float(symmetric_sentence_res_similarity(s1_array[i], s2_array[i]) / res_maxVal)\n",
    "        ne_overlap = named_entity_overlap(s1_array[i], s2_array[i])\n",
    "        verb_overlap = head_verb_overlap(s1_array[i], s2_array[i])\n",
    "        #data.append([w_norm_overlap, l_norm_overlap, sif_sim, cos_sim, spacy_sim, syn_overlap, normalized_syn_overlap])\n",
    "        #data.insert(len(data),[w_norm_overlap, l_norm_overlap, spacy_sim, sif_sim, cos_sim, syn_overlap, normalized_syn_overlap, path_similarity, ne_overlap, verb_overlap, len(s1_tokens[i]), len(s2_tokens[i])])\n",
    "        #data.append([w_norm_overlap, l_norm_overlap, spacy_sim, sif_sim, cos_sim, syn_overlap, normalized_syn_overlap,path_similarity,wup_similarity,lin_similarity,jcn_similarity,lch_similarity,len(s1_tokens[i]), len(s2_tokens[i])])\n",
    "        data.insert(len(data),[len(s1_tokens), len(s2_tokens), w_norm_overlap, l_norm_overlap, spacy_sim, sif_sim, cos_sim, syn_overlap, normalized_syn_overlap, path_similarity, ne_overlap, len(s1_tokens[i]), len(s2_tokens[i])])\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1484, 1484, 0.6428571428571429, 0.6666666666666666, 0.9699586985720816, 0.4043188683415115, 0.5949218057093537, 144, 0.72544080604534, 1.0, 0.0, 28, 23], [1484, 1484, 0.631578947368421, 0.6, 0.9316565540040632, 0.37040524322972224, 0.474330706497194, 72, 0.6605504587155964, 1.0, 1.0, 10, 16], [1484, 1484, 0.5, 0.5, 0.9247478261787359, 0.1358693286767868, 0.392181175971253, 96, 0.6295081967213115, 1.0, 0.5, 16, 19], [1484, 1484, 0.7333333333333333, 0.7333333333333333, 0.9677497361187998, 0.6935512636502701, 0.668348418668298, 98, 0.7127272727272728, 1.0, 0.3333333333333333, 20, 29], [1484, 1484, 0.24, 0.24, 0.8618763728182428, 4.979960298599938e-10, 0.12170566815950139, 58, 0.5714285714285714, 1.0, 0.0, 18, 18]]\n"
     ]
    }
   ],
   "source": [
    "train_input = pipeline(train_data)\n",
    "print(train_input[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 4, 3, 3, 2]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['scores'][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "In this section we fit our feature set to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=8,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=14,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier(random_state=14, max_depth=8)\n",
    "dt_classifier.fit(train_input, train_data['scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "#print(f'Nodes: {dt_classifier.tree_.node_count}')\n",
    "#print(f'Max Depth: {dt_classifier.tree_.max_depth}')\n",
    "\n",
    "training_scores = train_data['scores']\n",
    "print(dt_classifier.score(train_input, training_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
       "            oob_score=False, random_state=14, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=14, n_estimators=100)\n",
    "rf_classifier.fit(train_input, train_data['scores'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rf_classifier, open(\"D:\\\\UTD\\\\NLPProject\\\\v3\\\\semantic-textual-similarity\\\\random_forest.pkl\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=2000,\n",
       "       n_jobs=3, nthread=None, objective='multi:softprob', random_state=42,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost_model = XGBClassifier(n_jobs=3, \n",
    "                              n_estimators=2000, \n",
    "                              #max_depth=6,\n",
    "                              #gamma=.05, \n",
    "                              random_state=42)\n",
    "xgboost_model.fit(np.asarray(train_input), np.asarray(train_data['scores']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "let me remind you that our ally include fervent supporter of this tax\n"
     ]
    }
   ],
   "source": [
    "#dev_data = preprocess(\"./data/dev-set.txt\")\n",
    "dev_data = preprocess(\"D:\\\\UTD\\\\NLPProject\\\\v2\\\\semantic-textual-similarity\\\\data\\\\dev-set.txt\")\n",
    "dev_input = pipeline(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shruti\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "dt_dev_predictions = dt_classifier.predict(dev_input)\n",
    "rf_dev_predictions = rf_classifier.predict(dev_input)\n",
    "xgb_dev_predictions = xgboost_model.predict(np.asarray(dev_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1210\n"
     ]
    }
   ],
   "source": [
    "f= open(\"D:\\\\UTD\\\\NLPProject\\\\v3\\\\semantic-textual-similarity\\\\dev-set-predicted-answers.txt\",\"w+\")\n",
    "f.write(\"id\\tGoldTag\\n\")\n",
    "i=1\n",
    "for x in rf_dev_predictions:\n",
    "    f.write(\"p_%d\\t%d\\n\" % (i,x))\n",
    "    i+=1\n",
    "print(i)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1209\n",
      "1209\n",
      "1209\n",
      "1209\n"
     ]
    }
   ],
   "source": [
    "# make sure our lengths match up\n",
    "print(len(dev_data['scores']))\n",
    "print(len(dt_dev_predictions))\n",
    "print(len(rf_dev_predictions))\n",
    "print(len(xgb_dev_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Avg Error</th>\n",
       "      <th>F-Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.413565</td>\n",
       "      <td>0.832093</td>\n",
       "      <td>0.423625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.459057</td>\n",
       "      <td>0.748553</td>\n",
       "      <td>0.464456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.438379</td>\n",
       "      <td>0.768404</td>\n",
       "      <td>0.448696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Model  Accuracy  Avg Error   F-Score\n",
       "0  Decision Tree  0.413565   0.832093  0.423625\n",
       "1  Random Forest  0.459057   0.748553  0.464456\n",
       "2        XGBoost  0.438379   0.768404  0.448696"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def get_metrics(name, predictions, scores):\n",
    "    correct = 0\n",
    "    total_error = 0\n",
    "    for i in range(0, len(predictions)): \n",
    "        if predictions[i] == scores[i]:\n",
    "            correct += 1\n",
    "        total_error += abs(int(scores[i]) - int(predictions[i]))\n",
    "    acc = correct / len(predictions) \n",
    "    avg_err = total_error / len(predictions)\n",
    "    \n",
    "    f1 = f1_score(scores, predictions, average='weighted')\n",
    "    return name, acc, avg_err, f1\n",
    "\n",
    "\n",
    "# Ensure all our arrays are full of ints for metrics.\n",
    "dev_data['scores'] = [int(i) for i in dev_data['scores']] \n",
    "rf_dev_predictions = [int(i) for i in rf_dev_predictions] \n",
    "xgb_dev_predictions = [int(i) for i in xgb_dev_predictions] \n",
    "dt_dev_predictions = [int(i) for i in dt_dev_predictions] \n",
    "\n",
    "\n",
    "\n",
    "xgb_metrics = get_metrics('XGBoost', xgb_dev_predictions, dev_data['scores'])\n",
    "dt_metrics  = get_metrics('Decision Tree', dt_dev_predictions, dev_data['scores'])\n",
    "rf_metrics = get_metrics('Random Forest', rf_dev_predictions, dev_data['scores'])\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    [dt_metrics, rf_metrics, xgb_metrics], \n",
    "    columns = ['Model', 'Accuracy', 'Avg Error', 'F-Score']) \n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}