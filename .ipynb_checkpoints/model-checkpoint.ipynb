{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same readData from STS.py\n",
    "def readData(fileName):\n",
    "\n",
    "    first_sentence = []\n",
    "    second_sentence = []\n",
    "    score = []\n",
    "    file = open(fileName, encoding=\"utf8\")\n",
    "    text = file.readline()\n",
    "    text = file.read()\n",
    "    # loop to extract a set of two sentences\n",
    "    for sentence in text.split('\\n'):\n",
    "        # creating two separate lists of the sentences\n",
    "        # '.rstrip('.') only removes the last period in the sentence\n",
    "        first_sentence.insert(len(first_sentence),\n",
    "                              (sentence.split('\\t')[1].lower()).rstrip('.'))\n",
    "        second_sentence.insert(len(first_sentence),\n",
    "                               (sentence.split('\\t')[2].lower()).rstrip('.'))\n",
    "        # inserting the score as a separate lists\n",
    "        score.insert(len(first_sentence), (sentence.split('\\t')[3]))\n",
    "\n",
    "    # print(first_sentence)\n",
    "    return first_sentence, second_sentence, score\n",
    "\n",
    "\n",
    "def preprocess(fileName):\n",
    "\n",
    "    first_sentence, second_sentence, score = readData(fileName)\n",
    "    first_sentence_tokens = []\n",
    "    second_sentence_tokens = []\n",
    "\n",
    "    # tokenizing and tagging\n",
    "    first_sentence_tags = []\n",
    "    second_sentence_tags = []\n",
    "\n",
    "    for sentence in first_sentence:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        first_sentence_tokens.insert(len(first_sentence_tokens), tokens)\n",
    "        first_sentence_tags.insert(\n",
    "            len(first_sentence_tags), nltk.pos_tag(tokens))\n",
    "        # print(first_sentence_tokens)\n",
    "\n",
    "    for sentence in second_sentence:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        second_sentence_tokens.insert(len(second_sentence_tokens), tokens)\n",
    "        second_sentence_tags.insert(\n",
    "            len(second_sentence_tags), nltk.pos_tag(tokens))\n",
    "\n",
    "        # print(second_sentence_tokens)\n",
    "\n",
    "    # lemmatizing\n",
    "    first_sentence_lemmas = []\n",
    "    second_sentence_lemmas = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for sentence in first_sentence_tokens:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        first_sentence_lemmas.insert(\n",
    "            len(first_sentence_lemmas), sentence_components)\n",
    "\n",
    "    for sentence in second_sentence_tokens:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        second_sentence_lemmas.insert(\n",
    "            len(second_sentence_lemmas), sentence_components)\n",
    "\n",
    "    return first_sentence, second_sentence, score, first_sentence_tokens, second_sentence_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_arr_train, s2_arr_train, scores_train, s1_tokens_train, s2_tokens_train = preprocess(\"./data/train-set.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This section includes all the code/functions to create features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cosine_similarity(sentence1, sentence2):\n",
    "\n",
    "    # remove the stopwords, transform into TF-IDF matrix, then\n",
    "    tfidf_matrix = TfidfVectorizer(\n",
    "        stop_words=\"english\").fit_transform([sentence1, sentence2])\n",
    "    cos_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "    cos_sim = cos_sim_matrix[0][1]\n",
    "\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "In this section we run the data through the pipeline to get it into the form necessary to create our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(s1_array, s2_array):\n",
    "    # TODO add a check to ensure the lengths of these arrays are the same\n",
    "    data = []\n",
    "    for i in range(0, len(s1_array)):\n",
    "        cos_sim = calc_cosine_similarity(s1_array[i], s2_array[i])\n",
    "        data.append([i, cos_sim])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0.5949218057093537], [1, 0.474330706497194], [2, 0.392181175971253], [3, 0.668348418668298], [4, 0.12170566815950139]]\n"
     ]
    }
   ],
   "source": [
    "data = pipeline(s1_arr_train, s2_arr_train)\n",
    "print(data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', '4', '3', '3', '2']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "In this section we fit our feature set to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=14,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier(random_state=14)\n",
    "dt_classifier.fit(data,scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 1399\n",
      "Max Depth: 29\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Nodes: {dt_classifier.tree_.node_count}')\n",
    "print(f'Max Depth: {dt_classifier.tree_.max_depth}')\n",
    "print(f'Accuracy: {dt_classifier.score(data, scores_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_arr_dev, s2_arr_dev, scores_dev, s1_tokens_dev, s2_tokens_dev = preprocess(\"./data/dev-set.txt\")\n",
    "dev_data = pipeline(s1_arr_dev, s2_arr_dev)\n",
    "dev_predictions = dt_classifier.predict(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1209\n",
      "1209\n"
     ]
    }
   ],
   "source": [
    "# make sure our lengths match up\n",
    "print(len(scores_dev))\n",
    "print(len(dev_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(dev_predictions))N:\n",
    "    correct = 0\n",
    "    if dev_predictions[i] == scores_dev[i]:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / len(dev_predictions)\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
