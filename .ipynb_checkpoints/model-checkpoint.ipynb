{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.parse.corenlp import CoreNLPParser\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.probability import FreqDist\n",
    "from collections import Counter\n",
    "import math\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same readData from STS.py\n",
    "def readData(fileName):\n",
    "\n",
    "    first_sentence = []\n",
    "    second_sentence = []\n",
    "    score = []\n",
    "    file = open(fileName, encoding=\"utf8\")\n",
    "    text = file.readline()\n",
    "    text = file.read()\n",
    "    # loop to extract a set of two sentences\n",
    "    for sentence in text.split('\\n'):\n",
    "        # creating two separate lists of the sentences\n",
    "        # '.rstrip('.') only removes the last period in the sentence\n",
    "        first_sentence.insert(len(first_sentence),\n",
    "                              (sentence.split('\\t')[1].lower()).rstrip('.'))\n",
    "        second_sentence.insert(len(first_sentence),\n",
    "                               (sentence.split('\\t')[2].lower()).rstrip('.'))\n",
    "        # inserting the score as a separate lists\n",
    "        score.insert(len(first_sentence), (sentence.split('\\t')[3]))\n",
    "\n",
    "    # print(first_sentence)\n",
    "    return first_sentence, second_sentence, score\n",
    "\n",
    "\n",
    "def preprocess(fileName):\n",
    "\n",
    "    first_sentence, second_sentence, score = readData(fileName)\n",
    "    first_sentence_tokens = []\n",
    "    second_sentence_tokens = []\n",
    "\n",
    "    # tokenizing and tagging\n",
    "    first_sentence_tags = []\n",
    "    second_sentence_tags = []\n",
    "\n",
    "    for sentence in first_sentence:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        first_sentence_tokens.insert(len(first_sentence_tokens), tokens)\n",
    "        first_sentence_tags.insert(\n",
    "            len(first_sentence_tags), nltk.pos_tag(tokens))\n",
    "        # print(first_sentence_tokens)\n",
    "\n",
    "    for sentence in second_sentence:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        second_sentence_tokens.insert(len(second_sentence_tokens), tokens)\n",
    "        second_sentence_tags.insert(\n",
    "            len(second_sentence_tags), nltk.pos_tag(tokens))\n",
    "\n",
    "        # print(second_sentence_tokens)\n",
    "\n",
    "    # lemmatizing\n",
    "    first_sentence_lemmas = []\n",
    "    second_sentence_lemmas = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for sentence in first_sentence_tokens:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        first_sentence_lemmas.insert(\n",
    "            len(first_sentence_lemmas), sentence_components)\n",
    "\n",
    "    for sentence in second_sentence_tokens:\n",
    "        sentence_components = []\n",
    "        for token in sentence:\n",
    "            lemmas = lemmatizer.lemmatize(token)\n",
    "            sentence_components.insert(len(sentence_components), lemmas)\n",
    "        second_sentence_lemmas.insert(\n",
    "            len(second_sentence_lemmas), sentence_components)\n",
    "\n",
    "    return first_sentence, second_sentence, score, first_sentence_tokens, second_sentence_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_arr_train, s2_arr_train, scores_train, s1_tokens_train, s2_tokens_train = preprocess(\"./data/train-set.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "This section includes all the code/functions to create features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_cosine_similarity(sentence1, sentence2):\n",
    "\n",
    "    # remove the stopwords, transform into TF-IDF matrix, then\n",
    "    tfidf_matrix = TfidfVectorizer(\n",
    "        stop_words=\"english\").fit_transform([sentence1, sentence2])\n",
    "    cos_sim_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "#     print(tfidf_matrix.toarray())\n",
    "\n",
    "    cos_sim = cos_sim_matrix[0][1]\n",
    "\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smooth Inverse Frequency (SIF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_distribution(s1_tokens_array, s2_tokens_array):\n",
    "    freq_dist = FreqDist()\n",
    "    for i in range(len(s1_tokens_array)):\n",
    "        for token in (s1_tokens_array[i] + s2_tokens_array[i]):\n",
    "            freq_dist[token.lower()] += 1\n",
    "    return freq_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 5169), (',', 3690), ('of', 2497), ('to', 2133), ('and', 1716), ('a', 1615), ('in', 1573), ('is', 891), ('that', 831), ('on', 820), ('for', 756), ('it', 587), ('this', 579), ('we', 531), ('with', 464), ('be', 459), ('by', 443), ('i', 425), ('which', 403), ('have', 384), ('not', 366), ('at', 343), ('as', 334), ('are', 333), ('has', 319), ('said', 316), ('was', 304), ('european', 287), (\"'s\", 280), ('from', 261), ('``', 252), (\"''\", 242), ('will', 233), ('.', 229), ('also', 223), ('its', 194), ('but', 193), ('would', 191), ('all', 188), ('percent', 187)]\n"
     ]
    }
   ],
   "source": [
    "freq_dist = frequency_distribution(s1_tokens_train, s2_tokens_train)\n",
    "print(freq_dist.most_common(40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_sif_similarity(s1, s2, a = .001):\n",
    "    vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "    X = vectorizer.fit_transform([s1, s2])\n",
    "    X_arr = X.toarray()\n",
    "    sif_matrix = []\n",
    "    for i in range(0, len(X_arr)):\n",
    "        sif_arr = []\n",
    "        for j in range(0, len(X_arr[i])):\n",
    "            word = vectorizer.get_feature_names()[j]\n",
    "            w = a / (a + freq_dist[word])\n",
    "            v = X_arr[i][j]\n",
    "            sif_arr.append(v*w)\n",
    "        sif_matrix.append(sif_arr)\n",
    "    sif_cos_sim_matrix = cosine_similarity(sif_matrix, sif_matrix)\n",
    "    sif_cos_sim = sif_cos_sim_matrix[0][1]\n",
    "    return sif_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4515545128534995e-10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_sif_similarity('I like some apples', 'I like the pears')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Synset+ Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tokenized_sentence_list = s1_tokens_train+s2_tokens_train\n",
    "words_filtered = []\n",
    "\n",
    "# print(words)\n",
    "\n",
    "# looking through I've noticed there are a number of stop-words that can be added to the set\n",
    "stop_words.add(',')\n",
    "stop_words.add('``')\n",
    "stop_words.add(\"n't\")\n",
    "\n",
    "for tsl in tokenized_sentence_list:\n",
    "    for w in tsl:\n",
    "        if w not in stop_words and w not in words_filtered:\n",
    "            words_filtered.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(token_list):\n",
    "    blank_list = []\n",
    "    for w in token_list:\n",
    "        if w not in stop_words:\n",
    "            blank_list.append(w)\n",
    "    return blank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_tokens(token_list):\n",
    "    blank_list = []\n",
    "    for w in token_list:\n",
    "        if w not in blank_list:\n",
    "            blank_list.append(w)\n",
    "    return blank_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sources', 'close', 'sale', 'said', 'vivendi', 'keeping', 'door', 'open', 'bids', 'hoped', 'see', 'bidders', 'interested', 'individual', 'assets', 'team']\n",
      "['sources', 'close', 'sale', 'said', 'vivendi', 'keeping', 'door', 'open', 'bids', 'next', 'day', 'two']\n",
      "9\n",
      "0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "s1 = s1_tokens_train[0]\n",
    "s2 = s2_tokens_train[0]\n",
    "\n",
    "s1 = remove_stopwords(s1)\n",
    "s1 = remove_duplicate_tokens(s1)\n",
    "\n",
    "s2 = remove_stopwords(s2)\n",
    "s2 = remove_duplicate_tokens(s2)\n",
    "\n",
    "print(s1)\n",
    "print(s2)\n",
    "\n",
    "overlap = 0\n",
    "encountered_indexes = []\n",
    "for word in (s1+s2):\n",
    "    word_index = words_filtered.index(word)\n",
    "    if word_index in encountered_indexes: # we know we have found an overlap\n",
    "        overlap += 1\n",
    "    encountered_indexes.append(word_index)\n",
    "    \n",
    "print(overlap)\n",
    "avg_sentence_len = len(s1+s2) / 2\n",
    "\n",
    "print(overlap / avg_sentence_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_basic_overlap(s1_tokens, s2_tokens):\n",
    "    s1_tokens = remove_stopwords(s1_tokens)\n",
    "    s1_tokens = remove_duplicate_tokens(s1_tokens)\n",
    "\n",
    "    s2_tokens = remove_stopwords(s2_tokens)\n",
    "    s2_tokens = remove_duplicate_tokens(s2_tokens)\n",
    "    \n",
    "    overlap = 0\n",
    "    encountered_indexes = []\n",
    "    for word in (s1_tokens+s2_tokens):\n",
    "        try:\n",
    "            word_index = words_filtered.index(word)\n",
    "            if word_index in encountered_indexes: # we know we have found an overlap\n",
    "                overlap += 1\n",
    "            encountered_indexes.append(word_index)\n",
    "        except ValueError:\n",
    "            print(word+'not found in lexicon. Skipping...')\n",
    "\n",
    "    avg_sentence_len = len(s1_tokens+s2_tokens) / 2\n",
    "    \n",
    "    overlap_normlalized = overlap / avg_sentence_len\n",
    "    return overlap, overlap_normlalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "In this section we run the data through the pipeline to get it into the form necessary to create our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(s1_array, s2_array, s1_tokens, s2_tokens):\n",
    "    # TODO add a check to ensure the lengths of these arrays are the same\n",
    "    data = []\n",
    "    for i in range(0, len(s1_array)):\n",
    "        cos_sim = calc_cosine_similarity(s1_array[i], s2_array[i])\n",
    "        sif_sim = calc_sif_similarity(s1_array[i], s2_array[i])\n",
    "        overlap, normalized_overlap = calc_basic_overlap(s1_tokens[i], s2_tokens[i])\n",
    "        data.append([i, cos_sim, sif_sim, overlap, normalized_overlap])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0.5949218057093537, 0.4043188683415115, 9, 0.6428571428571429], [1, 0.474330706497194, 0.37040524322972224, 6, 0.631578947368421], [2, 0.392181175971253, 0.1358693286767868, 5, 0.5], [3, 0.668348418668298, 0.6935512636502701, 11, 0.7333333333333333], [4, 0.12170566815950139, 4.979960298599938e-10, 3, 0.24]]\n"
     ]
    }
   ],
   "source": [
    "data = pipeline(s1_arr_train, s2_arr_train, s1_tokens_train, s2_tokens_train)\n",
    "print(data[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4', '4', '3', '3', '2']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_train[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "In this section we fit our feature set to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=14,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_classifier = DecisionTreeClassifier(random_state=14)\n",
    "dt_classifier.fit(data,scores_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 1095\n",
      "Max Depth: 26\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Nodes: {dt_classifier.tree_.node_count}')\n",
    "print(f'Max Depth: {dt_classifier.tree_.max_depth}')\n",
    "print(f'Accuracy: {dt_classifier.score(data, scores_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1_arr_dev, s2_arr_dev, scores_dev, s1_tokens_dev, s2_tokens_dev = preprocess(\"./data/dev-set.txt\")\n",
    "# dev_data = pipeline(s1_arr_dev, s2_arr_dev)\n",
    "# dev_predictions = dt_classifier.predict(dev_data)\n",
    "\n",
    "# xgboost_model = xgb.XGBRegressor(booster='gbtree', \n",
    "#                        n_estimators=1000,\n",
    "#                        n_jobs=4,\n",
    "#                        learning_rate=.05,\n",
    "#                        max_depth=3,\n",
    "#                        random_state=42,\n",
    "#                        gamma=.05,\n",
    "#                        early_stopping_rounds = 5)\n",
    "\n",
    "# xgboost_model.fit(data, scores_train,\n",
    "# #                   eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "#                   eval_metric='logloss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'aujourd' is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-607588210af3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ms1_arr_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_arr_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1_tokens_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_tokens_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./data/dev-set.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdev_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_arr_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_arr_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1_tokens_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_tokens_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdev_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdt_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-8a69f9a66346>\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(s1_array, s2_array, s1_tokens, s2_tokens)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mcos_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_cosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0msif_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_sif_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_array\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moverlap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_overlap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalc_basic_overlap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_tokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msif_sim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverlap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_overlap\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-ab147d901471>\u001b[0m in \u001b[0;36mcalc_basic_overlap\u001b[0;34m(s1_tokens, s2_tokens)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mencountered_indexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ms1_tokens\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0ms2_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mword_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwords_filtered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mencountered_indexes\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# we know we have found an overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moverlap\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'aujourd' is not in list"
     ]
    }
   ],
   "source": [
    "s1_arr_dev, s2_arr_dev, scores_dev, s1_tokens_dev, s2_tokens_dev = preprocess(\"./data/dev-set.txt\")\n",
    "dev_data = pipeline(s1_arr_dev, s2_arr_dev, s1_tokens_dev, s2_tokens_dev)\n",
    "dev_predictions = dt_classifier.predict(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure our lengths match up\n",
    "print(len(scores_dev))\n",
    "print(len(dev_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for i in range(0, len(dev_predictions)): \n",
    "    if dev_predictions[i] == scores_dev[i]:\n",
    "        correct += 1\n",
    "    \n",
    "#     print(dev_predictions[i], scores_dev[i], (dev_predictions[i] == scores_dev[i]))\n",
    "    \n",
    "print(correct / len(dev_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
